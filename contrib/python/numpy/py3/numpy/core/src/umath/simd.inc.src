

/*
 * This file is for the definitions of simd vectorized operations.
 *
 * Currently contains sse2 functions that are built on amd64, x32 or
 * non-generic builds (CFLAGS=-march=...)
 * In future it may contain other instruction sets like AVX or NEON detected
 * at runtime in which case it needs to be included indirectly via a file
 * compiled with special options (or use gcc target attributes) so the binary
 * stays portable.
 */


#ifndef __NPY_SIMD_INC
#define __NPY_SIMD_INC

#include "lowlevel_strided_loops.h"
#include "numpy/npy_common.h"
#include "numpy/npy_math.h"
#include "npy_simd_data.h"
#ifdef NPY_HAVE_SSE2_INTRINSICS
#include <emmintrin.h>
#if !defined(_MSC_VER) || _MSC_VER >= 1600
#include <immintrin.h>
#else
#undef __AVX2__
#undef __AVX512F__
#endif
#endif
#include "loops_utils.h" // nomemoverlap
#include <assert.h>
#include <stdlib.h>
#include <float.h>
#include <string.h> /* for memcpy */

#define VECTOR_SIZE_BYTES 16

/*
 * Dispatcher functions
 * decide whether the operation can be vectorized and run it
 * if it was run returns true and false if nothing was done
 */

/*
 *****************************************************************************
 **                           CMPLX DISPATCHERS
 *****************************************************************************
 */

/**begin repeat
 * #TYPE = CFLOAT, CDOUBLE#
 * #type= npy_float, npy_double#
 * #esize = 8, 16#
 */

/**begin repeat1
 *  #func = square, absolute, conjugate#
 *  #outsize = 1, 2, 1#
 *  #max_stride = 2, 8, 8#
 */

#if defined HAVE_ATTRIBUTE_TARGET_AVX512F_WITH_INTRINSICS && defined NPY_HAVE_SSE2_INTRINSICS
static NPY_INLINE NPY_GCC_TARGET_AVX512F void
AVX512F_@func@_@TYPE@(@type@*, @type@*, const npy_intp n, const npy_intp stride);
#endif

static NPY_INLINE int
run_unary_avx512f_@func@_@TYPE@(char **args, const npy_intp *dimensions, const npy_intp *steps)
{
#if defined HAVE_ATTRIBUTE_TARGET_AVX512F_WITH_INTRINSICS && defined NPY_HAVE_SSE2_INTRINSICS
    if ((IS_OUTPUT_BLOCKABLE_UNARY(@esize@, (npy_uint)(@esize@/@outsize@), 64)) && (labs(steps[0]) < 2*@max_stride@*@esize@)) {
        AVX512F_@func@_@TYPE@((@type@*)args[1], (@type@*)args[0], dimensions[0], steps[0]);
        return 1;
    }
    else
        return 0;
#endif
    return 0;
}

/**end repeat1**/
/**end repeat**/

/*
 *****************************************************************************
 **                           FLOAT DISPATCHERS
 *****************************************************************************
 */

/**begin repeat
 * #type = npy_float, npy_double, npy_longdouble#
 * #TYPE = FLOAT, DOUBLE, LONGDOUBLE#
 * #EXISTS = 1, 1, 0#
 */

/**begin repeat1
 *  #func = maximum, minimum#
 */

#if defined HAVE_ATTRIBUTE_TARGET_AVX512F_WITH_INTRINSICS && defined NPY_HAVE_SSE2_INTRINSICS && @EXISTS@
static NPY_INLINE NPY_GCC_TARGET_AVX512F void
AVX512F_@func@_@TYPE@(char **args, npy_intp const *dimensions, npy_intp const *steps);
#endif

static NPY_INLINE int
run_binary_avx512f_@func@_@TYPE@(char **args, npy_intp const *dimensions, npy_intp const *steps)
{
#if defined HAVE_ATTRIBUTE_TARGET_AVX512F_WITH_INTRINSICS && defined NPY_HAVE_SSE2_INTRINSICS && @EXISTS@
    if (IS_BINARY_SMALL_STEPS_AND_NOMEMOVERLAP) {
        AVX512F_@func@_@TYPE@(args, dimensions, steps);
        return 1;
    }
    else
        return 0;
#endif
    return 0;
}
/**end repeat1**/

/**end repeat**/

/**begin repeat
 * #type = npy_float, npy_double, npy_longdouble#
 * #TYPE = FLOAT, DOUBLE, LONGDOUBLE#
 * #EXISTS = 1, 1, 0#
 */

/**begin repeat1
 * #func = isnan, isfinite, isinf, signbit#
 */

#if defined HAVE_ATTRIBUTE_TARGET_AVX512_SKX_WITH_INTRINSICS && defined NPY_HAVE_SSE2_INTRINSICS && @EXISTS@
static NPY_INLINE NPY_GCC_TARGET_AVX512_SKX void
AVX512_SKX_@func@_@TYPE@(npy_bool*, @type@*, const npy_intp n, const npy_intp stride);
#endif

static NPY_INLINE int
run_@func@_avx512_skx_@TYPE@(char **args, npy_intp const *dimensions, npy_intp const *steps)
{
#if defined HAVE_ATTRIBUTE_TARGET_AVX512_SKX_WITH_INTRINSICS && defined NPY_HAVE_SSE2_INTRINSICS && @EXISTS@
    if (IS_OUTPUT_BLOCKABLE_UNARY(sizeof(@type@), sizeof(npy_bool), 64)) {
        AVX512_SKX_@func@_@TYPE@((npy_bool*)args[1], (@type@*)args[0], dimensions[0], steps[0]);
        return 1;
    }
    else {
        return 0;
    }
#endif
    return 0;
}


/**end repeat1**/
/**end repeat**/

/**begin repeat
 * #ISA = FMA, AVX512F#
 * #isa = fma, avx512f#
 * #CHK = HAVE_ATTRIBUTE_TARGET_AVX2_WITH_INTRINSICS, HAVE_ATTRIBUTE_TARGET_AVX512F_WITH_INTRINSICS#
 * #REGISTER_SIZE = 32, 64#
 */

/* prototypes */

/**begin repeat1
 * #type = npy_float, npy_double#
 * #TYPE = FLOAT, DOUBLE#
 */

/**begin repeat2
 *  #func = rint, floor, trunc#
 */

#if defined @CHK@ && defined NPY_HAVE_SSE2_INTRINSICS
static NPY_INLINE NPY_GCC_TARGET_@ISA@ void
@ISA@_@func@_@TYPE@(@type@ *, @type@ *, const npy_intp n, const npy_intp stride);
#endif

static NPY_INLINE int
run_unary_@isa@_@func@_@TYPE@(char **args, npy_intp const *dimensions, npy_intp const *steps)
{
#if defined @CHK@ && defined NPY_HAVE_SSE2_INTRINSICS
    if (IS_OUTPUT_BLOCKABLE_UNARY(sizeof(@type@), sizeof(@type@), @REGISTER_SIZE@)) {
        @ISA@_@func@_@TYPE@((@type@*)args[1], (@type@*)args[0], dimensions[0], steps[0]);
        return 1;
    }
    else
        return 0;
#endif
    return 0;
}

/**end repeat2**/
/**end repeat1**/
/**end repeat**/

/**begin repeat
 * Float types
 *  #type = npy_float, npy_double, npy_longdouble#
 *  #TYPE = FLOAT, DOUBLE, LONGDOUBLE#
 *  #vector = 1, 1, 0#
 *  #VECTOR = NPY_SIMD, NPY_SIMD_F64, 0 #
 */

/**begin repeat1
 * #func = negative, minimum, maximum#
 * #check = IS_BLOCKABLE_UNARY, IS_BLOCKABLE_REDUCE*2 #
 * #name = unary, unary_reduce*2#
 */

#if @vector@ && defined NPY_HAVE_SSE2_INTRINSICS

/* prototypes */
static void
sse2_@func@_@TYPE@(@type@ *, @type@ *, const npy_intp n);

#endif

static NPY_INLINE int
run_@name@_simd_@func@_@TYPE@(char **args, npy_intp const *dimensions, npy_intp const *steps)
{
#if @vector@ && defined NPY_HAVE_SSE2_INTRINSICS
    if (@check@(sizeof(@type@), VECTOR_SIZE_BYTES)) {
        sse2_@func@_@TYPE@((@type@*)args[1], (@type@*)args[0], dimensions[0]);
        return 1;
    }
#endif
    return 0;
}

/**end repeat1**/

/**begin repeat1
 * #kind = equal, not_equal, less, less_equal, greater, greater_equal,
 *         logical_and, logical_or#
 * #simd = 1, 1, 1, 1, 1, 1, 0, 0#
 */

#if @vector@ && @simd@ && defined NPY_HAVE_SSE2_INTRINSICS

/* prototypes */
static void
sse2_binary_@kind@_@TYPE@(npy_bool * op, @type@ * ip1, @type@ * ip2,
                          npy_intp n);
static void
sse2_binary_scalar1_@kind@_@TYPE@(npy_bool * op, @type@ * ip1, @type@ * ip2,
                                  npy_intp n);
static void
sse2_binary_scalar2_@kind@_@TYPE@(npy_bool * op, @type@ * ip1, @type@ * ip2,
                                  npy_intp n);

#endif

static NPY_INLINE int
run_binary_simd_@kind@_@TYPE@(char **args, npy_intp const *dimensions, npy_intp const *steps)
{
#if @vector@ && @simd@ && defined NPY_HAVE_SSE2_INTRINSICS
    @type@ * ip1 = (@type@ *)args[0];
    @type@ * ip2 = (@type@ *)args[1];
    npy_bool * op = (npy_bool *)args[2];
    npy_intp n = dimensions[0];
    /* argument one scalar */
    if (IS_BLOCKABLE_BINARY_SCALAR1_BOOL(sizeof(@type@), VECTOR_SIZE_BYTES)) {
        sse2_binary_scalar1_@kind@_@TYPE@(op, ip1, ip2, n);
        return 1;
    }
    /* argument two scalar */
    else if (IS_BLOCKABLE_BINARY_SCALAR2_BOOL(sizeof(@type@), VECTOR_SIZE_BYTES)) {
        sse2_binary_scalar2_@kind@_@TYPE@(op, ip1, ip2, n);
        return 1;
    }
    else if (IS_BLOCKABLE_BINARY_BOOL(sizeof(@type@), VECTOR_SIZE_BYTES)) {
        sse2_binary_@kind@_@TYPE@(op, ip1, ip2, n);
        return 1;
    }
#endif
    return 0;
}

/**end repeat1**/

/**begin repeat1
 * #kind = isnan, isfinite, isinf, signbit#
 */

#if @vector@ && defined NPY_HAVE_SSE2_INTRINSICS

static void
sse2_@kind@_@TYPE@(npy_bool * op, @type@ * ip1, npy_intp n);

#endif

static NPY_INLINE int
run_@kind@_simd_@TYPE@(char **args, npy_intp const *dimensions, npy_intp const *steps)
{
#if @vector@ && defined NPY_HAVE_SSE2_INTRINSICS
    if (steps[0] == sizeof(@type@) && steps[1] == 1 &&
        npy_is_aligned(args[0], sizeof(@type@))) {
        sse2_@kind@_@TYPE@((npy_bool*)args[1], (@type@*)args[0], dimensions[0]);
        return 1;
    }
#endif
    return 0;
}

/**end repeat1**/

/**end repeat**/

/*
 *****************************************************************************
 **                           BOOL DISPATCHERS
 *****************************************************************************
 */

/**begin repeat
 * # kind = logical_or, logical_and#
 */

#if defined NPY_HAVE_SSE2_INTRINSICS
static void
sse2_binary_@kind@_BOOL(npy_bool * op, npy_bool * ip1, npy_bool * ip2,
                        npy_intp n);

static void
sse2_reduce_@kind@_BOOL(npy_bool * op, npy_bool * ip, npy_intp n);
#endif

static NPY_INLINE int
run_binary_simd_@kind@_BOOL(char **args, npy_intp const *dimensions, npy_intp const *steps)
{
#if defined NPY_HAVE_SSE2_INTRINSICS
    if (sizeof(npy_bool) == 1 &&
            IS_BLOCKABLE_BINARY(sizeof(npy_bool), VECTOR_SIZE_BYTES)) {
        sse2_binary_@kind@_BOOL((npy_bool*)args[2], (npy_bool*)args[0],
                               (npy_bool*)args[1], dimensions[0]);
        return 1;
    }
#endif
    return 0;
}


static NPY_INLINE int
run_reduce_simd_@kind@_BOOL(char **args, npy_intp const *dimensions, npy_intp const *steps)
{
#if defined NPY_HAVE_SSE2_INTRINSICS
    if (sizeof(npy_bool) == 1 &&
            IS_BLOCKABLE_REDUCE(sizeof(npy_bool), VECTOR_SIZE_BYTES)) {
        sse2_reduce_@kind@_BOOL((npy_bool*)args[0], (npy_bool*)args[1],
                                dimensions[0]);
        return 1;
    }
#endif
    return 0;
}

/**end repeat**/

/**begin repeat
 * # kind = absolute, logical_not#
 */

#if defined NPY_HAVE_SSE2_INTRINSICS
static void
sse2_@kind@_BOOL(npy_bool *, npy_bool *, const npy_intp n);
#endif

static NPY_INLINE int
run_unary_simd_@kind@_BOOL(char **args, npy_intp const *dimensions, npy_intp const *steps)
{
#if defined NPY_HAVE_SSE2_INTRINSICS
    if (sizeof(npy_bool) == 1 &&
            IS_BLOCKABLE_UNARY(sizeof(npy_bool), VECTOR_SIZE_BYTES)) {
        sse2_@kind@_BOOL((npy_bool*)args[1], (npy_bool*)args[0], dimensions[0]);
        return 1;
    }
#endif
    return 0;
}

/**end repeat**/

#ifdef NPY_HAVE_SSE2_INTRINSICS

/*
 * Vectorized operations
 */
/*
 *****************************************************************************
 **                           FLOAT LOOPS
 *****************************************************************************
 */

/**begin repeat
* horizontal reductions on a vector
* # VOP = min, max#
*/

NPY_FINLINE npy_float sse2_horizontal_@VOP@___m128(__m128 v)
{
    npy_float r;
    __m128 tmp = _mm_movehl_ps(v, v);                   /* c     d     ... */
    __m128 m = _mm_@VOP@_ps(v, tmp);                    /* m(ac) m(bd) ... */
    tmp = _mm_shuffle_ps(m, m, _MM_SHUFFLE(1, 1, 1, 1));/* m(bd) m(bd) ... */
    _mm_store_ss(&r, _mm_@VOP@_ps(tmp, m));             /* m(acbd) ... */
    return r;
}

NPY_FINLINE npy_double sse2_horizontal_@VOP@___m128d(__m128d v)
{
    npy_double r;
    __m128d tmp = _mm_unpackhi_pd(v, v);    /* b     b */
    _mm_store_sd(&r, _mm_@VOP@_pd(tmp, v)); /* m(ab) m(bb) */
    return r;
}
/**end repeat**/

/**begin repeat
 *  #type = npy_float, npy_double#
 *  #TYPE = FLOAT, DOUBLE#
 *  #scalarf = npy_sqrtf, npy_sqrt#
 *  #c = f, #
 *  #vtype = __m128, __m128d#
 *  #vtype256 = __m256, __m256d#
 *  #vtype512 = __m512, __m512d#
 *  #vpre = _mm, _mm#
 *  #vpre256 = _mm256, _mm256#
 *  #vpre512 = _mm512, _mm512#
 *  #vsuf = ps, pd#
 *  #vsufs = ss, sd#
 *  #nan = NPY_NANF, NPY_NAN#
 *  #double = 0, 1#
 *  #cast = _mm_castps_si128, _mm_castpd_si128#
 */
/*
 * compress 4 vectors to 4/8 bytes in op with filled with 0 or 1
 * the last vector is passed as a pointer as MSVC 2010 is unable to ignore the
 * calling convention leading to C2719 on 32 bit, see #4795
 */
NPY_FINLINE void
sse2_compress4_to_byte_@TYPE@(@vtype@ r1, @vtype@ r2, @vtype@ r3, @vtype@ * r4,
                              npy_bool * op)
{
    const __m128i mask = @vpre@_set1_epi8(0x1);
    __m128i ir1 = @vpre@_packs_epi32(@cast@(r1), @cast@(r2));
    __m128i ir2 = @vpre@_packs_epi32(@cast@(r3), @cast@(*r4));
    __m128i rr = @vpre@_packs_epi16(ir1, ir2);
#if @double@
    rr = @vpre@_packs_epi16(rr, rr);
    rr = @vpre@_and_si128(rr, mask);
    @vpre@_storel_epi64((__m128i*)op, rr);
#else
    rr = @vpre@_and_si128(rr, mask);
    @vpre@_storeu_si128((__m128i*)op, rr);
#endif
}

static void
sse2_signbit_@TYPE@(npy_bool * op, @type@ * ip1, npy_intp n)
{
    LOOP_BLOCK_ALIGN_VAR(ip1, @type@, VECTOR_SIZE_BYTES) {
        op[i] = npy_signbit(ip1[i]) != 0;
    }
    LOOP_BLOCKED(@type@, VECTOR_SIZE_BYTES) {
        @vtype@ a = @vpre@_load_@vsuf@(&ip1[i]);
        int r = @vpre@_movemask_@vsuf@(a);
        if (sizeof(@type@) == 8) {
            op[i] = r & 1;
            op[i + 1] = (r >> 1);
        }
        else {
            op[i] = r & 1;
            op[i + 1] = (r >> 1) & 1;
            op[i + 2] = (r >> 2) & 1;
            op[i + 3] = (r >> 3);
        }
    }
    LOOP_BLOCKED_END {
        op[i] = npy_signbit(ip1[i]) != 0;
    }
}

/**begin repeat1
 * #kind = isnan, isfinite, isinf#
 * #var = 0, 1, 2#
 */

static void
sse2_@kind@_@TYPE@(npy_bool * op, @type@ * ip1, npy_intp n)
{
#if @var@ != 0 /* isinf/isfinite */
    /* signbit mask 0x7FFFFFFF after andnot */
    const @vtype@ mask = @vpre@_set1_@vsuf@(-0.@c@);
    const @vtype@ ones = @vpre@_cmpeq_@vsuf@(@vpre@_setzero_@vsuf@(),
                                             @vpre@_setzero_@vsuf@());
#if @double@
    const @vtype@ fltmax = @vpre@_set1_@vsuf@(DBL_MAX);
#else
    const @vtype@ fltmax = @vpre@_set1_@vsuf@(FLT_MAX);
#endif
#endif
    LOOP_BLOCK_ALIGN_VAR(ip1, @type@, VECTOR_SIZE_BYTES) {
        op[i] = npy_@kind@(ip1[i]) != 0;
    }
    LOOP_BLOCKED(@type@, 4 * VECTOR_SIZE_BYTES) {
        @vtype@ a = @vpre@_load_@vsuf@(&ip1[i + 0 * VECTOR_SIZE_BYTES / sizeof(@type@)]);
        @vtype@ b = @vpre@_load_@vsuf@(&ip1[i + 1 * VECTOR_SIZE_BYTES / sizeof(@type@)]);
        @vtype@ c = @vpre@_load_@vsuf@(&ip1[i + 2 * VECTOR_SIZE_BYTES / sizeof(@type@)]);
        @vtype@ d = @vpre@_load_@vsuf@(&ip1[i + 3 * VECTOR_SIZE_BYTES / sizeof(@type@)]);
        @vtype@ r1, r2, r3, r4;
#if @var@ != 0 /* isinf/isfinite */
        /* fabs via masking of sign bit */
        r1 = @vpre@_andnot_@vsuf@(mask, a);
        r2 = @vpre@_andnot_@vsuf@(mask, b);
        r3 = @vpre@_andnot_@vsuf@(mask, c);
        r4 = @vpre@_andnot_@vsuf@(mask, d);
#if @var@ == 1 /* isfinite */
        /* negative compare against max float, nan is always true */
        r1 = @vpre@_cmpnle_@vsuf@(r1, fltmax);
        r2 = @vpre@_cmpnle_@vsuf@(r2, fltmax);
        r3 = @vpre@_cmpnle_@vsuf@(r3, fltmax);
        r4 = @vpre@_cmpnle_@vsuf@(r4, fltmax);
#else /* isinf */
        r1 = @vpre@_cmpnlt_@vsuf@(fltmax, r1);
        r2 = @vpre@_cmpnlt_@vsuf@(fltmax, r2);
        r3 = @vpre@_cmpnlt_@vsuf@(fltmax, r3);
        r4 = @vpre@_cmpnlt_@vsuf@(fltmax, r4);
#endif
        /* flip results to what we want (andnot as there is no sse not) */
        r1 = @vpre@_andnot_@vsuf@(r1, ones);
        r2 = @vpre@_andnot_@vsuf@(r2, ones);
        r3 = @vpre@_andnot_@vsuf@(r3, ones);
        r4 = @vpre@_andnot_@vsuf@(r4, ones);
#endif
#if @var@ == 0 /* isnan */
        r1 = @vpre@_cmpneq_@vsuf@(a, a);
        r2 = @vpre@_cmpneq_@vsuf@(b, b);
        r3 = @vpre@_cmpneq_@vsuf@(c, c);
        r4 = @vpre@_cmpneq_@vsuf@(d, d);
#endif
        sse2_compress4_to_byte_@TYPE@(r1, r2, r3, &r4, &op[i]);
    }
    LOOP_BLOCKED_END {
        op[i] = npy_@kind@(ip1[i]) != 0;
    }
}

/**end repeat1**/

/**begin repeat1
 * #kind = equal, not_equal, less, less_equal, greater, greater_equal#
 * #OP = ==, !=, <, <=, >, >=#
 * #VOP = cmpeq, cmpneq, cmplt, cmple, cmpgt, cmpge#
*/

/* sets invalid fpu flag on QNaN for consistency with packed compare */
NPY_FINLINE int
sse2_ordered_cmp_@kind@_@TYPE@(const @type@ a, const @type@ b)
{
    @vtype@ one = @vpre@_set1_@vsuf@(1);
    @type@ tmp;
    @vtype@ v = @vpre@_@VOP@_@vsufs@(@vpre@_load_@vsufs@(&a),
                                     @vpre@_load_@vsufs@(&b));
    v = @vpre@_and_@vsuf@(v, one);
    @vpre@_store_@vsufs@(&tmp, v);
    return tmp;
}

static void
sse2_binary_@kind@_@TYPE@(npy_bool * op, @type@ * ip1, @type@ * ip2, npy_intp n)
{
    LOOP_BLOCK_ALIGN_VAR(ip1, @type@, VECTOR_SIZE_BYTES) {
        op[i] = sse2_ordered_cmp_@kind@_@TYPE@(ip1[i], ip2[i]);
    }
    LOOP_BLOCKED(@type@, 4 * VECTOR_SIZE_BYTES) {
        @vtype@ a1 = @vpre@_load_@vsuf@(&ip1[i + 0 * VECTOR_SIZE_BYTES / sizeof(@type@)]);
        @vtype@ b1 = @vpre@_load_@vsuf@(&ip1[i + 1 * VECTOR_SIZE_BYTES / sizeof(@type@)]);
        @vtype@ c1 = @vpre@_load_@vsuf@(&ip1[i + 2 * VECTOR_SIZE_BYTES / sizeof(@type@)]);
        @vtype@ d1 = @vpre@_load_@vsuf@(&ip1[i + 3 * VECTOR_SIZE_BYTES / sizeof(@type@)]);
        @vtype@ a2 = @vpre@_loadu_@vsuf@(&ip2[i + 0 * VECTOR_SIZE_BYTES / sizeof(@type@)]);
        @vtype@ b2 = @vpre@_loadu_@vsuf@(&ip2[i + 1 * VECTOR_SIZE_BYTES / sizeof(@type@)]);
        @vtype@ c2 = @vpre@_loadu_@vsuf@(&ip2[i + 2 * VECTOR_SIZE_BYTES / sizeof(@type@)]);
        @vtype@ d2 = @vpre@_loadu_@vsuf@(&ip2[i + 3 * VECTOR_SIZE_BYTES / sizeof(@type@)]);
        @vtype@ r1 = @vpre@_@VOP@_@vsuf@(a1, a2);
        @vtype@ r2 = @vpre@_@VOP@_@vsuf@(b1, b2);
        @vtype@ r3 = @vpre@_@VOP@_@vsuf@(c1, c2);
        @vtype@ r4 = @vpre@_@VOP@_@vsuf@(d1, d2);
        sse2_compress4_to_byte_@TYPE@(r1, r2, r3, &r4, &op[i]);
    }
    LOOP_BLOCKED_END {
        op[i] = sse2_ordered_cmp_@kind@_@TYPE@(ip1[i], ip2[i]);
    }
}


static void
sse2_binary_scalar1_@kind@_@TYPE@(npy_bool * op, @type@ * ip1, @type@ * ip2, npy_intp n)
{
    @vtype@ s = @vpre@_set1_@vsuf@(ip1[0]);
    LOOP_BLOCK_ALIGN_VAR(ip2, @type@, VECTOR_SIZE_BYTES) {
        op[i] = sse2_ordered_cmp_@kind@_@TYPE@(ip1[0], ip2[i]);
    }
    LOOP_BLOCKED(@type@, 4 * VECTOR_SIZE_BYTES) {
        @vtype@ a = @vpre@_load_@vsuf@(&ip2[i + 0 * VECTOR_SIZE_BYTES / sizeof(@type@)]);
        @vtype@ b = @vpre@_load_@vsuf@(&ip2[i + 1 * VECTOR_SIZE_BYTES / sizeof(@type@)]);
        @vtype@ c = @vpre@_load_@vsuf@(&ip2[i + 2 * VECTOR_SIZE_BYTES / sizeof(@type@)]);
        @vtype@ d = @vpre@_load_@vsuf@(&ip2[i + 3 * VECTOR_SIZE_BYTES / sizeof(@type@)]);
        @vtype@ r1 = @vpre@_@VOP@_@vsuf@(s, a);
        @vtype@ r2 = @vpre@_@VOP@_@vsuf@(s, b);
        @vtype@ r3 = @vpre@_@VOP@_@vsuf@(s, c);
        @vtype@ r4 = @vpre@_@VOP@_@vsuf@(s, d);
        sse2_compress4_to_byte_@TYPE@(r1, r2, r3, &r4, &op[i]);
    }
    LOOP_BLOCKED_END {
        op[i] = sse2_ordered_cmp_@kind@_@TYPE@(ip1[0], ip2[i]);
    }
}


static void
sse2_binary_scalar2_@kind@_@TYPE@(npy_bool * op, @type@ * ip1, @type@ * ip2, npy_intp n)
{
    @vtype@ s = @vpre@_set1_@vsuf@(ip2[0]);
    LOOP_BLOCK_ALIGN_VAR(ip1, @type@, VECTOR_SIZE_BYTES) {
        op[i] = sse2_ordered_cmp_@kind@_@TYPE@(ip1[i], ip2[0]);
    }
    LOOP_BLOCKED(@type@, 4 * VECTOR_SIZE_BYTES) {
        @vtype@ a = @vpre@_load_@vsuf@(&ip1[i + 0 * VECTOR_SIZE_BYTES / sizeof(@type@)]);
        @vtype@ b = @vpre@_load_@vsuf@(&ip1[i + 1 * VECTOR_SIZE_BYTES / sizeof(@type@)]);
        @vtype@ c = @vpre@_load_@vsuf@(&ip1[i + 2 * VECTOR_SIZE_BYTES / sizeof(@type@)]);
        @vtype@ d = @vpre@_load_@vsuf@(&ip1[i + 3 * VECTOR_SIZE_BYTES / sizeof(@type@)]);
        @vtype@ r1 = @vpre@_@VOP@_@vsuf@(a, s);
        @vtype@ r2 = @vpre@_@VOP@_@vsuf@(b, s);
        @vtype@ r3 = @vpre@_@VOP@_@vsuf@(c, s);
        @vtype@ r4 = @vpre@_@VOP@_@vsuf@(d, s);
        sse2_compress4_to_byte_@TYPE@(r1, r2, r3, &r4, &op[i]);
    }
    LOOP_BLOCKED_END {
        op[i] = sse2_ordered_cmp_@kind@_@TYPE@(ip1[i], ip2[0]);
    }
}
/**end repeat1**/


static void
sse2_negative_@TYPE@(@type@ * op, @type@ * ip, const npy_intp n)
{
    /*
     * get 0x7FFFFFFF mask (everything but signbit set)
     * float & ~mask will remove the sign, float ^ mask flips the sign
     * this is equivalent to how the compiler implements fabs on amd64
     */
    const @vtype@ mask = @vpre@_set1_@vsuf@(-0.@c@);

    /* align output to VECTOR_SIZE_BYTES bytes */
    LOOP_BLOCK_ALIGN_VAR(op, @type@, VECTOR_SIZE_BYTES) {
        op[i] = -ip[i];
    }
    assert((npy_uintp)n < (VECTOR_SIZE_BYTES / sizeof(@type@)) ||
           npy_is_aligned(&op[i], VECTOR_SIZE_BYTES));
    if (npy_is_aligned(&ip[i], VECTOR_SIZE_BYTES)) {
        LOOP_BLOCKED(@type@, VECTOR_SIZE_BYTES) {
            @vtype@ a = @vpre@_load_@vsuf@(&ip[i]);
            @vpre@_store_@vsuf@(&op[i], @vpre@_xor_@vsuf@(mask, a));
        }
    }
    else {
        LOOP_BLOCKED(@type@, VECTOR_SIZE_BYTES) {
            @vtype@ a = @vpre@_loadu_@vsuf@(&ip[i]);
            @vpre@_store_@vsuf@(&op[i], @vpre@_xor_@vsuf@(mask, a));
        }
    }
    LOOP_BLOCKED_END {
        op[i] = -ip[i];
    }
}
/**end repeat1**/


/**begin repeat1
 * #kind = maximum, minimum#
 * #VOP = max, min#
 * #OP = >=, <=#
 **/
/* arguments swapped as unary reduce has the swapped compared to unary */
static void
sse2_@kind@_@TYPE@(@type@ * ip, @type@ * op, const npy_intp n)
{
    const npy_intp stride = VECTOR_SIZE_BYTES / (npy_intp)sizeof(@type@);
    LOOP_BLOCK_ALIGN_VAR(ip, @type@, VECTOR_SIZE_BYTES) {
        /* Order of operations important for MSVC 2015 */
        *op = (*op @OP@ ip[i] || npy_isnan(*op)) ? *op : ip[i];
    }
    assert(n < stride || npy_is_aligned(&ip[i], VECTOR_SIZE_BYTES));
    if (i + 3 * stride <= n) {
        /* load the first elements */
        @vtype@ c1 = @vpre@_load_@vsuf@((@type@*)&ip[i]);
        @vtype@ c2 = @vpre@_load_@vsuf@((@type@*)&ip[i + stride]);
        i += 2 * stride;

        /* minps/minpd will set invalid flag if nan is encountered */
        npy_clear_floatstatus_barrier((char*)&c1);
        LOOP_BLOCKED(@type@, 2 * VECTOR_SIZE_BYTES) {
            @vtype@ v1 = @vpre@_load_@vsuf@((@type@*)&ip[i]);
            @vtype@ v2 = @vpre@_load_@vsuf@((@type@*)&ip[i + stride]);
            c1 = @vpre@_@VOP@_@vsuf@(c1, v1);
            c2 = @vpre@_@VOP@_@vsuf@(c2, v2);
        }
        c1 = @vpre@_@VOP@_@vsuf@(c1, c2);

        if (npy_get_floatstatus_barrier((char*)&c1) & NPY_FPE_INVALID) {
            *op = @nan@;
        }
        else {
            @type@ tmp = sse2_horizontal_@VOP@_@vtype@(c1);
            /* Order of operations important for MSVC 2015 */
            *op  = (*op @OP@ tmp || npy_isnan(*op)) ? *op : tmp;
        }
    }
    LOOP_BLOCKED_END {
        /* Order of operations important for MSVC 2015 */
        *op  = (*op @OP@ ip[i] || npy_isnan(*op)) ? *op : ip[i];
    }
    npy_clear_floatstatus_barrier((char*)op);
}
/**end repeat1**/

/**end repeat**/

/* bunch of helper functions used in ISA_exp/log_FLOAT*/

#if defined HAVE_ATTRIBUTE_TARGET_AVX2_WITH_INTRINSICS
NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_FMA __m256
fma_get_full_load_mask_ps(void)
{
    return _mm256_set1_ps(-1.0);
}

NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_FMA __m256i
fma_get_full_load_mask_pd(void)
{
    return _mm256_castpd_si256(_mm256_set1_pd(-1.0));
}

NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_FMA __m256
fma_get_partial_load_mask_ps(const npy_int num_elem, const npy_int num_lanes)
{
    float maskint[16] = {-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,
                            1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0};
    float* addr = maskint + num_lanes - num_elem;
    return _mm256_loadu_ps(addr);
}

NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_FMA __m256i
fma_get_partial_load_mask_pd(const npy_int num_elem, const npy_int num_lanes)
{
    npy_int maskint[16] = {-1,-1,-1,-1,-1,-1,-1,-1,1,1,1,1,1,1,1,1};
    npy_int* addr = maskint + 2*num_lanes - 2*num_elem;
    return _mm256_loadu_si256((__m256i*) addr);
}

NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_FMA __m256
fma_masked_gather_ps(__m256 src,
                     npy_float* addr,
                     __m256i vindex,
                     __m256 mask)
{
    return _mm256_mask_i32gather_ps(src, addr, vindex, mask, 4);
}

NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_FMA __m256d
fma_masked_gather_pd(__m256d src,
                     npy_double* addr,
                     __m128i vindex,
                     __m256d mask)
{
    return _mm256_mask_i32gather_pd(src, addr, vindex, mask, 8);
}

NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_FMA __m256
fma_masked_load_ps(__m256 mask, npy_float* addr)
{
    return _mm256_maskload_ps(addr, _mm256_cvtps_epi32(mask));
}

NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_FMA __m256d
fma_masked_load_pd(__m256i mask, npy_double* addr)
{
    return _mm256_maskload_pd(addr, mask);
}

NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_FMA __m256
fma_set_masked_lanes_ps(__m256 x, __m256 val, __m256 mask)
{
    return _mm256_blendv_ps(x, val, mask);
}

NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_FMA __m256d
fma_set_masked_lanes_pd(__m256d x, __m256d val, __m256d mask)
{
    return _mm256_blendv_pd(x, val, mask);
}

NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_FMA __m256
fma_blend(__m256 x, __m256 y, __m256 ymask)
{
    return _mm256_blendv_ps(x, y, ymask);
}

NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_FMA __m256
fma_invert_mask_ps(__m256 ymask)
{
    return _mm256_andnot_ps(ymask, _mm256_set1_ps(-1.0));
}

NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_FMA __m256i
fma_invert_mask_pd(__m256i ymask)
{
    return _mm256_andnot_si256(ymask, _mm256_set1_epi32(0xFFFFFFFF));
}

/**begin repeat
 *  #vsub = ps, pd#
 *  #vtype = __m256, __m256d#
 */
NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_FMA @vtype@
fma_abs_@vsub@(@vtype@ x)
{
    return _mm256_andnot_@vsub@(_mm256_set1_@vsub@(-0.0), x);
}

NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_FMA @vtype@
fma_reciprocal_@vsub@(@vtype@ x)
{
    return _mm256_div_@vsub@(_mm256_set1_@vsub@(1.0f), x);
}

NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_FMA @vtype@
fma_rint_@vsub@(@vtype@ x)
{
    return _mm256_round_@vsub@(x, _MM_FROUND_TO_NEAREST_INT);
}

NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_FMA @vtype@
fma_floor_@vsub@(@vtype@ x)
{
    return _mm256_round_@vsub@(x, _MM_FROUND_TO_NEG_INF);
}

NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_FMA @vtype@
fma_trunc_@vsub@(@vtype@ x)
{
    return _mm256_round_@vsub@(x, _MM_FROUND_TO_ZERO);
}
/**end repeat**/
#endif

#if defined HAVE_ATTRIBUTE_TARGET_AVX512F_WITH_INTRINSICS
NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F __mmask16
avx512_get_full_load_mask_ps(void)
{
    return 0xFFFF;
}

NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F __mmask8
avx512_get_full_load_mask_pd(void)
{
    return 0xFF;
}

NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F __mmask16
avx512_get_partial_load_mask_ps(const npy_int num_elem, const npy_int total_elem)
{
    return (0x0001 << num_elem) - 0x0001;
}

NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F __mmask8
avx512_get_partial_load_mask_pd(const npy_int num_elem, const npy_int total_elem)
{
    return (0x01 << num_elem) - 0x01;
}

NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F __m512
avx512_masked_gather_ps(__m512 src,
                        npy_float* addr,
                        __m512i vindex,
                        __mmask16 kmask)
{
    return _mm512_mask_i32gather_ps(src, kmask, vindex, addr, 4);
}

NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F __m512d
avx512_masked_gather_pd(__m512d src,
                        npy_double* addr,
                        __m256i vindex,
                        __mmask8 kmask)
{
    return _mm512_mask_i32gather_pd(src, kmask, vindex, addr, 8);
}

NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F __m512
avx512_masked_load_ps(__mmask16 mask, npy_float* addr)
{
    return _mm512_maskz_loadu_ps(mask, (__m512 *)addr);
}

NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F __m512d
avx512_masked_load_pd(__mmask8 mask, npy_double* addr)
{
    return _mm512_maskz_loadu_pd(mask, (__m512d *)addr);
}

NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F __m512
avx512_set_masked_lanes_ps(__m512 x, __m512 val, __mmask16 mask)
{
    return _mm512_mask_blend_ps(mask, x, val);
}

NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F __m512d
avx512_set_masked_lanes_pd(__m512d x, __m512d val, __mmask8 mask)
{
    return _mm512_mask_blend_pd(mask, x, val);
}

NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F __m512
avx512_blend(__m512 x, __m512 y, __mmask16 ymask)
{
    return _mm512_mask_mov_ps(x, ymask, y);
}

NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F __mmask16
avx512_invert_mask_ps(__mmask16 ymask)
{
    return _mm512_knot(ymask);
}

NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F __mmask8
avx512_invert_mask_pd(__mmask8 ymask)
{
    return _mm512_knot(ymask);
}

/**begin repeat
 *  #vsub  = ps, pd#
 *  #type= npy_float, npy_double#
 *  #epi_vsub  = epi32, epi64#
 *  #vtype = __m512, __m512d#
 *  #mask = __mmask16, __mmask8#
 *  #and_const = 0x7fffffff, 0x7fffffffffffffffLL#
 *  #neg_mask = 0x80000000, 0x8000000000000000#
 *  #perm_ = 0xb1, 0x55#
 *  #cmpx_img_mask = 0xAAAA, 0xAA#
 *  #cmpx_re_mask = 0x5555, 0x55#
 *  #INF = NPY_INFINITYF, NPY_INFINITY#
 *  #NAN = NPY_NANF, NPY_NAN#
 */
NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F @vtype@
avx512_abs_@vsub@(@vtype@ x)
{
    return (@vtype@) _mm512_and_@epi_vsub@((__m512i) x,
				    _mm512_set1_@epi_vsub@ (@and_const@));
}

NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F @vtype@
avx512_reciprocal_@vsub@(@vtype@ x)
{
    return _mm512_div_@vsub@(_mm512_set1_@vsub@(1.0f), x);
}

NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F @vtype@
avx512_rint_@vsub@(@vtype@ x)
{
    return _mm512_roundscale_@vsub@(x, 0x08);
}

NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F @vtype@
avx512_floor_@vsub@(@vtype@ x)
{
    return _mm512_roundscale_@vsub@(x, 0x09);
}

NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F @vtype@
avx512_trunc_@vsub@(@vtype@ x)
{
    return _mm512_roundscale_@vsub@(x, 0x0B);
}

NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F @vtype@
avx512_hadd_@vsub@(const @vtype@ x)
{
    return _mm512_add_@vsub@(x, _mm512_permute_@vsub@(x, @perm_@));
}

NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F @vtype@
avx512_hsub_@vsub@(const @vtype@ x)
{
    return _mm512_sub_@vsub@(x, _mm512_permute_@vsub@(x, @perm_@));
}

NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F @vtype@
avx512_cabsolute_@vsub@(const @vtype@ x1,
                        const @vtype@ x2,
                        const __m512i re_indices,
                        const __m512i im_indices)
{
    @vtype@ inf = _mm512_set1_@vsub@(@INF@);
    @vtype@ nan = _mm512_set1_@vsub@(@NAN@);
    @vtype@ x1_abs = avx512_abs_@vsub@(x1);
    @vtype@ x2_abs = avx512_abs_@vsub@(x2);
    @vtype@ re = _mm512_permutex2var_@vsub@(x1_abs, re_indices, x2_abs);
    @vtype@ im = _mm512_permutex2var_@vsub@(x1_abs, im_indices , x2_abs);
    /*
     * If real or imag = INF, then convert it to inf + j*inf
     * Handles: inf + j*nan, nan + j*inf
     */
    @mask@ re_infmask = _mm512_cmp_@vsub@_mask(re, inf, _CMP_EQ_OQ);
    @mask@ im_infmask = _mm512_cmp_@vsub@_mask(im, inf, _CMP_EQ_OQ);
    im = _mm512_mask_mov_@vsub@(im, re_infmask, inf);
    re = _mm512_mask_mov_@vsub@(re, im_infmask, inf);

    /*
     * If real or imag = NAN, then convert it to nan + j*nan
     * Handles: x + j*nan, nan + j*x
     */
    @mask@ re_nanmask = _mm512_cmp_@vsub@_mask(re, re, _CMP_NEQ_UQ);
    @mask@ im_nanmask = _mm512_cmp_@vsub@_mask(im, im, _CMP_NEQ_UQ);
    im = _mm512_mask_mov_@vsub@(im, re_nanmask, nan);
    re = _mm512_mask_mov_@vsub@(re, im_nanmask, nan);

    @vtype@ larger  = _mm512_max_@vsub@(re, im);
    @vtype@ smaller = _mm512_min_@vsub@(im, re);

    /*
     * Calculate div_mask to prevent 0./0. and inf/inf operations in div
     */
    @mask@ zeromask = _mm512_cmp_@vsub@_mask(larger, _mm512_setzero_@vsub@(), _CMP_EQ_OQ);
    @mask@ infmask = _mm512_cmp_@vsub@_mask(smaller, inf, _CMP_EQ_OQ);
    @mask@ div_mask = _mm512_knot(_mm512_kor(zeromask, infmask));
    @vtype@ ratio = _mm512_maskz_div_@vsub@(div_mask, smaller, larger);
    @vtype@ hypot = _mm512_sqrt_@vsub@(_mm512_fmadd_@vsub@(
                                        ratio, ratio, _mm512_set1_@vsub@(1.0f)));
    return _mm512_mul_@vsub@(hypot, larger);
}

NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F @vtype@
avx512_conjugate_@vsub@(const @vtype@ x)
{
    /*
     * __mm512_mask_xor_ps/pd requires AVX512DQ. We cast it to __m512i and
     * use the xor_epi32/64 uinstruction instead. Cast is a zero latency instruction
     */
    __m512i cast_x = _mm512_cast@vsub@_si512(x);
    __m512i res = _mm512_mask_xor_@epi_vsub@(cast_x, @cmpx_img_mask@,
                                        cast_x, _mm512_set1_@epi_vsub@(@neg_mask@));
    return _mm512_castsi512_@vsub@(res);
}

NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F @vtype@
avx512_cmul_@vsub@(@vtype@ x1, @vtype@ x2)
{
    // x1 = r1, i1
    // x2 = r2, i2
    @vtype@ x3  = _mm512_permute_@vsub@(x2, @perm_@);   // i2, r2
    @vtype@ x12 = _mm512_mul_@vsub@(x1, x2);            // r1*r2, i1*i2
    @vtype@ x13 = _mm512_mul_@vsub@(x1, x3);            // r1*i2, r2*i1
    @vtype@ outreal = avx512_hsub_@vsub@(x12);          // r1*r2 - i1*i2, r1*r2 - i1*i2
    @vtype@ outimg  = avx512_hadd_@vsub@(x13);          // r1*i2 + i1*r2, r1*i2 + i1*r2
    return _mm512_mask_blend_@vsub@(@cmpx_img_mask@, outreal, outimg);
}

NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F @vtype@
avx512_csquare_@vsub@(@vtype@ x)
{
    return avx512_cmul_@vsub@(x, x);
}

/**end repeat**/
#endif

/**begin repeat
 * #ISA = FMA, AVX512F#
 * #isa = fma, avx512#
 * #vtype = __m256, __m512#
 * #vsize = 256, 512#
 * #or = or_ps, kor#
 * #vsub = , _mask#
 * #mask = __m256, __mmask16#
 * #fmadd = _mm256_fmadd_ps, _mm512_fmadd_ps#
 * #CHK = HAVE_ATTRIBUTE_TARGET_AVX2_WITH_INTRINSICS, HAVE_ATTRIBUTE_TARGET_AVX512F_WITH_INTRINSICS#
 **/

#if defined @CHK@

NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_@ISA@ @vtype@
@isa@_sqrt_ps(@vtype@ x)
{
    return _mm@vsize@_sqrt_ps(x);
}

NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_@ISA@ @vtype@d
@isa@_sqrt_pd(@vtype@d x)
{
    return _mm@vsize@_sqrt_pd(x);
}

NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_@ISA@ @vtype@
@isa@_square_ps(@vtype@ x)
{
    return _mm@vsize@_mul_ps(x,x);
}

NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_@ISA@ @vtype@d
@isa@_square_pd(@vtype@d x)
{
    return _mm@vsize@_mul_pd(x,x);
}

#endif
/**end repeat**/

/**begin repeat
 * #type = npy_float, npy_double#
 * #TYPE = FLOAT, DOUBLE#
 * #num_lanes = 16, 8#
 * #vsuffix = ps, pd#
 * #mask = __mmask16, __mmask8#
 * #vtype = __m512, __m512d#
 * #scale = 4, 8#
 * #vindextype = __m512i, __m256i#
 * #vindexload = _mm512_loadu_si512, _mm256_loadu_si256#
 * #episize = epi32, epi64#
 */

/**begin repeat1
 * #func = isnan, isfinite, isinf, signbit#
 * #IMM8 = 0x81, 0x99, 0x18, 0x04#
 * #is_finite = 0, 1, 0, 0#
 * #is_signbit = 0, 0, 0, 1#
 */

#if defined HAVE_ATTRIBUTE_TARGET_AVX512_SKX_WITH_INTRINSICS && defined NPY_HAVE_SSE2_INTRINSICS
static NPY_INLINE NPY_GCC_TARGET_AVX512_SKX void
AVX512_SKX_@func@_@TYPE@(npy_bool* op, @type@* ip, const npy_intp array_size, const npy_intp steps)
{
    const npy_intp stride_ip = steps/(npy_intp)sizeof(@type@);
    npy_intp num_remaining_elements = array_size;

    @mask@ load_mask = avx512_get_full_load_mask_@vsuffix@();
#if @is_signbit@
    @vtype@ signbit = _mm512_set1_@vsuffix@(-0.0);
#endif

    /*
     * Note: while generally indices are npy_intp, we ensure that our maximum
     * index will fit in an int32 as a precondition for this function via
     * IS_OUTPUT_BLOCKABLE_UNARY
     */

    npy_int32 index_ip[@num_lanes@];
    for (npy_int32 ii = 0; ii < @num_lanes@; ii++) {
        index_ip[ii] = ii*stride_ip;
    }
    @vindextype@ vindex_ip = @vindexload@((@vindextype@*)&index_ip[0]);
    @vtype@ zeros_f = _mm512_setzero_@vsuffix@();
    __m512i ones = _mm512_set1_@episize@(1);

    while (num_remaining_elements > 0) {
        if (num_remaining_elements < @num_lanes@) {
            load_mask = avx512_get_partial_load_mask_@vsuffix@(
                                    num_remaining_elements, @num_lanes@);
        }
        @vtype@ x1;
        if (stride_ip == 1) {
            x1 = avx512_masked_load_@vsuffix@(load_mask, ip);
        }
        else {
            x1 = avx512_masked_gather_@vsuffix@(zeros_f, ip, vindex_ip, load_mask);
        }
#if @is_signbit@
        x1 = _mm512_and_@vsuffix@(x1,signbit);
#endif

        @mask@ fpclassmask = _mm512_fpclass_@vsuffix@_mask(x1, @IMM8@);
#if @is_finite@
        fpclassmask = _mm512_knot(fpclassmask);
#endif

        __m128i out =_mm512_maskz_cvts@episize@_epi8(fpclassmask, ones);
        _mm_mask_storeu_epi8(op, load_mask, out);

        ip += @num_lanes@*stride_ip;
        op += @num_lanes@;
        num_remaining_elements -= @num_lanes@;
    }
}
#endif
/**end repeat1**/
/**end repeat**/

/**begin repeat
 * #type = npy_float, npy_double#
 * #TYPE = FLOAT, DOUBLE#
 * #num_lanes = 16, 8#
 * #vsuffix = ps, pd#
 * #mask = __mmask16, __mmask8#
 * #vtype1 = __m512, __m512d#
 * #vtype2 = __m512i, __m256i#
 * #scale = 4, 8#
 * #vindextype = __m512i, __m256i#
 * #vindexsize = 512, 256#
 * #vindexload = _mm512_loadu_si512, _mm256_loadu_si256#
 * #vtype2_load = _mm512_maskz_loadu_epi32, _mm256_maskz_loadu_epi32#
 * #vtype2_gather = _mm512_mask_i32gather_epi32, _mm256_mmask_i32gather_epi32#
 * #vtype2_store = _mm512_mask_storeu_epi32, _mm256_mask_storeu_epi32#
 * #vtype2_scatter = _mm512_mask_i32scatter_epi32, _mm256_mask_i32scatter_epi32#
 * #setzero = _mm512_setzero_epi32, _mm256_setzero_si256#
 */
/**begin repeat1
 *  #func = maximum, minimum#
 *  #vectorf = max, min#
 */

#if defined HAVE_ATTRIBUTE_TARGET_AVX512F_WITH_INTRINSICS && defined NPY_HAVE_SSE2_INTRINSICS
static NPY_INLINE NPY_GCC_TARGET_AVX512F void
AVX512F_@func@_@TYPE@(char **args, npy_intp const *dimensions, npy_intp const *steps)
{
    const npy_intp stride_ip1 = steps[0]/(npy_intp)sizeof(@type@);
    const npy_intp stride_ip2 = steps[1]/(npy_intp)sizeof(@type@);
    const npy_intp stride_op = steps[2]/(npy_intp)sizeof(@type@);
    const npy_intp array_size = dimensions[0];
    npy_intp num_remaining_elements = array_size;
    @type@* ip1 = (@type@*) args[0];
    @type@* ip2 = (@type@*) args[1];
    @type@* op  = (@type@*) args[2];

    @mask@ load_mask = avx512_get_full_load_mask_@vsuffix@();

    /*
     * Note: while generally indices are npy_intp, we ensure that our maximum index
     * will fit in an int32 as a precondition for this function via
     * IS_BINARY_SMALL_STEPS_AND_NOMEMOVERLAP
     */

    npy_int32 index_ip1[@num_lanes@], index_ip2[@num_lanes@], index_op[@num_lanes@];
    for (npy_int32 ii = 0; ii < @num_lanes@; ii++) {
        index_ip1[ii] = ii*stride_ip1;
        index_ip2[ii] = ii*stride_ip2;
        index_op[ii] = ii*stride_op;
    }
    @vindextype@ vindex_ip1 = @vindexload@((@vindextype@*)&index_ip1[0]);
    @vindextype@ vindex_ip2 = @vindexload@((@vindextype@*)&index_ip2[0]);
    @vindextype@ vindex_op  = @vindexload@((@vindextype@*)&index_op[0]);
    @vtype1@ zeros_f = _mm512_setzero_@vsuffix@();

    while (num_remaining_elements > 0) {
        if (num_remaining_elements < @num_lanes@) {
            load_mask = avx512_get_partial_load_mask_@vsuffix@(
                                    num_remaining_elements, @num_lanes@);
        }
        @vtype1@ x1, x2;
        if (stride_ip1 == 1) {
            x1 = avx512_masked_load_@vsuffix@(load_mask, ip1);
        }
        else {
            x1 = avx512_masked_gather_@vsuffix@(zeros_f, ip1, vindex_ip1, load_mask);
        }
        if (stride_ip2 == 1) {
            x2 = avx512_masked_load_@vsuffix@(load_mask, ip2);
        }
        else {
            x2 = avx512_masked_gather_@vsuffix@(zeros_f, ip2, vindex_ip2, load_mask);
        }

        /*
         * when only one of the argument is a nan, the maxps/maxpd instruction
         * returns the second argument. The additional blend instruction fixes
         * this issue to conform with NumPy behaviour.
         */
        @mask@ nan_mask = _mm512_cmp_@vsuffix@_mask(x1, x1, _CMP_NEQ_UQ);
        @vtype1@ out = _mm512_@vectorf@_@vsuffix@(x1, x2);
        out = _mm512_mask_blend_@vsuffix@(nan_mask, out, x1);

        if (stride_op == 1) {
            _mm512_mask_storeu_@vsuffix@(op, load_mask, out);
        }
        else {
            /* scatter! */
            _mm512_mask_i32scatter_@vsuffix@(op, load_mask, vindex_op, out, @scale@);
        }

        ip1 += @num_lanes@*stride_ip1;
        ip2 += @num_lanes@*stride_ip2;
        op += @num_lanes@*stride_op;
        num_remaining_elements -= @num_lanes@;
    }
}
#endif
/**end repeat1**/
/**end repeat**/

/**begin repeat
 * #ISA = FMA, AVX512F#
 * #isa = fma, avx512#
 * #vsize = 256, 512#
 * #BYTES = 32, 64#
 * #cvtps_epi32 = _mm256_cvtps_epi32, #
 * #mask = __m256, __mmask16#
 * #vsub = , _mask#
 * #vtype = __m256, __m512#
 * #cvtps_epi32 = _mm256_cvtps_epi32, #
 * #masked_store = _mm256_maskstore_ps, _mm512_mask_storeu_ps#
 * #CHK = HAVE_ATTRIBUTE_TARGET_AVX2_WITH_INTRINSICS, HAVE_ATTRIBUTE_TARGET_AVX512F_WITH_INTRINSICS#
 */

/**begin repeat1
 *  #func = rint, floor, trunc#
 *  #vectorf = rint, floor, trunc#
 */

#if defined @CHK@
static NPY_INLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_@ISA@ void
@ISA@_@func@_FLOAT(npy_float* op,
                   npy_float* ip,
                   const npy_intp array_size,
                   const npy_intp steps)
{
    const npy_intp stride = steps/(npy_intp)sizeof(npy_float);
    const npy_int num_lanes = @BYTES@/(npy_intp)sizeof(npy_float);
    npy_intp num_remaining_elements = array_size;
    @vtype@ ones_f = _mm@vsize@_set1_ps(1.0f);
    @mask@ load_mask = @isa@_get_full_load_mask_ps();
    /*
     * Note: while generally indices are npy_intp, we ensure that our maximum index
     * will fit in an int32 as a precondition for this function via
     * IS_OUTPUT_BLOCKABLE_UNARY
     */

    npy_int32 indexarr[16];
    for (npy_int32 ii = 0; ii < 16; ii++) {
        indexarr[ii] = ii*stride;
    }
    @vtype@i vindex = _mm@vsize@_loadu_si@vsize@((@vtype@i*)&indexarr[0]);

    while (num_remaining_elements > 0) {
        if (num_remaining_elements < num_lanes) {
            load_mask = @isa@_get_partial_load_mask_ps(num_remaining_elements,
                                                       num_lanes);
        }
        @vtype@ x;
        if (stride == 1) {
            x = @isa@_masked_load_ps(load_mask, ip);
        }
        else {
            x = @isa@_masked_gather_ps(ones_f, ip, vindex, load_mask);
        }
        @vtype@ out = @isa@_@vectorf@_ps(x);
        @masked_store@(op, @cvtps_epi32@(load_mask), out);

        ip += num_lanes*stride;
        op += num_lanes;
        num_remaining_elements -= num_lanes;
    }
}
#endif
/**end repeat1**/
/**end repeat**/

/**begin repeat
 * #ISA = FMA, AVX512F#
 * #isa = fma, avx512#
 * #vsize = 256, 512#
 * #BYTES = 32, 64#
 * #cvtps_epi32 = _mm256_cvtps_epi32, #
 * #mask = __m256i, __mmask8#
 * #vsub = , _mask#
 * #vtype = __m256d, __m512d#
 * #vindextype = __m128i, __m256i#
 * #vindexsize = 128, 256#
 * #vindexload = _mm_loadu_si128, _mm256_loadu_si256#
 * #cvtps_epi32 = _mm256_cvtpd_epi32, #
 * #castmask = _mm256_castsi256_pd, #
 * #masked_store = _mm256_maskstore_pd, _mm512_mask_storeu_pd#
 * #CHK = HAVE_ATTRIBUTE_TARGET_AVX2_WITH_INTRINSICS, HAVE_ATTRIBUTE_TARGET_AVX512F_WITH_INTRINSICS#
 */

/**begin repeat1
 *  #func = rint, floor, trunc#
 *  #vectorf =  rint, floor, trunc#
 */

#if defined @CHK@
static NPY_INLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_@ISA@ void
@ISA@_@func@_DOUBLE(npy_double* op,
                    npy_double* ip,
                    const npy_intp array_size,
                    const npy_intp steps)
{
    const npy_intp stride = steps/(npy_intp)sizeof(npy_double);
    const npy_int num_lanes = @BYTES@/(npy_intp)sizeof(npy_double);
    npy_intp num_remaining_elements = array_size;
    @mask@ load_mask = @isa@_get_full_load_mask_pd();
    @vtype@ ones_d = _mm@vsize@_set1_pd(1.0f);

    /*
     * Note: while generally indices are npy_intp, we ensure that our maximum index
     * will fit in an int32 as a precondition for this function via
     * IS_OUTPUT_BLOCKABLE_UNARY
     */
    npy_int32 indexarr[8];
    for (npy_int32 ii = 0; ii < 8; ii++) {
        indexarr[ii] = ii*stride;
    }
    @vindextype@ vindex = @vindexload@((@vindextype@*)&indexarr[0]);

    while (num_remaining_elements > 0) {
        if (num_remaining_elements < num_lanes) {
            load_mask = @isa@_get_partial_load_mask_pd(num_remaining_elements,
                                                       num_lanes);
        }
        @vtype@ x;
        if (stride == 1) {
            x = @isa@_masked_load_pd(load_mask, ip);
        }
        else {
            x = @isa@_masked_gather_pd(ones_d, ip, vindex, @castmask@(load_mask));
        }
        @vtype@ out = @isa@_@vectorf@_pd(x);
        @masked_store@(op, load_mask, out);

        ip += num_lanes*stride;
        op += num_lanes;
        num_remaining_elements -= num_lanes;
    }
}
#endif
/**end repeat1**/
/**end repeat**/

/**begin repeat
 * #TYPE = CFLOAT, CDOUBLE#
 * #type = npy_float, npy_double#
 * #num_lanes = 16, 8#
 * #vsuffix = ps, pd#
 * #epi_vsub  = epi32, epi64#
 * #mask = __mmask16, __mmask8#
 * #vtype = __m512, __m512d#
 * #scale = 4, 8#
 * #vindextype = __m512i, __m256i#
 * #vindexload = _mm512_loadu_si512, _mm256_loadu_si256#
 * #storemask = 0xFF, 0xF#
 * #IS_FLOAT = 1, 0#
 */

/**begin repeat1
 *  #func = square, conjugate#
 *  #vectorf = avx512_csquare, avx512_conjugate#
 */

#if defined HAVE_ATTRIBUTE_TARGET_AVX512F_WITH_INTRINSICS && defined NPY_HAVE_SSE2_INTRINSICS
static NPY_GCC_OPT_3 NPY_INLINE NPY_GCC_TARGET_AVX512F void
AVX512F_@func@_@TYPE@(@type@ * op,
                      @type@ * ip,
                      const npy_intp array_size,
                      const npy_intp steps)
{
    npy_intp num_remaining_elements = 2*array_size;
    const npy_intp stride_ip1 = steps/(npy_intp)sizeof(@type@)/2;

     /*
      * Note: while generally indices are npy_intp, we ensure that our maximum index
      * will fit in an int32 as a precondition for this function via max_stride
      */
    npy_int32 index_ip1[16];
    for (npy_int32 ii = 0; ii < @num_lanes@; ii=ii+2) {
        index_ip1[ii] = ii*stride_ip1;
        index_ip1[ii+1] = ii*stride_ip1 + 1;
    }
    @vindextype@ vindex = @vindexload@((@vindextype@*)index_ip1);
    @mask@ load_mask = avx512_get_full_load_mask_@vsuffix@();
    @vtype@ zeros = _mm512_setzero_@vsuffix@();

    while (num_remaining_elements > 0) {
        if (num_remaining_elements < @num_lanes@) {
            load_mask = avx512_get_partial_load_mask_@vsuffix@(
                                    num_remaining_elements, @num_lanes@);
        }
        @vtype@ x1;
        if (stride_ip1 == 1) {
            x1 = avx512_masked_load_@vsuffix@(load_mask, ip);
        }
        else {
            x1  = avx512_masked_gather_@vsuffix@(zeros, ip, vindex, load_mask);
        }

        @vtype@ out = @vectorf@_@vsuffix@(x1);

        _mm512_mask_storeu_@vsuffix@(op, load_mask, out);
        op += @num_lanes@;
        ip += @num_lanes@*stride_ip1;
        num_remaining_elements -= @num_lanes@;
    }
}
#endif
/**end repeat1**/

#if defined HAVE_ATTRIBUTE_TARGET_AVX512F_WITH_INTRINSICS && defined NPY_HAVE_SSE2_INTRINSICS
static NPY_GCC_OPT_3 NPY_INLINE NPY_GCC_TARGET_AVX512F void
AVX512F_absolute_@TYPE@(@type@ * op,
                        @type@ * ip,
                        const npy_intp array_size,
                        const npy_intp steps)
{
    npy_intp num_remaining_elements = 2*array_size;
    const npy_intp stride_ip1 = steps/(npy_intp)sizeof(@type@)/2;

    /*
     * Note: while generally indices are npy_intp, we ensure that our maximum index
     * will fit in an int32 as a precondition for this function via max_stride
     */
    npy_int32 index_ip[32];
    for (npy_int32 ii = 0; ii < 2*@num_lanes@; ii=ii+2) {
        index_ip[ii] = ii*stride_ip1;
        index_ip[ii+1] = ii*stride_ip1 + 1;
    }
    @vindextype@ vindex1 = @vindexload@((@vindextype@*)index_ip);
    @vindextype@ vindex2 = @vindexload@((@vindextype@*)(index_ip+@num_lanes@));

    @mask@ load_mask1 = avx512_get_full_load_mask_@vsuffix@();
    @mask@ load_mask2 = avx512_get_full_load_mask_@vsuffix@();
    @mask@ store_mask = avx512_get_full_load_mask_@vsuffix@();
    @vtype@ zeros = _mm512_setzero_@vsuffix@();

#if @IS_FLOAT@
    __m512i re_index = _mm512_set_epi32(30,28,26,24,22,20,18,16,14,12,10,8,6,4,2,0);
    __m512i im_index  = _mm512_set_epi32(31,29,27,25,23,21,19,17,15,13,11,9,7,5,3,1);
#else
    __m512i re_index = _mm512_set_epi64(14,12,10,8,6,4,2,0);
    __m512i im_index  = _mm512_set_epi64(15,13,11,9,7,5,3,1);
#endif

    while (num_remaining_elements > 0) {
        if (num_remaining_elements < @num_lanes@) {
            load_mask1 = avx512_get_partial_load_mask_@vsuffix@(
                                    num_remaining_elements, @num_lanes@);
            load_mask2 = 0x0000;
            store_mask = avx512_get_partial_load_mask_@vsuffix@(
                                    num_remaining_elements/2, @num_lanes@);
        } else if (num_remaining_elements < 2*@num_lanes@) {
            load_mask1 = avx512_get_full_load_mask_@vsuffix@();
            load_mask2 = avx512_get_partial_load_mask_@vsuffix@(
                                    num_remaining_elements - @num_lanes@, @num_lanes@);
            store_mask = avx512_get_partial_load_mask_@vsuffix@(
                                    num_remaining_elements/2, @num_lanes@);
        }
        @vtype@ x1, x2;
        if (stride_ip1 == 1) {
            x1 = avx512_masked_load_@vsuffix@(load_mask1, ip);
            x2 = avx512_masked_load_@vsuffix@(load_mask2, ip+@num_lanes@);
        }
        else {
            x1  = avx512_masked_gather_@vsuffix@(zeros, ip, vindex1, load_mask1);
            x2  = avx512_masked_gather_@vsuffix@(zeros, ip, vindex2, load_mask2);
        }

        @vtype@ out = avx512_cabsolute_@vsuffix@(x1, x2, re_index, im_index);

        _mm512_mask_storeu_@vsuffix@(op, store_mask, out);
        op += @num_lanes@;
        ip += 2*@num_lanes@*stride_ip1;
        num_remaining_elements -= 2*@num_lanes@;
    }
    npy_clear_floatstatus_barrier((char*)&num_remaining_elements);
}

#endif
/**end repeat**/

/*
 *****************************************************************************
 **                           BOOL LOOPS
 *****************************************************************************
 */

/**begin repeat
 * # kind = logical_or, logical_and#
 * # and = 0, 1#
 * # op = ||, &&#
 * # sc = !=, ==#
 * # vpre = _mm*2#
 * # vsuf = si128*2#
 * # vtype = __m128i*2#
 * # type = npy_bool*2#
 * # vload = _mm_load_si128*2#
 * # vloadu = _mm_loadu_si128*2#
 * # vstore = _mm_store_si128*2#
 */

/*
 * convert any bit set to boolean true so vectorized and normal operations are
 * consistent, should not be required if bool is used correctly everywhere but
 * you never know
 */
#if !@and@
NPY_FINLINE @vtype@ byte_to_true(@vtype@ v)
{
    const @vtype@ zero = @vpre@_setzero_@vsuf@();
    const @vtype@ truemask = @vpre@_set1_epi8(1 == 1);
    /* get 0xFF for zeros */
    @vtype@ tmp = @vpre@_cmpeq_epi8(v, zero);
    /* filled with 0xFF/0x00, negate and mask to boolean true */
    return @vpre@_andnot_@vsuf@(tmp, truemask);
}
#endif

static void
sse2_binary_@kind@_BOOL(npy_bool * op, npy_bool * ip1, npy_bool * ip2, npy_intp n)
{
    LOOP_BLOCK_ALIGN_VAR(op, @type@, VECTOR_SIZE_BYTES)
        op[i] = ip1[i] @op@ ip2[i];
    LOOP_BLOCKED(@type@, VECTOR_SIZE_BYTES) {
        @vtype@ a = @vloadu@((@vtype@*)&ip1[i]);
        @vtype@ b = @vloadu@((@vtype@*)&ip2[i]);
#if @and@
        const @vtype@ zero = @vpre@_setzero_@vsuf@();
        /* get 0xFF for non zeros*/
        @vtype@ tmp = @vpre@_cmpeq_epi8(a, zero);
        /* andnot -> 0x00 for zeros xFF for non zeros, & with ip2 */
        tmp = @vpre@_andnot_@vsuf@(tmp, b);
#else
        @vtype@ tmp = @vpre@_or_@vsuf@(a, b);
#endif

        @vstore@((@vtype@*)&op[i], byte_to_true(tmp));
    }
    LOOP_BLOCKED_END {
        op[i] = (ip1[i] @op@ ip2[i]);
    }
}


static void
sse2_reduce_@kind@_BOOL(npy_bool * op, npy_bool * ip, const npy_intp n)
{
    const @vtype@ zero = @vpre@_setzero_@vsuf@();
    LOOP_BLOCK_ALIGN_VAR(ip, npy_bool, VECTOR_SIZE_BYTES) {
        *op = *op @op@ ip[i];
        if (*op @sc@ 0) {
            return;
        }
    }
    /* unrolled once to replace a slow movmsk with a fast pmaxb */
    LOOP_BLOCKED(npy_bool, 2 * VECTOR_SIZE_BYTES) {
        @vtype@ v = @vload@((@vtype@*)&ip[i]);
        @vtype@ v2 = @vload@((@vtype@*)&ip[i + VECTOR_SIZE_BYTES]);
        v = @vpre@_cmpeq_epi8(v, zero);
        v2 = @vpre@_cmpeq_epi8(v2, zero);
#if @and@
        if ((@vpre@_movemask_epi8(@vpre@_max_epu8(v, v2)) != 0)) {
            *op = 0;
#else
        if ((@vpre@_movemask_epi8(@vpre@_min_epu8(v, v2)) != 0xFFFF)) {
            *op = 1;
#endif
            return;
        }
    }
    LOOP_BLOCKED_END {
        *op = *op @op@ ip[i];
        if (*op @sc@ 0) {
            return;
        }
    }
}

/**end repeat**/

/**begin repeat
 * # kind = absolute, logical_not#
 * # op = !=, ==#
 * # not = 0, 1#
 * # vpre = _mm*2#
 * # vsuf = si128*2#
 * # vtype = __m128i*2#
 * # type = npy_bool*2#
 * # vloadu = _mm_loadu_si128*2#
 * # vstore = _mm_store_si128*2#
 */

static void
sse2_@kind@_BOOL(@type@ * op, @type@ * ip, const npy_intp n)
{
    LOOP_BLOCK_ALIGN_VAR(op, @type@, VECTOR_SIZE_BYTES)
        op[i] = (ip[i] @op@ 0);
    LOOP_BLOCKED(@type@, VECTOR_SIZE_BYTES) {
        @vtype@ a = @vloadu@((@vtype@*)&ip[i]);
#if @not@
        const @vtype@ zero = @vpre@_setzero_@vsuf@();
        const @vtype@ truemask = @vpre@_set1_epi8(1 == 1);
        /* equivalent to byte_to_true but can skip the negation */
        a = @vpre@_cmpeq_epi8(a, zero);
        a = @vpre@_and_@vsuf@(a, truemask);
#else
        /* abs is kind of pointless but maybe its used for byte_to_true */
        a = byte_to_true(a);
#endif
        @vstore@((@vtype@*)&op[i], a);
    }
    LOOP_BLOCKED_END {
        op[i] = (ip[i] @op@ 0);
    }
}

/**end repeat**/

#undef VECTOR_SIZE_BYTES
#endif  /* NPY_HAVE_SSE2_INTRINSICS */
#endif
