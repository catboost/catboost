#line 1 "numpy/core/src/umath/loops_utils.h.src"

/*
 *****************************************************************************
 **       This file was autogenerated from a template  DO NOT EDIT!!!!      **
 **       Changes should be made to the original source (.src) file         **
 *****************************************************************************
 */

#line 1
#ifndef _NPY_UMATH_LOOPS_UTILS_H_
#define _NPY_UMATH_LOOPS_UTILS_H_

#include "numpy/npy_common.h" // NPY_FINLINE
#include "numpy/halffloat.h" // npy_half_to_float

/**
 * Old versions of MSVC causes ambiguous link errors when we deal with large SIMD kernels
 * which lead to break the build, probably related to the following bug:
 * https://developercommunity.visualstudio.com/content/problem/415095/internal-compiler-error-with-perfectly-forwarded-r.html
 */
#if defined(_MSC_VER) && _MSC_VER < 1916
    #define SIMD_MSVC_NOINLINE __declspec(noinline)
#else
    #define SIMD_MSVC_NOINLINE
#endif
/*
 * nomemoverlap - returns false if two strided arrays have an overlapping
 * region in memory. ip_size/op_size = size of the arrays which can be negative
 * indicating negative steps.
 */
NPY_FINLINE npy_bool
nomemoverlap(char *ip, npy_intp ip_size, char *op, npy_intp op_size)
{
    char *ip_start, *ip_end, *op_start, *op_end;
    if (ip_size < 0) {
        ip_start = ip + ip_size;
        ip_end = ip;
    }
    else {
        ip_start = ip;
        ip_end = ip + ip_size;
    }
    if (op_size < 0) {
        op_start = op + op_size;
        op_end = op;
    }
    else {
        op_start = op;
        op_end = op + op_size;
    }
    return (ip_start == op_start && op_end == ip_end) ||
           (ip_start > op_end) || (op_start > ip_end);
}

// returns true if two strided arrays have an overlapping region in memory
// same as `nomemoverlap()` but requires array length and step sizes
NPY_FINLINE npy_bool
is_mem_overlap(const void *src, npy_intp src_step, const void *dst, npy_intp dst_step, npy_intp len)
{
    return !(nomemoverlap((char*)src, src_step*len, (char*)dst, dst_step*len));
}

/*
 * cutoff blocksize for pairwise summation
 * decreasing it decreases errors slightly as more pairs are summed but
 * also lowers performance, as the inner loop is unrolled eight times it is
 * effectively 16
 */
#define PW_BLOCKSIZE    128

#line 71

/*
 * Pairwise summation, rounding error O(lg n) instead of O(n).
 * The recursion depth is O(lg n) as well.
 * when updating also update similar complex floats summation
 */
static inline npy_float
FLOAT_pairwise_sum(char *a, npy_intp n, npy_intp stride)
{
    if (n < 8) {
        npy_intp i;
        /*
         * Start with -0 to preserve -0 values.  The reason is that summing
         * only -0 should return -0, but `0 + -0 == 0` while `-0 + -0 == -0`.
         */
        npy_float res = -0.0;

        for (i = 0; i < n; i++) {
            res += (*((npy_float*)(a + i * stride)));
        }
        return res;
    }
    else if (n <= PW_BLOCKSIZE) {
        npy_intp i;
        npy_float r[8], res;

        /*
         * sum a block with 8 accumulators
         * 8 times unroll reduces blocksize to 16 and allows vectorization with
         * avx without changing summation ordering
         */
        r[0] = (*((npy_float *)(a + 0 * stride)));
        r[1] = (*((npy_float *)(a + 1 * stride)));
        r[2] = (*((npy_float *)(a + 2 * stride)));
        r[3] = (*((npy_float *)(a + 3 * stride)));
        r[4] = (*((npy_float *)(a + 4 * stride)));
        r[5] = (*((npy_float *)(a + 5 * stride)));
        r[6] = (*((npy_float *)(a + 6 * stride)));
        r[7] = (*((npy_float *)(a + 7 * stride)));

        for (i = 8; i < n - (n % 8); i += 8) {
            /* small blocksizes seems to mess with hardware prefetch */
            NPY_PREFETCH(a + (i + 512/(npy_intp)sizeof(npy_float))*stride, 0, 3);
            r[0] += (*((npy_float *)(a + (i + 0) * stride)));
            r[1] += (*((npy_float *)(a + (i + 1) * stride)));
            r[2] += (*((npy_float *)(a + (i + 2) * stride)));
            r[3] += (*((npy_float *)(a + (i + 3) * stride)));
            r[4] += (*((npy_float *)(a + (i + 4) * stride)));
            r[5] += (*((npy_float *)(a + (i + 5) * stride)));
            r[6] += (*((npy_float *)(a + (i + 6) * stride)));
            r[7] += (*((npy_float *)(a + (i + 7) * stride)));
        }

        /* accumulate now to avoid stack spills for single peel loop */
        res = ((r[0] + r[1]) + (r[2] + r[3])) +
              ((r[4] + r[5]) + (r[6] + r[7]));

        /* do non multiple of 8 rest */
        for (; i < n; i++) {
            res += (*((npy_float *)(a + i * stride)));
        }
        return res;
    }
    else {
        /* divide by two but avoid non-multiples of unroll factor */
        npy_intp n2 = n / 2;

        n2 -= n2 % 8;
        return FLOAT_pairwise_sum(a, n2, stride) +
               FLOAT_pairwise_sum(a + n2 * stride, n - n2, stride);
    }
}


#line 71

/*
 * Pairwise summation, rounding error O(lg n) instead of O(n).
 * The recursion depth is O(lg n) as well.
 * when updating also update similar complex floats summation
 */
static inline npy_double
DOUBLE_pairwise_sum(char *a, npy_intp n, npy_intp stride)
{
    if (n < 8) {
        npy_intp i;
        /*
         * Start with -0 to preserve -0 values.  The reason is that summing
         * only -0 should return -0, but `0 + -0 == 0` while `-0 + -0 == -0`.
         */
        npy_double res = -0.0;

        for (i = 0; i < n; i++) {
            res += (*((npy_double*)(a + i * stride)));
        }
        return res;
    }
    else if (n <= PW_BLOCKSIZE) {
        npy_intp i;
        npy_double r[8], res;

        /*
         * sum a block with 8 accumulators
         * 8 times unroll reduces blocksize to 16 and allows vectorization with
         * avx without changing summation ordering
         */
        r[0] = (*((npy_double *)(a + 0 * stride)));
        r[1] = (*((npy_double *)(a + 1 * stride)));
        r[2] = (*((npy_double *)(a + 2 * stride)));
        r[3] = (*((npy_double *)(a + 3 * stride)));
        r[4] = (*((npy_double *)(a + 4 * stride)));
        r[5] = (*((npy_double *)(a + 5 * stride)));
        r[6] = (*((npy_double *)(a + 6 * stride)));
        r[7] = (*((npy_double *)(a + 7 * stride)));

        for (i = 8; i < n - (n % 8); i += 8) {
            /* small blocksizes seems to mess with hardware prefetch */
            NPY_PREFETCH(a + (i + 512/(npy_intp)sizeof(npy_double))*stride, 0, 3);
            r[0] += (*((npy_double *)(a + (i + 0) * stride)));
            r[1] += (*((npy_double *)(a + (i + 1) * stride)));
            r[2] += (*((npy_double *)(a + (i + 2) * stride)));
            r[3] += (*((npy_double *)(a + (i + 3) * stride)));
            r[4] += (*((npy_double *)(a + (i + 4) * stride)));
            r[5] += (*((npy_double *)(a + (i + 5) * stride)));
            r[6] += (*((npy_double *)(a + (i + 6) * stride)));
            r[7] += (*((npy_double *)(a + (i + 7) * stride)));
        }

        /* accumulate now to avoid stack spills for single peel loop */
        res = ((r[0] + r[1]) + (r[2] + r[3])) +
              ((r[4] + r[5]) + (r[6] + r[7]));

        /* do non multiple of 8 rest */
        for (; i < n; i++) {
            res += (*((npy_double *)(a + i * stride)));
        }
        return res;
    }
    else {
        /* divide by two but avoid non-multiples of unroll factor */
        npy_intp n2 = n / 2;

        n2 -= n2 % 8;
        return DOUBLE_pairwise_sum(a, n2, stride) +
               DOUBLE_pairwise_sum(a + n2 * stride, n - n2, stride);
    }
}


#line 71

/*
 * Pairwise summation, rounding error O(lg n) instead of O(n).
 * The recursion depth is O(lg n) as well.
 * when updating also update similar complex floats summation
 */
static inline npy_longdouble
LONGDOUBLE_pairwise_sum(char *a, npy_intp n, npy_intp stride)
{
    if (n < 8) {
        npy_intp i;
        /*
         * Start with -0 to preserve -0 values.  The reason is that summing
         * only -0 should return -0, but `0 + -0 == 0` while `-0 + -0 == -0`.
         */
        npy_longdouble res = -0.0;

        for (i = 0; i < n; i++) {
            res += (*((npy_longdouble*)(a + i * stride)));
        }
        return res;
    }
    else if (n <= PW_BLOCKSIZE) {
        npy_intp i;
        npy_longdouble r[8], res;

        /*
         * sum a block with 8 accumulators
         * 8 times unroll reduces blocksize to 16 and allows vectorization with
         * avx without changing summation ordering
         */
        r[0] = (*((npy_longdouble *)(a + 0 * stride)));
        r[1] = (*((npy_longdouble *)(a + 1 * stride)));
        r[2] = (*((npy_longdouble *)(a + 2 * stride)));
        r[3] = (*((npy_longdouble *)(a + 3 * stride)));
        r[4] = (*((npy_longdouble *)(a + 4 * stride)));
        r[5] = (*((npy_longdouble *)(a + 5 * stride)));
        r[6] = (*((npy_longdouble *)(a + 6 * stride)));
        r[7] = (*((npy_longdouble *)(a + 7 * stride)));

        for (i = 8; i < n - (n % 8); i += 8) {
            /* small blocksizes seems to mess with hardware prefetch */
            NPY_PREFETCH(a + (i + 512/(npy_intp)sizeof(npy_longdouble))*stride, 0, 3);
            r[0] += (*((npy_longdouble *)(a + (i + 0) * stride)));
            r[1] += (*((npy_longdouble *)(a + (i + 1) * stride)));
            r[2] += (*((npy_longdouble *)(a + (i + 2) * stride)));
            r[3] += (*((npy_longdouble *)(a + (i + 3) * stride)));
            r[4] += (*((npy_longdouble *)(a + (i + 4) * stride)));
            r[5] += (*((npy_longdouble *)(a + (i + 5) * stride)));
            r[6] += (*((npy_longdouble *)(a + (i + 6) * stride)));
            r[7] += (*((npy_longdouble *)(a + (i + 7) * stride)));
        }

        /* accumulate now to avoid stack spills for single peel loop */
        res = ((r[0] + r[1]) + (r[2] + r[3])) +
              ((r[4] + r[5]) + (r[6] + r[7]));

        /* do non multiple of 8 rest */
        for (; i < n; i++) {
            res += (*((npy_longdouble *)(a + i * stride)));
        }
        return res;
    }
    else {
        /* divide by two but avoid non-multiples of unroll factor */
        npy_intp n2 = n / 2;

        n2 -= n2 % 8;
        return LONGDOUBLE_pairwise_sum(a, n2, stride) +
               LONGDOUBLE_pairwise_sum(a + n2 * stride, n - n2, stride);
    }
}


#line 71

/*
 * Pairwise summation, rounding error O(lg n) instead of O(n).
 * The recursion depth is O(lg n) as well.
 * when updating also update similar complex floats summation
 */
static inline npy_float
HALF_pairwise_sum(char *a, npy_intp n, npy_intp stride)
{
    if (n < 8) {
        npy_intp i;
        /*
         * Start with -0 to preserve -0 values.  The reason is that summing
         * only -0 should return -0, but `0 + -0 == 0` while `-0 + -0 == -0`.
         */
        npy_float res = -0.0;

        for (i = 0; i < n; i++) {
            res += npy_half_to_float(*((npy_half*)(a + i * stride)));
        }
        return res;
    }
    else if (n <= PW_BLOCKSIZE) {
        npy_intp i;
        npy_float r[8], res;

        /*
         * sum a block with 8 accumulators
         * 8 times unroll reduces blocksize to 16 and allows vectorization with
         * avx without changing summation ordering
         */
        r[0] = npy_half_to_float(*((npy_half *)(a + 0 * stride)));
        r[1] = npy_half_to_float(*((npy_half *)(a + 1 * stride)));
        r[2] = npy_half_to_float(*((npy_half *)(a + 2 * stride)));
        r[3] = npy_half_to_float(*((npy_half *)(a + 3 * stride)));
        r[4] = npy_half_to_float(*((npy_half *)(a + 4 * stride)));
        r[5] = npy_half_to_float(*((npy_half *)(a + 5 * stride)));
        r[6] = npy_half_to_float(*((npy_half *)(a + 6 * stride)));
        r[7] = npy_half_to_float(*((npy_half *)(a + 7 * stride)));

        for (i = 8; i < n - (n % 8); i += 8) {
            /* small blocksizes seems to mess with hardware prefetch */
            NPY_PREFETCH(a + (i + 512/(npy_intp)sizeof(npy_half))*stride, 0, 3);
            r[0] += npy_half_to_float(*((npy_half *)(a + (i + 0) * stride)));
            r[1] += npy_half_to_float(*((npy_half *)(a + (i + 1) * stride)));
            r[2] += npy_half_to_float(*((npy_half *)(a + (i + 2) * stride)));
            r[3] += npy_half_to_float(*((npy_half *)(a + (i + 3) * stride)));
            r[4] += npy_half_to_float(*((npy_half *)(a + (i + 4) * stride)));
            r[5] += npy_half_to_float(*((npy_half *)(a + (i + 5) * stride)));
            r[6] += npy_half_to_float(*((npy_half *)(a + (i + 6) * stride)));
            r[7] += npy_half_to_float(*((npy_half *)(a + (i + 7) * stride)));
        }

        /* accumulate now to avoid stack spills for single peel loop */
        res = ((r[0] + r[1]) + (r[2] + r[3])) +
              ((r[4] + r[5]) + (r[6] + r[7]));

        /* do non multiple of 8 rest */
        for (; i < n; i++) {
            res += npy_half_to_float(*((npy_half *)(a + i * stride)));
        }
        return res;
    }
    else {
        /* divide by two but avoid non-multiples of unroll factor */
        npy_intp n2 = n / 2;

        n2 -= n2 % 8;
        return HALF_pairwise_sum(a, n2, stride) +
               HALF_pairwise_sum(a + n2 * stride, n - n2, stride);
    }
}



#line 154
/* similar to pairwise sum of real floats */
static inline void
CFLOAT_pairwise_sum(npy_float *rr, npy_float * ri, char * a, npy_intp n,
                    npy_intp stride)
{
    assert(n % 2 == 0);
    if (n < 8) {
        npy_intp i;

        *rr = -0.0;
        *ri = -0.0;
        for (i = 0; i < n; i += 2) {
            *rr += *((npy_float *)(a + i * stride + 0));
            *ri += *((npy_float *)(a + i * stride + sizeof(npy_float)));
        }
        return;
    }
    else if (n <= PW_BLOCKSIZE) {
        npy_intp i;
        npy_float r[8];

        /*
         * sum a block with 8 accumulators
         * 8 times unroll reduces blocksize to 16 and allows vectorization with
         * avx without changing summation ordering
         */
        r[0] = *((npy_float *)(a + 0 * stride));
        r[1] = *((npy_float *)(a + 0 * stride + sizeof(npy_float)));
        r[2] = *((npy_float *)(a + 2 * stride));
        r[3] = *((npy_float *)(a + 2 * stride + sizeof(npy_float)));
        r[4] = *((npy_float *)(a + 4 * stride));
        r[5] = *((npy_float *)(a + 4 * stride + sizeof(npy_float)));
        r[6] = *((npy_float *)(a + 6 * stride));
        r[7] = *((npy_float *)(a + 6 * stride + sizeof(npy_float)));

        for (i = 8; i < n - (n % 8); i += 8) {
            /* small blocksizes seems to mess with hardware prefetch */
            NPY_PREFETCH(a + (i + 512/(npy_intp)sizeof(npy_float))*stride, 0, 3);
            r[0] += *((npy_float *)(a + (i + 0) * stride));
            r[1] += *((npy_float *)(a + (i + 0) * stride + sizeof(npy_float)));
            r[2] += *((npy_float *)(a + (i + 2) * stride));
            r[3] += *((npy_float *)(a + (i + 2) * stride + sizeof(npy_float)));
            r[4] += *((npy_float *)(a + (i + 4) * stride));
            r[5] += *((npy_float *)(a + (i + 4) * stride + sizeof(npy_float)));
            r[6] += *((npy_float *)(a + (i + 6) * stride));
            r[7] += *((npy_float *)(a + (i + 6) * stride + sizeof(npy_float)));
        }

        /* accumulate now to avoid stack spills for single peel loop */
        *rr = ((r[0] + r[2]) + (r[4] + r[6]));
        *ri = ((r[1] + r[3]) + (r[5] + r[7]));

        /* do non multiple of 8 rest */
        for (; i < n; i+=2) {
            *rr += *((npy_float *)(a + i * stride + 0));
            *ri += *((npy_float *)(a + i * stride + sizeof(npy_float)));
        }
        return;
    }
    else {
        /* divide by two but avoid non-multiples of unroll factor */
        npy_float rr1, ri1, rr2, ri2;
        npy_intp n2 = n / 2;

        n2 -= n2 % 8;
        CFLOAT_pairwise_sum(&rr1, &ri1, a, n2, stride);
        CFLOAT_pairwise_sum(&rr2, &ri2, a + n2 * stride, n - n2, stride);
        *rr = rr1 + rr2;
        *ri = ri1 + ri2;
        return;
    }
}

#line 154
/* similar to pairwise sum of real floats */
static inline void
CDOUBLE_pairwise_sum(npy_double *rr, npy_double * ri, char * a, npy_intp n,
                    npy_intp stride)
{
    assert(n % 2 == 0);
    if (n < 8) {
        npy_intp i;

        *rr = -0.0;
        *ri = -0.0;
        for (i = 0; i < n; i += 2) {
            *rr += *((npy_double *)(a + i * stride + 0));
            *ri += *((npy_double *)(a + i * stride + sizeof(npy_double)));
        }
        return;
    }
    else if (n <= PW_BLOCKSIZE) {
        npy_intp i;
        npy_double r[8];

        /*
         * sum a block with 8 accumulators
         * 8 times unroll reduces blocksize to 16 and allows vectorization with
         * avx without changing summation ordering
         */
        r[0] = *((npy_double *)(a + 0 * stride));
        r[1] = *((npy_double *)(a + 0 * stride + sizeof(npy_double)));
        r[2] = *((npy_double *)(a + 2 * stride));
        r[3] = *((npy_double *)(a + 2 * stride + sizeof(npy_double)));
        r[4] = *((npy_double *)(a + 4 * stride));
        r[5] = *((npy_double *)(a + 4 * stride + sizeof(npy_double)));
        r[6] = *((npy_double *)(a + 6 * stride));
        r[7] = *((npy_double *)(a + 6 * stride + sizeof(npy_double)));

        for (i = 8; i < n - (n % 8); i += 8) {
            /* small blocksizes seems to mess with hardware prefetch */
            NPY_PREFETCH(a + (i + 512/(npy_intp)sizeof(npy_double))*stride, 0, 3);
            r[0] += *((npy_double *)(a + (i + 0) * stride));
            r[1] += *((npy_double *)(a + (i + 0) * stride + sizeof(npy_double)));
            r[2] += *((npy_double *)(a + (i + 2) * stride));
            r[3] += *((npy_double *)(a + (i + 2) * stride + sizeof(npy_double)));
            r[4] += *((npy_double *)(a + (i + 4) * stride));
            r[5] += *((npy_double *)(a + (i + 4) * stride + sizeof(npy_double)));
            r[6] += *((npy_double *)(a + (i + 6) * stride));
            r[7] += *((npy_double *)(a + (i + 6) * stride + sizeof(npy_double)));
        }

        /* accumulate now to avoid stack spills for single peel loop */
        *rr = ((r[0] + r[2]) + (r[4] + r[6]));
        *ri = ((r[1] + r[3]) + (r[5] + r[7]));

        /* do non multiple of 8 rest */
        for (; i < n; i+=2) {
            *rr += *((npy_double *)(a + i * stride + 0));
            *ri += *((npy_double *)(a + i * stride + sizeof(npy_double)));
        }
        return;
    }
    else {
        /* divide by two but avoid non-multiples of unroll factor */
        npy_double rr1, ri1, rr2, ri2;
        npy_intp n2 = n / 2;

        n2 -= n2 % 8;
        CDOUBLE_pairwise_sum(&rr1, &ri1, a, n2, stride);
        CDOUBLE_pairwise_sum(&rr2, &ri2, a + n2 * stride, n - n2, stride);
        *rr = rr1 + rr2;
        *ri = ri1 + ri2;
        return;
    }
}

#line 154
/* similar to pairwise sum of real floats */
static inline void
CLONGDOUBLE_pairwise_sum(npy_longdouble *rr, npy_longdouble * ri, char * a, npy_intp n,
                    npy_intp stride)
{
    assert(n % 2 == 0);
    if (n < 8) {
        npy_intp i;

        *rr = -0.0;
        *ri = -0.0;
        for (i = 0; i < n; i += 2) {
            *rr += *((npy_longdouble *)(a + i * stride + 0));
            *ri += *((npy_longdouble *)(a + i * stride + sizeof(npy_longdouble)));
        }
        return;
    }
    else if (n <= PW_BLOCKSIZE) {
        npy_intp i;
        npy_longdouble r[8];

        /*
         * sum a block with 8 accumulators
         * 8 times unroll reduces blocksize to 16 and allows vectorization with
         * avx without changing summation ordering
         */
        r[0] = *((npy_longdouble *)(a + 0 * stride));
        r[1] = *((npy_longdouble *)(a + 0 * stride + sizeof(npy_longdouble)));
        r[2] = *((npy_longdouble *)(a + 2 * stride));
        r[3] = *((npy_longdouble *)(a + 2 * stride + sizeof(npy_longdouble)));
        r[4] = *((npy_longdouble *)(a + 4 * stride));
        r[5] = *((npy_longdouble *)(a + 4 * stride + sizeof(npy_longdouble)));
        r[6] = *((npy_longdouble *)(a + 6 * stride));
        r[7] = *((npy_longdouble *)(a + 6 * stride + sizeof(npy_longdouble)));

        for (i = 8; i < n - (n % 8); i += 8) {
            /* small blocksizes seems to mess with hardware prefetch */
            NPY_PREFETCH(a + (i + 512/(npy_intp)sizeof(npy_longdouble))*stride, 0, 3);
            r[0] += *((npy_longdouble *)(a + (i + 0) * stride));
            r[1] += *((npy_longdouble *)(a + (i + 0) * stride + sizeof(npy_longdouble)));
            r[2] += *((npy_longdouble *)(a + (i + 2) * stride));
            r[3] += *((npy_longdouble *)(a + (i + 2) * stride + sizeof(npy_longdouble)));
            r[4] += *((npy_longdouble *)(a + (i + 4) * stride));
            r[5] += *((npy_longdouble *)(a + (i + 4) * stride + sizeof(npy_longdouble)));
            r[6] += *((npy_longdouble *)(a + (i + 6) * stride));
            r[7] += *((npy_longdouble *)(a + (i + 6) * stride + sizeof(npy_longdouble)));
        }

        /* accumulate now to avoid stack spills for single peel loop */
        *rr = ((r[0] + r[2]) + (r[4] + r[6]));
        *ri = ((r[1] + r[3]) + (r[5] + r[7]));

        /* do non multiple of 8 rest */
        for (; i < n; i+=2) {
            *rr += *((npy_longdouble *)(a + i * stride + 0));
            *ri += *((npy_longdouble *)(a + i * stride + sizeof(npy_longdouble)));
        }
        return;
    }
    else {
        /* divide by two but avoid non-multiples of unroll factor */
        npy_longdouble rr1, ri1, rr2, ri2;
        npy_intp n2 = n / 2;

        n2 -= n2 % 8;
        CLONGDOUBLE_pairwise_sum(&rr1, &ri1, a, n2, stride);
        CLONGDOUBLE_pairwise_sum(&rr2, &ri2, a + n2 * stride, n - n2, stride);
        *rr = rr1 + rr2;
        *ri = ri1 + ri2;
        return;
    }
}


#endif // _NPY_UMATH_LOOPS_UTILS_H_

