#line 1 "numpy/core/src/umath/simd.inc.src"

/*
 *****************************************************************************
 **       This file was autogenerated from a template  DO NOT EDIT!!!!      **
 **       Changes should be made to the original source (.src) file         **
 *****************************************************************************
 */

#line 1


/*
 * This file is for the definitions of simd vectorized operations.
 *
 * Currently contains sse2 functions that are built on amd64, x32 or
 * non-generic builds (CFLAGS=-march=...)
 * In future it may contain other instruction sets like AVX or NEON detected
 * at runtime in which case it needs to be included indirectly via a file
 * compiled with special options (or use gcc target attributes) so the binary
 * stays portable.
 */


#ifndef __NPY_SIMD_INC
#define __NPY_SIMD_INC

#include "lowlevel_strided_loops.h"
#include "numpy/npy_common.h"
#include "numpy/npy_math.h"
#include "npy_simd_data.h"
#ifdef NPY_HAVE_SSE2_INTRINSICS
#include <emmintrin.h>
#if !defined(_MSC_VER) || _MSC_VER >= 1600
#include <immintrin.h>
#else
#undef __AVX2__
#undef __AVX512F__
#endif
#endif
#include "loops_utils.h" // nomemoverlap
#include <assert.h>
#include <stdlib.h>
#include <float.h>
#include <string.h> /* for memcpy */

#define VECTOR_SIZE_BYTES 16

/*
 * Dispatcher functions
 * decide whether the operation can be vectorized and run it
 * if it was run returns true and false if nothing was done
 */

/*
 *****************************************************************************
 **                           CMPLX DISPATCHERS
 *****************************************************************************
 */

#line 56

#line 62

#if defined HAVE_ATTRIBUTE_TARGET_AVX512F_WITH_INTRINSICS && defined NPY_HAVE_SSE2_INTRINSICS
static NPY_INLINE NPY_GCC_TARGET_AVX512F void
AVX512F_square_CFLOAT(npy_float*, npy_float*, const npy_intp n, const npy_intp stride);
#endif

static NPY_INLINE int
run_unary_avx512f_square_CFLOAT(char **args, const npy_intp *dimensions, const npy_intp *steps)
{
#if defined HAVE_ATTRIBUTE_TARGET_AVX512F_WITH_INTRINSICS && defined NPY_HAVE_SSE2_INTRINSICS
    if ((IS_OUTPUT_BLOCKABLE_UNARY(8, (npy_uint)(8/1), 64)) && (labs(steps[0]) < 2*2*8)) {
        AVX512F_square_CFLOAT((npy_float*)args[1], (npy_float*)args[0], dimensions[0], steps[0]);
        return 1;
    }
    else
        return 0;
#endif
    return 0;
}


#line 62

#if defined HAVE_ATTRIBUTE_TARGET_AVX512F_WITH_INTRINSICS && defined NPY_HAVE_SSE2_INTRINSICS
static NPY_INLINE NPY_GCC_TARGET_AVX512F void
AVX512F_absolute_CFLOAT(npy_float*, npy_float*, const npy_intp n, const npy_intp stride);
#endif

static NPY_INLINE int
run_unary_avx512f_absolute_CFLOAT(char **args, const npy_intp *dimensions, const npy_intp *steps)
{
#if defined HAVE_ATTRIBUTE_TARGET_AVX512F_WITH_INTRINSICS && defined NPY_HAVE_SSE2_INTRINSICS
    if ((IS_OUTPUT_BLOCKABLE_UNARY(8, (npy_uint)(8/2), 64)) && (labs(steps[0]) < 2*8*8)) {
        AVX512F_absolute_CFLOAT((npy_float*)args[1], (npy_float*)args[0], dimensions[0], steps[0]);
        return 1;
    }
    else
        return 0;
#endif
    return 0;
}


#line 62

#if defined HAVE_ATTRIBUTE_TARGET_AVX512F_WITH_INTRINSICS && defined NPY_HAVE_SSE2_INTRINSICS
static NPY_INLINE NPY_GCC_TARGET_AVX512F void
AVX512F_conjugate_CFLOAT(npy_float*, npy_float*, const npy_intp n, const npy_intp stride);
#endif

static NPY_INLINE int
run_unary_avx512f_conjugate_CFLOAT(char **args, const npy_intp *dimensions, const npy_intp *steps)
{
#if defined HAVE_ATTRIBUTE_TARGET_AVX512F_WITH_INTRINSICS && defined NPY_HAVE_SSE2_INTRINSICS
    if ((IS_OUTPUT_BLOCKABLE_UNARY(8, (npy_uint)(8/1), 64)) && (labs(steps[0]) < 2*8*8)) {
        AVX512F_conjugate_CFLOAT((npy_float*)args[1], (npy_float*)args[0], dimensions[0], steps[0]);
        return 1;
    }
    else
        return 0;
#endif
    return 0;
}



#line 56

#line 62

#if defined HAVE_ATTRIBUTE_TARGET_AVX512F_WITH_INTRINSICS && defined NPY_HAVE_SSE2_INTRINSICS
static NPY_INLINE NPY_GCC_TARGET_AVX512F void
AVX512F_square_CDOUBLE(npy_double*, npy_double*, const npy_intp n, const npy_intp stride);
#endif

static NPY_INLINE int
run_unary_avx512f_square_CDOUBLE(char **args, const npy_intp *dimensions, const npy_intp *steps)
{
#if defined HAVE_ATTRIBUTE_TARGET_AVX512F_WITH_INTRINSICS && defined NPY_HAVE_SSE2_INTRINSICS
    if ((IS_OUTPUT_BLOCKABLE_UNARY(16, (npy_uint)(16/1), 64)) && (labs(steps[0]) < 2*2*16)) {
        AVX512F_square_CDOUBLE((npy_double*)args[1], (npy_double*)args[0], dimensions[0], steps[0]);
        return 1;
    }
    else
        return 0;
#endif
    return 0;
}


#line 62

#if defined HAVE_ATTRIBUTE_TARGET_AVX512F_WITH_INTRINSICS && defined NPY_HAVE_SSE2_INTRINSICS
static NPY_INLINE NPY_GCC_TARGET_AVX512F void
AVX512F_absolute_CDOUBLE(npy_double*, npy_double*, const npy_intp n, const npy_intp stride);
#endif

static NPY_INLINE int
run_unary_avx512f_absolute_CDOUBLE(char **args, const npy_intp *dimensions, const npy_intp *steps)
{
#if defined HAVE_ATTRIBUTE_TARGET_AVX512F_WITH_INTRINSICS && defined NPY_HAVE_SSE2_INTRINSICS
    if ((IS_OUTPUT_BLOCKABLE_UNARY(16, (npy_uint)(16/2), 64)) && (labs(steps[0]) < 2*8*16)) {
        AVX512F_absolute_CDOUBLE((npy_double*)args[1], (npy_double*)args[0], dimensions[0], steps[0]);
        return 1;
    }
    else
        return 0;
#endif
    return 0;
}


#line 62

#if defined HAVE_ATTRIBUTE_TARGET_AVX512F_WITH_INTRINSICS && defined NPY_HAVE_SSE2_INTRINSICS
static NPY_INLINE NPY_GCC_TARGET_AVX512F void
AVX512F_conjugate_CDOUBLE(npy_double*, npy_double*, const npy_intp n, const npy_intp stride);
#endif

static NPY_INLINE int
run_unary_avx512f_conjugate_CDOUBLE(char **args, const npy_intp *dimensions, const npy_intp *steps)
{
#if defined HAVE_ATTRIBUTE_TARGET_AVX512F_WITH_INTRINSICS && defined NPY_HAVE_SSE2_INTRINSICS
    if ((IS_OUTPUT_BLOCKABLE_UNARY(16, (npy_uint)(16/1), 64)) && (labs(steps[0]) < 2*8*16)) {
        AVX512F_conjugate_CDOUBLE((npy_double*)args[1], (npy_double*)args[0], dimensions[0], steps[0]);
        return 1;
    }
    else
        return 0;
#endif
    return 0;
}




/*
 *****************************************************************************
 **                           FLOAT DISPATCHERS
 *****************************************************************************
 */

#line 96

#line 100

#if defined HAVE_ATTRIBUTE_TARGET_AVX512_SKX_WITH_INTRINSICS && defined NPY_HAVE_SSE2_INTRINSICS && 1
static NPY_INLINE NPY_GCC_TARGET_AVX512_SKX void
AVX512_SKX_isnan_FLOAT(npy_bool*, npy_float*, const npy_intp n, const npy_intp stride);
#endif

static NPY_INLINE int
run_isnan_avx512_skx_FLOAT(char **args, npy_intp const *dimensions, npy_intp const *steps)
{
#if defined HAVE_ATTRIBUTE_TARGET_AVX512_SKX_WITH_INTRINSICS && defined NPY_HAVE_SSE2_INTRINSICS && 1
    if (IS_OUTPUT_BLOCKABLE_UNARY(sizeof(npy_float), sizeof(npy_bool), 64)) {
        AVX512_SKX_isnan_FLOAT((npy_bool*)args[1], (npy_float*)args[0], dimensions[0], steps[0]);
        return 1;
    }
    else {
        return 0;
    }
#endif
    return 0;
}



#line 100

#if defined HAVE_ATTRIBUTE_TARGET_AVX512_SKX_WITH_INTRINSICS && defined NPY_HAVE_SSE2_INTRINSICS && 1
static NPY_INLINE NPY_GCC_TARGET_AVX512_SKX void
AVX512_SKX_isfinite_FLOAT(npy_bool*, npy_float*, const npy_intp n, const npy_intp stride);
#endif

static NPY_INLINE int
run_isfinite_avx512_skx_FLOAT(char **args, npy_intp const *dimensions, npy_intp const *steps)
{
#if defined HAVE_ATTRIBUTE_TARGET_AVX512_SKX_WITH_INTRINSICS && defined NPY_HAVE_SSE2_INTRINSICS && 1
    if (IS_OUTPUT_BLOCKABLE_UNARY(sizeof(npy_float), sizeof(npy_bool), 64)) {
        AVX512_SKX_isfinite_FLOAT((npy_bool*)args[1], (npy_float*)args[0], dimensions[0], steps[0]);
        return 1;
    }
    else {
        return 0;
    }
#endif
    return 0;
}



#line 100

#if defined HAVE_ATTRIBUTE_TARGET_AVX512_SKX_WITH_INTRINSICS && defined NPY_HAVE_SSE2_INTRINSICS && 1
static NPY_INLINE NPY_GCC_TARGET_AVX512_SKX void
AVX512_SKX_isinf_FLOAT(npy_bool*, npy_float*, const npy_intp n, const npy_intp stride);
#endif

static NPY_INLINE int
run_isinf_avx512_skx_FLOAT(char **args, npy_intp const *dimensions, npy_intp const *steps)
{
#if defined HAVE_ATTRIBUTE_TARGET_AVX512_SKX_WITH_INTRINSICS && defined NPY_HAVE_SSE2_INTRINSICS && 1
    if (IS_OUTPUT_BLOCKABLE_UNARY(sizeof(npy_float), sizeof(npy_bool), 64)) {
        AVX512_SKX_isinf_FLOAT((npy_bool*)args[1], (npy_float*)args[0], dimensions[0], steps[0]);
        return 1;
    }
    else {
        return 0;
    }
#endif
    return 0;
}



#line 100

#if defined HAVE_ATTRIBUTE_TARGET_AVX512_SKX_WITH_INTRINSICS && defined NPY_HAVE_SSE2_INTRINSICS && 1
static NPY_INLINE NPY_GCC_TARGET_AVX512_SKX void
AVX512_SKX_signbit_FLOAT(npy_bool*, npy_float*, const npy_intp n, const npy_intp stride);
#endif

static NPY_INLINE int
run_signbit_avx512_skx_FLOAT(char **args, npy_intp const *dimensions, npy_intp const *steps)
{
#if defined HAVE_ATTRIBUTE_TARGET_AVX512_SKX_WITH_INTRINSICS && defined NPY_HAVE_SSE2_INTRINSICS && 1
    if (IS_OUTPUT_BLOCKABLE_UNARY(sizeof(npy_float), sizeof(npy_bool), 64)) {
        AVX512_SKX_signbit_FLOAT((npy_bool*)args[1], (npy_float*)args[0], dimensions[0], steps[0]);
        return 1;
    }
    else {
        return 0;
    }
#endif
    return 0;
}




#line 96

#line 100

#if defined HAVE_ATTRIBUTE_TARGET_AVX512_SKX_WITH_INTRINSICS && defined NPY_HAVE_SSE2_INTRINSICS && 1
static NPY_INLINE NPY_GCC_TARGET_AVX512_SKX void
AVX512_SKX_isnan_DOUBLE(npy_bool*, npy_double*, const npy_intp n, const npy_intp stride);
#endif

static NPY_INLINE int
run_isnan_avx512_skx_DOUBLE(char **args, npy_intp const *dimensions, npy_intp const *steps)
{
#if defined HAVE_ATTRIBUTE_TARGET_AVX512_SKX_WITH_INTRINSICS && defined NPY_HAVE_SSE2_INTRINSICS && 1
    if (IS_OUTPUT_BLOCKABLE_UNARY(sizeof(npy_double), sizeof(npy_bool), 64)) {
        AVX512_SKX_isnan_DOUBLE((npy_bool*)args[1], (npy_double*)args[0], dimensions[0], steps[0]);
        return 1;
    }
    else {
        return 0;
    }
#endif
    return 0;
}



#line 100

#if defined HAVE_ATTRIBUTE_TARGET_AVX512_SKX_WITH_INTRINSICS && defined NPY_HAVE_SSE2_INTRINSICS && 1
static NPY_INLINE NPY_GCC_TARGET_AVX512_SKX void
AVX512_SKX_isfinite_DOUBLE(npy_bool*, npy_double*, const npy_intp n, const npy_intp stride);
#endif

static NPY_INLINE int
run_isfinite_avx512_skx_DOUBLE(char **args, npy_intp const *dimensions, npy_intp const *steps)
{
#if defined HAVE_ATTRIBUTE_TARGET_AVX512_SKX_WITH_INTRINSICS && defined NPY_HAVE_SSE2_INTRINSICS && 1
    if (IS_OUTPUT_BLOCKABLE_UNARY(sizeof(npy_double), sizeof(npy_bool), 64)) {
        AVX512_SKX_isfinite_DOUBLE((npy_bool*)args[1], (npy_double*)args[0], dimensions[0], steps[0]);
        return 1;
    }
    else {
        return 0;
    }
#endif
    return 0;
}



#line 100

#if defined HAVE_ATTRIBUTE_TARGET_AVX512_SKX_WITH_INTRINSICS && defined NPY_HAVE_SSE2_INTRINSICS && 1
static NPY_INLINE NPY_GCC_TARGET_AVX512_SKX void
AVX512_SKX_isinf_DOUBLE(npy_bool*, npy_double*, const npy_intp n, const npy_intp stride);
#endif

static NPY_INLINE int
run_isinf_avx512_skx_DOUBLE(char **args, npy_intp const *dimensions, npy_intp const *steps)
{
#if defined HAVE_ATTRIBUTE_TARGET_AVX512_SKX_WITH_INTRINSICS && defined NPY_HAVE_SSE2_INTRINSICS && 1
    if (IS_OUTPUT_BLOCKABLE_UNARY(sizeof(npy_double), sizeof(npy_bool), 64)) {
        AVX512_SKX_isinf_DOUBLE((npy_bool*)args[1], (npy_double*)args[0], dimensions[0], steps[0]);
        return 1;
    }
    else {
        return 0;
    }
#endif
    return 0;
}



#line 100

#if defined HAVE_ATTRIBUTE_TARGET_AVX512_SKX_WITH_INTRINSICS && defined NPY_HAVE_SSE2_INTRINSICS && 1
static NPY_INLINE NPY_GCC_TARGET_AVX512_SKX void
AVX512_SKX_signbit_DOUBLE(npy_bool*, npy_double*, const npy_intp n, const npy_intp stride);
#endif

static NPY_INLINE int
run_signbit_avx512_skx_DOUBLE(char **args, npy_intp const *dimensions, npy_intp const *steps)
{
#if defined HAVE_ATTRIBUTE_TARGET_AVX512_SKX_WITH_INTRINSICS && defined NPY_HAVE_SSE2_INTRINSICS && 1
    if (IS_OUTPUT_BLOCKABLE_UNARY(sizeof(npy_double), sizeof(npy_bool), 64)) {
        AVX512_SKX_signbit_DOUBLE((npy_bool*)args[1], (npy_double*)args[0], dimensions[0], steps[0]);
        return 1;
    }
    else {
        return 0;
    }
#endif
    return 0;
}




#line 96

#line 100

#if defined HAVE_ATTRIBUTE_TARGET_AVX512_SKX_WITH_INTRINSICS && defined NPY_HAVE_SSE2_INTRINSICS && 0
static NPY_INLINE NPY_GCC_TARGET_AVX512_SKX void
AVX512_SKX_isnan_LONGDOUBLE(npy_bool*, npy_longdouble*, const npy_intp n, const npy_intp stride);
#endif

static NPY_INLINE int
run_isnan_avx512_skx_LONGDOUBLE(char **args, npy_intp const *dimensions, npy_intp const *steps)
{
#if defined HAVE_ATTRIBUTE_TARGET_AVX512_SKX_WITH_INTRINSICS && defined NPY_HAVE_SSE2_INTRINSICS && 0
    if (IS_OUTPUT_BLOCKABLE_UNARY(sizeof(npy_longdouble), sizeof(npy_bool), 64)) {
        AVX512_SKX_isnan_LONGDOUBLE((npy_bool*)args[1], (npy_longdouble*)args[0], dimensions[0], steps[0]);
        return 1;
    }
    else {
        return 0;
    }
#endif
    return 0;
}



#line 100

#if defined HAVE_ATTRIBUTE_TARGET_AVX512_SKX_WITH_INTRINSICS && defined NPY_HAVE_SSE2_INTRINSICS && 0
static NPY_INLINE NPY_GCC_TARGET_AVX512_SKX void
AVX512_SKX_isfinite_LONGDOUBLE(npy_bool*, npy_longdouble*, const npy_intp n, const npy_intp stride);
#endif

static NPY_INLINE int
run_isfinite_avx512_skx_LONGDOUBLE(char **args, npy_intp const *dimensions, npy_intp const *steps)
{
#if defined HAVE_ATTRIBUTE_TARGET_AVX512_SKX_WITH_INTRINSICS && defined NPY_HAVE_SSE2_INTRINSICS && 0
    if (IS_OUTPUT_BLOCKABLE_UNARY(sizeof(npy_longdouble), sizeof(npy_bool), 64)) {
        AVX512_SKX_isfinite_LONGDOUBLE((npy_bool*)args[1], (npy_longdouble*)args[0], dimensions[0], steps[0]);
        return 1;
    }
    else {
        return 0;
    }
#endif
    return 0;
}



#line 100

#if defined HAVE_ATTRIBUTE_TARGET_AVX512_SKX_WITH_INTRINSICS && defined NPY_HAVE_SSE2_INTRINSICS && 0
static NPY_INLINE NPY_GCC_TARGET_AVX512_SKX void
AVX512_SKX_isinf_LONGDOUBLE(npy_bool*, npy_longdouble*, const npy_intp n, const npy_intp stride);
#endif

static NPY_INLINE int
run_isinf_avx512_skx_LONGDOUBLE(char **args, npy_intp const *dimensions, npy_intp const *steps)
{
#if defined HAVE_ATTRIBUTE_TARGET_AVX512_SKX_WITH_INTRINSICS && defined NPY_HAVE_SSE2_INTRINSICS && 0
    if (IS_OUTPUT_BLOCKABLE_UNARY(sizeof(npy_longdouble), sizeof(npy_bool), 64)) {
        AVX512_SKX_isinf_LONGDOUBLE((npy_bool*)args[1], (npy_longdouble*)args[0], dimensions[0], steps[0]);
        return 1;
    }
    else {
        return 0;
    }
#endif
    return 0;
}



#line 100

#if defined HAVE_ATTRIBUTE_TARGET_AVX512_SKX_WITH_INTRINSICS && defined NPY_HAVE_SSE2_INTRINSICS && 0
static NPY_INLINE NPY_GCC_TARGET_AVX512_SKX void
AVX512_SKX_signbit_LONGDOUBLE(npy_bool*, npy_longdouble*, const npy_intp n, const npy_intp stride);
#endif

static NPY_INLINE int
run_signbit_avx512_skx_LONGDOUBLE(char **args, npy_intp const *dimensions, npy_intp const *steps)
{
#if defined HAVE_ATTRIBUTE_TARGET_AVX512_SKX_WITH_INTRINSICS && defined NPY_HAVE_SSE2_INTRINSICS && 0
    if (IS_OUTPUT_BLOCKABLE_UNARY(sizeof(npy_longdouble), sizeof(npy_bool), 64)) {
        AVX512_SKX_signbit_LONGDOUBLE((npy_bool*)args[1], (npy_longdouble*)args[0], dimensions[0], steps[0]);
        return 1;
    }
    else {
        return 0;
    }
#endif
    return 0;
}





#line 132

#line 138

#if 1 && defined NPY_HAVE_SSE2_INTRINSICS

/* prototypes */
static void
sse2_negative_FLOAT(npy_float *, npy_float *, const npy_intp n);

#endif

static NPY_INLINE int
run_unary_simd_negative_FLOAT(char **args, npy_intp const *dimensions, npy_intp const *steps)
{
#if 1 && defined NPY_HAVE_SSE2_INTRINSICS
    if (IS_BLOCKABLE_UNARY(sizeof(npy_float), VECTOR_SIZE_BYTES)) {
        sse2_negative_FLOAT((npy_float*)args[1], (npy_float*)args[0], dimensions[0]);
        return 1;
    }
#endif
    return 0;
}



#line 164

#if 1 && defined NPY_HAVE_SSE2_INTRINSICS

static void
sse2_isnan_FLOAT(npy_bool * op, npy_float * ip1, npy_intp n);

#endif

static NPY_INLINE int
run_isnan_simd_FLOAT(char **args, npy_intp const *dimensions, npy_intp const *steps)
{
#if 1 && defined NPY_HAVE_SSE2_INTRINSICS
    if (steps[0] == sizeof(npy_float) && steps[1] == 1 &&
        npy_is_aligned(args[0], sizeof(npy_float))) {
        sse2_isnan_FLOAT((npy_bool*)args[1], (npy_float*)args[0], dimensions[0]);
        return 1;
    }
#endif
    return 0;
}


#line 164

#if 1 && defined NPY_HAVE_SSE2_INTRINSICS

static void
sse2_isfinite_FLOAT(npy_bool * op, npy_float * ip1, npy_intp n);

#endif

static NPY_INLINE int
run_isfinite_simd_FLOAT(char **args, npy_intp const *dimensions, npy_intp const *steps)
{
#if 1 && defined NPY_HAVE_SSE2_INTRINSICS
    if (steps[0] == sizeof(npy_float) && steps[1] == 1 &&
        npy_is_aligned(args[0], sizeof(npy_float))) {
        sse2_isfinite_FLOAT((npy_bool*)args[1], (npy_float*)args[0], dimensions[0]);
        return 1;
    }
#endif
    return 0;
}


#line 164

#if 1 && defined NPY_HAVE_SSE2_INTRINSICS

static void
sse2_isinf_FLOAT(npy_bool * op, npy_float * ip1, npy_intp n);

#endif

static NPY_INLINE int
run_isinf_simd_FLOAT(char **args, npy_intp const *dimensions, npy_intp const *steps)
{
#if 1 && defined NPY_HAVE_SSE2_INTRINSICS
    if (steps[0] == sizeof(npy_float) && steps[1] == 1 &&
        npy_is_aligned(args[0], sizeof(npy_float))) {
        sse2_isinf_FLOAT((npy_bool*)args[1], (npy_float*)args[0], dimensions[0]);
        return 1;
    }
#endif
    return 0;
}


#line 164

#if 1 && defined NPY_HAVE_SSE2_INTRINSICS

static void
sse2_signbit_FLOAT(npy_bool * op, npy_float * ip1, npy_intp n);

#endif

static NPY_INLINE int
run_signbit_simd_FLOAT(char **args, npy_intp const *dimensions, npy_intp const *steps)
{
#if 1 && defined NPY_HAVE_SSE2_INTRINSICS
    if (steps[0] == sizeof(npy_float) && steps[1] == 1 &&
        npy_is_aligned(args[0], sizeof(npy_float))) {
        sse2_signbit_FLOAT((npy_bool*)args[1], (npy_float*)args[0], dimensions[0]);
        return 1;
    }
#endif
    return 0;
}




#line 132

#line 138

#if 1 && defined NPY_HAVE_SSE2_INTRINSICS

/* prototypes */
static void
sse2_negative_DOUBLE(npy_double *, npy_double *, const npy_intp n);

#endif

static NPY_INLINE int
run_unary_simd_negative_DOUBLE(char **args, npy_intp const *dimensions, npy_intp const *steps)
{
#if 1 && defined NPY_HAVE_SSE2_INTRINSICS
    if (IS_BLOCKABLE_UNARY(sizeof(npy_double), VECTOR_SIZE_BYTES)) {
        sse2_negative_DOUBLE((npy_double*)args[1], (npy_double*)args[0], dimensions[0]);
        return 1;
    }
#endif
    return 0;
}



#line 164

#if 1 && defined NPY_HAVE_SSE2_INTRINSICS

static void
sse2_isnan_DOUBLE(npy_bool * op, npy_double * ip1, npy_intp n);

#endif

static NPY_INLINE int
run_isnan_simd_DOUBLE(char **args, npy_intp const *dimensions, npy_intp const *steps)
{
#if 1 && defined NPY_HAVE_SSE2_INTRINSICS
    if (steps[0] == sizeof(npy_double) && steps[1] == 1 &&
        npy_is_aligned(args[0], sizeof(npy_double))) {
        sse2_isnan_DOUBLE((npy_bool*)args[1], (npy_double*)args[0], dimensions[0]);
        return 1;
    }
#endif
    return 0;
}


#line 164

#if 1 && defined NPY_HAVE_SSE2_INTRINSICS

static void
sse2_isfinite_DOUBLE(npy_bool * op, npy_double * ip1, npy_intp n);

#endif

static NPY_INLINE int
run_isfinite_simd_DOUBLE(char **args, npy_intp const *dimensions, npy_intp const *steps)
{
#if 1 && defined NPY_HAVE_SSE2_INTRINSICS
    if (steps[0] == sizeof(npy_double) && steps[1] == 1 &&
        npy_is_aligned(args[0], sizeof(npy_double))) {
        sse2_isfinite_DOUBLE((npy_bool*)args[1], (npy_double*)args[0], dimensions[0]);
        return 1;
    }
#endif
    return 0;
}


#line 164

#if 1 && defined NPY_HAVE_SSE2_INTRINSICS

static void
sse2_isinf_DOUBLE(npy_bool * op, npy_double * ip1, npy_intp n);

#endif

static NPY_INLINE int
run_isinf_simd_DOUBLE(char **args, npy_intp const *dimensions, npy_intp const *steps)
{
#if 1 && defined NPY_HAVE_SSE2_INTRINSICS
    if (steps[0] == sizeof(npy_double) && steps[1] == 1 &&
        npy_is_aligned(args[0], sizeof(npy_double))) {
        sse2_isinf_DOUBLE((npy_bool*)args[1], (npy_double*)args[0], dimensions[0]);
        return 1;
    }
#endif
    return 0;
}


#line 164

#if 1 && defined NPY_HAVE_SSE2_INTRINSICS

static void
sse2_signbit_DOUBLE(npy_bool * op, npy_double * ip1, npy_intp n);

#endif

static NPY_INLINE int
run_signbit_simd_DOUBLE(char **args, npy_intp const *dimensions, npy_intp const *steps)
{
#if 1 && defined NPY_HAVE_SSE2_INTRINSICS
    if (steps[0] == sizeof(npy_double) && steps[1] == 1 &&
        npy_is_aligned(args[0], sizeof(npy_double))) {
        sse2_signbit_DOUBLE((npy_bool*)args[1], (npy_double*)args[0], dimensions[0]);
        return 1;
    }
#endif
    return 0;
}




#line 132

#line 138

#if 0 && defined NPY_HAVE_SSE2_INTRINSICS

/* prototypes */
static void
sse2_negative_LONGDOUBLE(npy_longdouble *, npy_longdouble *, const npy_intp n);

#endif

static NPY_INLINE int
run_unary_simd_negative_LONGDOUBLE(char **args, npy_intp const *dimensions, npy_intp const *steps)
{
#if 0 && defined NPY_HAVE_SSE2_INTRINSICS
    if (IS_BLOCKABLE_UNARY(sizeof(npy_longdouble), VECTOR_SIZE_BYTES)) {
        sse2_negative_LONGDOUBLE((npy_longdouble*)args[1], (npy_longdouble*)args[0], dimensions[0]);
        return 1;
    }
#endif
    return 0;
}



#line 164

#if 0 && defined NPY_HAVE_SSE2_INTRINSICS

static void
sse2_isnan_LONGDOUBLE(npy_bool * op, npy_longdouble * ip1, npy_intp n);

#endif

static NPY_INLINE int
run_isnan_simd_LONGDOUBLE(char **args, npy_intp const *dimensions, npy_intp const *steps)
{
#if 0 && defined NPY_HAVE_SSE2_INTRINSICS
    if (steps[0] == sizeof(npy_longdouble) && steps[1] == 1 &&
        npy_is_aligned(args[0], sizeof(npy_longdouble))) {
        sse2_isnan_LONGDOUBLE((npy_bool*)args[1], (npy_longdouble*)args[0], dimensions[0]);
        return 1;
    }
#endif
    return 0;
}


#line 164

#if 0 && defined NPY_HAVE_SSE2_INTRINSICS

static void
sse2_isfinite_LONGDOUBLE(npy_bool * op, npy_longdouble * ip1, npy_intp n);

#endif

static NPY_INLINE int
run_isfinite_simd_LONGDOUBLE(char **args, npy_intp const *dimensions, npy_intp const *steps)
{
#if 0 && defined NPY_HAVE_SSE2_INTRINSICS
    if (steps[0] == sizeof(npy_longdouble) && steps[1] == 1 &&
        npy_is_aligned(args[0], sizeof(npy_longdouble))) {
        sse2_isfinite_LONGDOUBLE((npy_bool*)args[1], (npy_longdouble*)args[0], dimensions[0]);
        return 1;
    }
#endif
    return 0;
}


#line 164

#if 0 && defined NPY_HAVE_SSE2_INTRINSICS

static void
sse2_isinf_LONGDOUBLE(npy_bool * op, npy_longdouble * ip1, npy_intp n);

#endif

static NPY_INLINE int
run_isinf_simd_LONGDOUBLE(char **args, npy_intp const *dimensions, npy_intp const *steps)
{
#if 0 && defined NPY_HAVE_SSE2_INTRINSICS
    if (steps[0] == sizeof(npy_longdouble) && steps[1] == 1 &&
        npy_is_aligned(args[0], sizeof(npy_longdouble))) {
        sse2_isinf_LONGDOUBLE((npy_bool*)args[1], (npy_longdouble*)args[0], dimensions[0]);
        return 1;
    }
#endif
    return 0;
}


#line 164

#if 0 && defined NPY_HAVE_SSE2_INTRINSICS

static void
sse2_signbit_LONGDOUBLE(npy_bool * op, npy_longdouble * ip1, npy_intp n);

#endif

static NPY_INLINE int
run_signbit_simd_LONGDOUBLE(char **args, npy_intp const *dimensions, npy_intp const *steps)
{
#if 0 && defined NPY_HAVE_SSE2_INTRINSICS
    if (steps[0] == sizeof(npy_longdouble) && steps[1] == 1 &&
        npy_is_aligned(args[0], sizeof(npy_longdouble))) {
        sse2_signbit_LONGDOUBLE((npy_bool*)args[1], (npy_longdouble*)args[0], dimensions[0]);
        return 1;
    }
#endif
    return 0;
}





/*
 *****************************************************************************
 **                           BOOL DISPATCHERS
 *****************************************************************************
 */

#line 198

#if defined NPY_HAVE_SSE2_INTRINSICS
static void
sse2_binary_logical_or_BOOL(npy_bool * op, npy_bool * ip1, npy_bool * ip2,
                        npy_intp n);

static void
sse2_reduce_logical_or_BOOL(npy_bool * op, npy_bool * ip, npy_intp n);
#endif

static NPY_INLINE int
run_binary_simd_logical_or_BOOL(char **args, npy_intp const *dimensions, npy_intp const *steps)
{
#if defined NPY_HAVE_SSE2_INTRINSICS
    if (sizeof(npy_bool) == 1 &&
            IS_BLOCKABLE_BINARY(sizeof(npy_bool), VECTOR_SIZE_BYTES)) {
        sse2_binary_logical_or_BOOL((npy_bool*)args[2], (npy_bool*)args[0],
                               (npy_bool*)args[1], dimensions[0]);
        return 1;
    }
#endif
    return 0;
}


static NPY_INLINE int
run_reduce_simd_logical_or_BOOL(char **args, npy_intp const *dimensions, npy_intp const *steps)
{
#if defined NPY_HAVE_SSE2_INTRINSICS
    if (sizeof(npy_bool) == 1 &&
            IS_BLOCKABLE_REDUCE(sizeof(npy_bool), VECTOR_SIZE_BYTES)) {
        sse2_reduce_logical_or_BOOL((npy_bool*)args[0], (npy_bool*)args[1],
                                dimensions[0]);
        return 1;
    }
#endif
    return 0;
}


#line 198

#if defined NPY_HAVE_SSE2_INTRINSICS
static void
sse2_binary_logical_and_BOOL(npy_bool * op, npy_bool * ip1, npy_bool * ip2,
                        npy_intp n);

static void
sse2_reduce_logical_and_BOOL(npy_bool * op, npy_bool * ip, npy_intp n);
#endif

static NPY_INLINE int
run_binary_simd_logical_and_BOOL(char **args, npy_intp const *dimensions, npy_intp const *steps)
{
#if defined NPY_HAVE_SSE2_INTRINSICS
    if (sizeof(npy_bool) == 1 &&
            IS_BLOCKABLE_BINARY(sizeof(npy_bool), VECTOR_SIZE_BYTES)) {
        sse2_binary_logical_and_BOOL((npy_bool*)args[2], (npy_bool*)args[0],
                               (npy_bool*)args[1], dimensions[0]);
        return 1;
    }
#endif
    return 0;
}


static NPY_INLINE int
run_reduce_simd_logical_and_BOOL(char **args, npy_intp const *dimensions, npy_intp const *steps)
{
#if defined NPY_HAVE_SSE2_INTRINSICS
    if (sizeof(npy_bool) == 1 &&
            IS_BLOCKABLE_REDUCE(sizeof(npy_bool), VECTOR_SIZE_BYTES)) {
        sse2_reduce_logical_and_BOOL((npy_bool*)args[0], (npy_bool*)args[1],
                                dimensions[0]);
        return 1;
    }
#endif
    return 0;
}



#line 242

#if defined NPY_HAVE_SSE2_INTRINSICS
static void
sse2_absolute_BOOL(npy_bool *, npy_bool *, const npy_intp n);
#endif

static NPY_INLINE int
run_unary_simd_absolute_BOOL(char **args, npy_intp const *dimensions, npy_intp const *steps)
{
#if defined NPY_HAVE_SSE2_INTRINSICS
    if (sizeof(npy_bool) == 1 &&
            IS_BLOCKABLE_UNARY(sizeof(npy_bool), VECTOR_SIZE_BYTES)) {
        sse2_absolute_BOOL((npy_bool*)args[1], (npy_bool*)args[0], dimensions[0]);
        return 1;
    }
#endif
    return 0;
}


#line 242

#if defined NPY_HAVE_SSE2_INTRINSICS
static void
sse2_logical_not_BOOL(npy_bool *, npy_bool *, const npy_intp n);
#endif

static NPY_INLINE int
run_unary_simd_logical_not_BOOL(char **args, npy_intp const *dimensions, npy_intp const *steps)
{
#if defined NPY_HAVE_SSE2_INTRINSICS
    if (sizeof(npy_bool) == 1 &&
            IS_BLOCKABLE_UNARY(sizeof(npy_bool), VECTOR_SIZE_BYTES)) {
        sse2_logical_not_BOOL((npy_bool*)args[1], (npy_bool*)args[0], dimensions[0]);
        return 1;
    }
#endif
    return 0;
}



#ifdef NPY_HAVE_SSE2_INTRINSICS

/*
 * Vectorized operations
 */
/*
 *****************************************************************************
 **                           FLOAT LOOPS
 *****************************************************************************
 */

#line 278

NPY_FINLINE npy_float sse2_horizontal_min___m128(__m128 v)
{
    npy_float r;
    __m128 tmp = _mm_movehl_ps(v, v);                   /* c     d     ... */
    __m128 m = _mm_min_ps(v, tmp);                    /* m(ac) m(bd) ... */
    tmp = _mm_shuffle_ps(m, m, _MM_SHUFFLE(1, 1, 1, 1));/* m(bd) m(bd) ... */
    _mm_store_ss(&r, _mm_min_ps(tmp, m));             /* m(acbd) ... */
    return r;
}

NPY_FINLINE npy_double sse2_horizontal_min___m128d(__m128d v)
{
    npy_double r;
    __m128d tmp = _mm_unpackhi_pd(v, v);    /* b     b */
    _mm_store_sd(&r, _mm_min_pd(tmp, v)); /* m(ab) m(bb) */
    return r;
}

#line 278

NPY_FINLINE npy_float sse2_horizontal_max___m128(__m128 v)
{
    npy_float r;
    __m128 tmp = _mm_movehl_ps(v, v);                   /* c     d     ... */
    __m128 m = _mm_max_ps(v, tmp);                    /* m(ac) m(bd) ... */
    tmp = _mm_shuffle_ps(m, m, _MM_SHUFFLE(1, 1, 1, 1));/* m(bd) m(bd) ... */
    _mm_store_ss(&r, _mm_max_ps(tmp, m));             /* m(acbd) ... */
    return r;
}

NPY_FINLINE npy_double sse2_horizontal_max___m128d(__m128d v)
{
    npy_double r;
    __m128d tmp = _mm_unpackhi_pd(v, v);    /* b     b */
    _mm_store_sd(&r, _mm_max_pd(tmp, v)); /* m(ab) m(bb) */
    return r;
}


#line 315
/*
 * compress 4 vectors to 4/8 bytes in op with filled with 0 or 1
 * the last vector is passed as a pointer as MSVC 2010 is unable to ignore the
 * calling convention leading to C2719 on 32 bit, see #4795
 */
NPY_FINLINE void
sse2_compress4_to_byte_FLOAT(__m128 r1, __m128 r2, __m128 r3, __m128 * r4,
                              npy_bool * op)
{
    const __m128i mask = _mm_set1_epi8(0x1);
    __m128i ir1 = _mm_packs_epi32(_mm_castps_si128(r1), _mm_castps_si128(r2));
    __m128i ir2 = _mm_packs_epi32(_mm_castps_si128(r3), _mm_castps_si128(*r4));
    __m128i rr = _mm_packs_epi16(ir1, ir2);
#if 0
    rr = _mm_packs_epi16(rr, rr);
    rr = _mm_and_si128(rr, mask);
    _mm_storel_epi64((__m128i*)op, rr);
#else
    rr = _mm_and_si128(rr, mask);
    _mm_storeu_si128((__m128i*)op, rr);
#endif
}

static void
sse2_signbit_FLOAT(npy_bool * op, npy_float * ip1, npy_intp n)
{
    LOOP_BLOCK_ALIGN_VAR(ip1, npy_float, VECTOR_SIZE_BYTES) {
        op[i] = npy_signbit(ip1[i]) != 0;
    }
    LOOP_BLOCKED(npy_float, VECTOR_SIZE_BYTES) {
        __m128 a = _mm_load_ps(&ip1[i]);
        int r = _mm_movemask_ps(a);
        if (sizeof(npy_float) == 8) {
            op[i] = r & 1;
            op[i + 1] = (r >> 1);
        }
        else {
            op[i] = r & 1;
            op[i + 1] = (r >> 1) & 1;
            op[i + 2] = (r >> 2) & 1;
            op[i + 3] = (r >> 3);
        }
    }
    LOOP_BLOCKED_END {
        op[i] = npy_signbit(ip1[i]) != 0;
    }
}

#line 367

static void
sse2_isnan_FLOAT(npy_bool * op, npy_float * ip1, npy_intp n)
{
#if 0 != 0 /* isinf/isfinite */
    /* signbit mask 0x7FFFFFFF after andnot */
    const __m128 mask = _mm_set1_ps(-0.f);
    const __m128 ones = _mm_cmpeq_ps(_mm_setzero_ps(),
                                             _mm_setzero_ps());
#if 0
    const __m128 fltmax = _mm_set1_ps(DBL_MAX);
#else
    const __m128 fltmax = _mm_set1_ps(FLT_MAX);
#endif
#endif
    LOOP_BLOCK_ALIGN_VAR(ip1, npy_float, VECTOR_SIZE_BYTES) {
        op[i] = npy_isnan(ip1[i]) != 0;
    }
    LOOP_BLOCKED(npy_float, 4 * VECTOR_SIZE_BYTES) {
        __m128 a = _mm_load_ps(&ip1[i + 0 * VECTOR_SIZE_BYTES / sizeof(npy_float)]);
        __m128 b = _mm_load_ps(&ip1[i + 1 * VECTOR_SIZE_BYTES / sizeof(npy_float)]);
        __m128 c = _mm_load_ps(&ip1[i + 2 * VECTOR_SIZE_BYTES / sizeof(npy_float)]);
        __m128 d = _mm_load_ps(&ip1[i + 3 * VECTOR_SIZE_BYTES / sizeof(npy_float)]);
        __m128 r1, r2, r3, r4;
#if 0 != 0 /* isinf/isfinite */
        /* fabs via masking of sign bit */
        r1 = _mm_andnot_ps(mask, a);
        r2 = _mm_andnot_ps(mask, b);
        r3 = _mm_andnot_ps(mask, c);
        r4 = _mm_andnot_ps(mask, d);
#if 0 == 1 /* isfinite */
        /* negative compare against max float, nan is always true */
        r1 = _mm_cmpnle_ps(r1, fltmax);
        r2 = _mm_cmpnle_ps(r2, fltmax);
        r3 = _mm_cmpnle_ps(r3, fltmax);
        r4 = _mm_cmpnle_ps(r4, fltmax);
#else /* isinf */
        r1 = _mm_cmpnlt_ps(fltmax, r1);
        r2 = _mm_cmpnlt_ps(fltmax, r2);
        r3 = _mm_cmpnlt_ps(fltmax, r3);
        r4 = _mm_cmpnlt_ps(fltmax, r4);
#endif
        /* flip results to what we want (andnot as there is no sse not) */
        r1 = _mm_andnot_ps(r1, ones);
        r2 = _mm_andnot_ps(r2, ones);
        r3 = _mm_andnot_ps(r3, ones);
        r4 = _mm_andnot_ps(r4, ones);
#endif
#if 0 == 0 /* isnan */
        r1 = _mm_cmpneq_ps(a, a);
        r2 = _mm_cmpneq_ps(b, b);
        r3 = _mm_cmpneq_ps(c, c);
        r4 = _mm_cmpneq_ps(d, d);
#endif
        sse2_compress4_to_byte_FLOAT(r1, r2, r3, &r4, &op[i]);
    }
    LOOP_BLOCKED_END {
        op[i] = npy_isnan(ip1[i]) != 0;
    }
}


#line 367

static void
sse2_isfinite_FLOAT(npy_bool * op, npy_float * ip1, npy_intp n)
{
#if 1 != 0 /* isinf/isfinite */
    /* signbit mask 0x7FFFFFFF after andnot */
    const __m128 mask = _mm_set1_ps(-0.f);
    const __m128 ones = _mm_cmpeq_ps(_mm_setzero_ps(),
                                             _mm_setzero_ps());
#if 0
    const __m128 fltmax = _mm_set1_ps(DBL_MAX);
#else
    const __m128 fltmax = _mm_set1_ps(FLT_MAX);
#endif
#endif
    LOOP_BLOCK_ALIGN_VAR(ip1, npy_float, VECTOR_SIZE_BYTES) {
        op[i] = npy_isfinite(ip1[i]) != 0;
    }
    LOOP_BLOCKED(npy_float, 4 * VECTOR_SIZE_BYTES) {
        __m128 a = _mm_load_ps(&ip1[i + 0 * VECTOR_SIZE_BYTES / sizeof(npy_float)]);
        __m128 b = _mm_load_ps(&ip1[i + 1 * VECTOR_SIZE_BYTES / sizeof(npy_float)]);
        __m128 c = _mm_load_ps(&ip1[i + 2 * VECTOR_SIZE_BYTES / sizeof(npy_float)]);
        __m128 d = _mm_load_ps(&ip1[i + 3 * VECTOR_SIZE_BYTES / sizeof(npy_float)]);
        __m128 r1, r2, r3, r4;
#if 1 != 0 /* isinf/isfinite */
        /* fabs via masking of sign bit */
        r1 = _mm_andnot_ps(mask, a);
        r2 = _mm_andnot_ps(mask, b);
        r3 = _mm_andnot_ps(mask, c);
        r4 = _mm_andnot_ps(mask, d);
#if 1 == 1 /* isfinite */
        /* negative compare against max float, nan is always true */
        r1 = _mm_cmpnle_ps(r1, fltmax);
        r2 = _mm_cmpnle_ps(r2, fltmax);
        r3 = _mm_cmpnle_ps(r3, fltmax);
        r4 = _mm_cmpnle_ps(r4, fltmax);
#else /* isinf */
        r1 = _mm_cmpnlt_ps(fltmax, r1);
        r2 = _mm_cmpnlt_ps(fltmax, r2);
        r3 = _mm_cmpnlt_ps(fltmax, r3);
        r4 = _mm_cmpnlt_ps(fltmax, r4);
#endif
        /* flip results to what we want (andnot as there is no sse not) */
        r1 = _mm_andnot_ps(r1, ones);
        r2 = _mm_andnot_ps(r2, ones);
        r3 = _mm_andnot_ps(r3, ones);
        r4 = _mm_andnot_ps(r4, ones);
#endif
#if 1 == 0 /* isnan */
        r1 = _mm_cmpneq_ps(a, a);
        r2 = _mm_cmpneq_ps(b, b);
        r3 = _mm_cmpneq_ps(c, c);
        r4 = _mm_cmpneq_ps(d, d);
#endif
        sse2_compress4_to_byte_FLOAT(r1, r2, r3, &r4, &op[i]);
    }
    LOOP_BLOCKED_END {
        op[i] = npy_isfinite(ip1[i]) != 0;
    }
}


#line 367

static void
sse2_isinf_FLOAT(npy_bool * op, npy_float * ip1, npy_intp n)
{
#if 2 != 0 /* isinf/isfinite */
    /* signbit mask 0x7FFFFFFF after andnot */
    const __m128 mask = _mm_set1_ps(-0.f);
    const __m128 ones = _mm_cmpeq_ps(_mm_setzero_ps(),
                                             _mm_setzero_ps());
#if 0
    const __m128 fltmax = _mm_set1_ps(DBL_MAX);
#else
    const __m128 fltmax = _mm_set1_ps(FLT_MAX);
#endif
#endif
    LOOP_BLOCK_ALIGN_VAR(ip1, npy_float, VECTOR_SIZE_BYTES) {
        op[i] = npy_isinf(ip1[i]) != 0;
    }
    LOOP_BLOCKED(npy_float, 4 * VECTOR_SIZE_BYTES) {
        __m128 a = _mm_load_ps(&ip1[i + 0 * VECTOR_SIZE_BYTES / sizeof(npy_float)]);
        __m128 b = _mm_load_ps(&ip1[i + 1 * VECTOR_SIZE_BYTES / sizeof(npy_float)]);
        __m128 c = _mm_load_ps(&ip1[i + 2 * VECTOR_SIZE_BYTES / sizeof(npy_float)]);
        __m128 d = _mm_load_ps(&ip1[i + 3 * VECTOR_SIZE_BYTES / sizeof(npy_float)]);
        __m128 r1, r2, r3, r4;
#if 2 != 0 /* isinf/isfinite */
        /* fabs via masking of sign bit */
        r1 = _mm_andnot_ps(mask, a);
        r2 = _mm_andnot_ps(mask, b);
        r3 = _mm_andnot_ps(mask, c);
        r4 = _mm_andnot_ps(mask, d);
#if 2 == 1 /* isfinite */
        /* negative compare against max float, nan is always true */
        r1 = _mm_cmpnle_ps(r1, fltmax);
        r2 = _mm_cmpnle_ps(r2, fltmax);
        r3 = _mm_cmpnle_ps(r3, fltmax);
        r4 = _mm_cmpnle_ps(r4, fltmax);
#else /* isinf */
        r1 = _mm_cmpnlt_ps(fltmax, r1);
        r2 = _mm_cmpnlt_ps(fltmax, r2);
        r3 = _mm_cmpnlt_ps(fltmax, r3);
        r4 = _mm_cmpnlt_ps(fltmax, r4);
#endif
        /* flip results to what we want (andnot as there is no sse not) */
        r1 = _mm_andnot_ps(r1, ones);
        r2 = _mm_andnot_ps(r2, ones);
        r3 = _mm_andnot_ps(r3, ones);
        r4 = _mm_andnot_ps(r4, ones);
#endif
#if 2 == 0 /* isnan */
        r1 = _mm_cmpneq_ps(a, a);
        r2 = _mm_cmpneq_ps(b, b);
        r3 = _mm_cmpneq_ps(c, c);
        r4 = _mm_cmpneq_ps(d, d);
#endif
        sse2_compress4_to_byte_FLOAT(r1, r2, r3, &r4, &op[i]);
    }
    LOOP_BLOCKED_END {
        op[i] = npy_isinf(ip1[i]) != 0;
    }
}



static void
sse2_negative_FLOAT(npy_float * op, npy_float * ip, const npy_intp n)
{
    /*
     * get 0x7FFFFFFF mask (everything but signbit set)
     * float & ~mask will remove the sign, float ^ mask flips the sign
     * this is equivalent to how the compiler implements fabs on amd64
     */
    const __m128 mask = _mm_set1_ps(-0.f);

    /* align output to VECTOR_SIZE_BYTES bytes */
    LOOP_BLOCK_ALIGN_VAR(op, npy_float, VECTOR_SIZE_BYTES) {
        op[i] = -ip[i];
    }
    assert((npy_uintp)n < (VECTOR_SIZE_BYTES / sizeof(npy_float)) ||
           npy_is_aligned(&op[i], VECTOR_SIZE_BYTES));
    if (npy_is_aligned(&ip[i], VECTOR_SIZE_BYTES)) {
        LOOP_BLOCKED(npy_float, VECTOR_SIZE_BYTES) {
            __m128 a = _mm_load_ps(&ip[i]);
            _mm_store_ps(&op[i], _mm_xor_ps(mask, a));
        }
    }
    else {
        LOOP_BLOCKED(npy_float, VECTOR_SIZE_BYTES) {
            __m128 a = _mm_loadu_ps(&ip[i]);
            _mm_store_ps(&op[i], _mm_xor_ps(mask, a));
        }
    }
    LOOP_BLOCKED_END {
        op[i] = -ip[i];
    }
}
/**end repeat1**/


#line 315
/*
 * compress 4 vectors to 4/8 bytes in op with filled with 0 or 1
 * the last vector is passed as a pointer as MSVC 2010 is unable to ignore the
 * calling convention leading to C2719 on 32 bit, see #4795
 */
NPY_FINLINE void
sse2_compress4_to_byte_DOUBLE(__m128d r1, __m128d r2, __m128d r3, __m128d * r4,
                              npy_bool * op)
{
    const __m128i mask = _mm_set1_epi8(0x1);
    __m128i ir1 = _mm_packs_epi32(_mm_castpd_si128(r1), _mm_castpd_si128(r2));
    __m128i ir2 = _mm_packs_epi32(_mm_castpd_si128(r3), _mm_castpd_si128(*r4));
    __m128i rr = _mm_packs_epi16(ir1, ir2);
#if 1
    rr = _mm_packs_epi16(rr, rr);
    rr = _mm_and_si128(rr, mask);
    _mm_storel_epi64((__m128i*)op, rr);
#else
    rr = _mm_and_si128(rr, mask);
    _mm_storeu_si128((__m128i*)op, rr);
#endif
}

static void
sse2_signbit_DOUBLE(npy_bool * op, npy_double * ip1, npy_intp n)
{
    LOOP_BLOCK_ALIGN_VAR(ip1, npy_double, VECTOR_SIZE_BYTES) {
        op[i] = npy_signbit(ip1[i]) != 0;
    }
    LOOP_BLOCKED(npy_double, VECTOR_SIZE_BYTES) {
        __m128d a = _mm_load_pd(&ip1[i]);
        int r = _mm_movemask_pd(a);
        if (sizeof(npy_double) == 8) {
            op[i] = r & 1;
            op[i + 1] = (r >> 1);
        }
        else {
            op[i] = r & 1;
            op[i + 1] = (r >> 1) & 1;
            op[i + 2] = (r >> 2) & 1;
            op[i + 3] = (r >> 3);
        }
    }
    LOOP_BLOCKED_END {
        op[i] = npy_signbit(ip1[i]) != 0;
    }
}

#line 367

static void
sse2_isnan_DOUBLE(npy_bool * op, npy_double * ip1, npy_intp n)
{
#if 0 != 0 /* isinf/isfinite */
    /* signbit mask 0x7FFFFFFF after andnot */
    const __m128d mask = _mm_set1_pd(-0.);
    const __m128d ones = _mm_cmpeq_pd(_mm_setzero_pd(),
                                             _mm_setzero_pd());
#if 1
    const __m128d fltmax = _mm_set1_pd(DBL_MAX);
#else
    const __m128d fltmax = _mm_set1_pd(FLT_MAX);
#endif
#endif
    LOOP_BLOCK_ALIGN_VAR(ip1, npy_double, VECTOR_SIZE_BYTES) {
        op[i] = npy_isnan(ip1[i]) != 0;
    }
    LOOP_BLOCKED(npy_double, 4 * VECTOR_SIZE_BYTES) {
        __m128d a = _mm_load_pd(&ip1[i + 0 * VECTOR_SIZE_BYTES / sizeof(npy_double)]);
        __m128d b = _mm_load_pd(&ip1[i + 1 * VECTOR_SIZE_BYTES / sizeof(npy_double)]);
        __m128d c = _mm_load_pd(&ip1[i + 2 * VECTOR_SIZE_BYTES / sizeof(npy_double)]);
        __m128d d = _mm_load_pd(&ip1[i + 3 * VECTOR_SIZE_BYTES / sizeof(npy_double)]);
        __m128d r1, r2, r3, r4;
#if 0 != 0 /* isinf/isfinite */
        /* fabs via masking of sign bit */
        r1 = _mm_andnot_pd(mask, a);
        r2 = _mm_andnot_pd(mask, b);
        r3 = _mm_andnot_pd(mask, c);
        r4 = _mm_andnot_pd(mask, d);
#if 0 == 1 /* isfinite */
        /* negative compare against max float, nan is always true */
        r1 = _mm_cmpnle_pd(r1, fltmax);
        r2 = _mm_cmpnle_pd(r2, fltmax);
        r3 = _mm_cmpnle_pd(r3, fltmax);
        r4 = _mm_cmpnle_pd(r4, fltmax);
#else /* isinf */
        r1 = _mm_cmpnlt_pd(fltmax, r1);
        r2 = _mm_cmpnlt_pd(fltmax, r2);
        r3 = _mm_cmpnlt_pd(fltmax, r3);
        r4 = _mm_cmpnlt_pd(fltmax, r4);
#endif
        /* flip results to what we want (andnot as there is no sse not) */
        r1 = _mm_andnot_pd(r1, ones);
        r2 = _mm_andnot_pd(r2, ones);
        r3 = _mm_andnot_pd(r3, ones);
        r4 = _mm_andnot_pd(r4, ones);
#endif
#if 0 == 0 /* isnan */
        r1 = _mm_cmpneq_pd(a, a);
        r2 = _mm_cmpneq_pd(b, b);
        r3 = _mm_cmpneq_pd(c, c);
        r4 = _mm_cmpneq_pd(d, d);
#endif
        sse2_compress4_to_byte_DOUBLE(r1, r2, r3, &r4, &op[i]);
    }
    LOOP_BLOCKED_END {
        op[i] = npy_isnan(ip1[i]) != 0;
    }
}


#line 367

static void
sse2_isfinite_DOUBLE(npy_bool * op, npy_double * ip1, npy_intp n)
{
#if 1 != 0 /* isinf/isfinite */
    /* signbit mask 0x7FFFFFFF after andnot */
    const __m128d mask = _mm_set1_pd(-0.);
    const __m128d ones = _mm_cmpeq_pd(_mm_setzero_pd(),
                                             _mm_setzero_pd());
#if 1
    const __m128d fltmax = _mm_set1_pd(DBL_MAX);
#else
    const __m128d fltmax = _mm_set1_pd(FLT_MAX);
#endif
#endif
    LOOP_BLOCK_ALIGN_VAR(ip1, npy_double, VECTOR_SIZE_BYTES) {
        op[i] = npy_isfinite(ip1[i]) != 0;
    }
    LOOP_BLOCKED(npy_double, 4 * VECTOR_SIZE_BYTES) {
        __m128d a = _mm_load_pd(&ip1[i + 0 * VECTOR_SIZE_BYTES / sizeof(npy_double)]);
        __m128d b = _mm_load_pd(&ip1[i + 1 * VECTOR_SIZE_BYTES / sizeof(npy_double)]);
        __m128d c = _mm_load_pd(&ip1[i + 2 * VECTOR_SIZE_BYTES / sizeof(npy_double)]);
        __m128d d = _mm_load_pd(&ip1[i + 3 * VECTOR_SIZE_BYTES / sizeof(npy_double)]);
        __m128d r1, r2, r3, r4;
#if 1 != 0 /* isinf/isfinite */
        /* fabs via masking of sign bit */
        r1 = _mm_andnot_pd(mask, a);
        r2 = _mm_andnot_pd(mask, b);
        r3 = _mm_andnot_pd(mask, c);
        r4 = _mm_andnot_pd(mask, d);
#if 1 == 1 /* isfinite */
        /* negative compare against max float, nan is always true */
        r1 = _mm_cmpnle_pd(r1, fltmax);
        r2 = _mm_cmpnle_pd(r2, fltmax);
        r3 = _mm_cmpnle_pd(r3, fltmax);
        r4 = _mm_cmpnle_pd(r4, fltmax);
#else /* isinf */
        r1 = _mm_cmpnlt_pd(fltmax, r1);
        r2 = _mm_cmpnlt_pd(fltmax, r2);
        r3 = _mm_cmpnlt_pd(fltmax, r3);
        r4 = _mm_cmpnlt_pd(fltmax, r4);
#endif
        /* flip results to what we want (andnot as there is no sse not) */
        r1 = _mm_andnot_pd(r1, ones);
        r2 = _mm_andnot_pd(r2, ones);
        r3 = _mm_andnot_pd(r3, ones);
        r4 = _mm_andnot_pd(r4, ones);
#endif
#if 1 == 0 /* isnan */
        r1 = _mm_cmpneq_pd(a, a);
        r2 = _mm_cmpneq_pd(b, b);
        r3 = _mm_cmpneq_pd(c, c);
        r4 = _mm_cmpneq_pd(d, d);
#endif
        sse2_compress4_to_byte_DOUBLE(r1, r2, r3, &r4, &op[i]);
    }
    LOOP_BLOCKED_END {
        op[i] = npy_isfinite(ip1[i]) != 0;
    }
}


#line 367

static void
sse2_isinf_DOUBLE(npy_bool * op, npy_double * ip1, npy_intp n)
{
#if 2 != 0 /* isinf/isfinite */
    /* signbit mask 0x7FFFFFFF after andnot */
    const __m128d mask = _mm_set1_pd(-0.);
    const __m128d ones = _mm_cmpeq_pd(_mm_setzero_pd(),
                                             _mm_setzero_pd());
#if 1
    const __m128d fltmax = _mm_set1_pd(DBL_MAX);
#else
    const __m128d fltmax = _mm_set1_pd(FLT_MAX);
#endif
#endif
    LOOP_BLOCK_ALIGN_VAR(ip1, npy_double, VECTOR_SIZE_BYTES) {
        op[i] = npy_isinf(ip1[i]) != 0;
    }
    LOOP_BLOCKED(npy_double, 4 * VECTOR_SIZE_BYTES) {
        __m128d a = _mm_load_pd(&ip1[i + 0 * VECTOR_SIZE_BYTES / sizeof(npy_double)]);
        __m128d b = _mm_load_pd(&ip1[i + 1 * VECTOR_SIZE_BYTES / sizeof(npy_double)]);
        __m128d c = _mm_load_pd(&ip1[i + 2 * VECTOR_SIZE_BYTES / sizeof(npy_double)]);
        __m128d d = _mm_load_pd(&ip1[i + 3 * VECTOR_SIZE_BYTES / sizeof(npy_double)]);
        __m128d r1, r2, r3, r4;
#if 2 != 0 /* isinf/isfinite */
        /* fabs via masking of sign bit */
        r1 = _mm_andnot_pd(mask, a);
        r2 = _mm_andnot_pd(mask, b);
        r3 = _mm_andnot_pd(mask, c);
        r4 = _mm_andnot_pd(mask, d);
#if 2 == 1 /* isfinite */
        /* negative compare against max float, nan is always true */
        r1 = _mm_cmpnle_pd(r1, fltmax);
        r2 = _mm_cmpnle_pd(r2, fltmax);
        r3 = _mm_cmpnle_pd(r3, fltmax);
        r4 = _mm_cmpnle_pd(r4, fltmax);
#else /* isinf */
        r1 = _mm_cmpnlt_pd(fltmax, r1);
        r2 = _mm_cmpnlt_pd(fltmax, r2);
        r3 = _mm_cmpnlt_pd(fltmax, r3);
        r4 = _mm_cmpnlt_pd(fltmax, r4);
#endif
        /* flip results to what we want (andnot as there is no sse not) */
        r1 = _mm_andnot_pd(r1, ones);
        r2 = _mm_andnot_pd(r2, ones);
        r3 = _mm_andnot_pd(r3, ones);
        r4 = _mm_andnot_pd(r4, ones);
#endif
#if 2 == 0 /* isnan */
        r1 = _mm_cmpneq_pd(a, a);
        r2 = _mm_cmpneq_pd(b, b);
        r3 = _mm_cmpneq_pd(c, c);
        r4 = _mm_cmpneq_pd(d, d);
#endif
        sse2_compress4_to_byte_DOUBLE(r1, r2, r3, &r4, &op[i]);
    }
    LOOP_BLOCKED_END {
        op[i] = npy_isinf(ip1[i]) != 0;
    }
}



static void
sse2_negative_DOUBLE(npy_double * op, npy_double * ip, const npy_intp n)
{
    /*
     * get 0x7FFFFFFF mask (everything but signbit set)
     * float & ~mask will remove the sign, float ^ mask flips the sign
     * this is equivalent to how the compiler implements fabs on amd64
     */
    const __m128d mask = _mm_set1_pd(-0.);

    /* align output to VECTOR_SIZE_BYTES bytes */
    LOOP_BLOCK_ALIGN_VAR(op, npy_double, VECTOR_SIZE_BYTES) {
        op[i] = -ip[i];
    }
    assert((npy_uintp)n < (VECTOR_SIZE_BYTES / sizeof(npy_double)) ||
           npy_is_aligned(&op[i], VECTOR_SIZE_BYTES));
    if (npy_is_aligned(&ip[i], VECTOR_SIZE_BYTES)) {
        LOOP_BLOCKED(npy_double, VECTOR_SIZE_BYTES) {
            __m128d a = _mm_load_pd(&ip[i]);
            _mm_store_pd(&op[i], _mm_xor_pd(mask, a));
        }
    }
    else {
        LOOP_BLOCKED(npy_double, VECTOR_SIZE_BYTES) {
            __m128d a = _mm_loadu_pd(&ip[i]);
            _mm_store_pd(&op[i], _mm_xor_pd(mask, a));
        }
    }
    LOOP_BLOCKED_END {
        op[i] = -ip[i];
    }
}
/**end repeat1**/



/* bunch of helper functions used in ISA_exp/log_FLOAT*/

#if defined HAVE_ATTRIBUTE_TARGET_AVX2_WITH_INTRINSICS
NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_FMA __m256
fma_get_full_load_mask_ps(void)
{
    return _mm256_set1_ps(-1.0);
}

NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_FMA __m256i
fma_get_full_load_mask_pd(void)
{
    return _mm256_castpd_si256(_mm256_set1_pd(-1.0));
}

NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_FMA __m256
fma_get_partial_load_mask_ps(const npy_int num_elem, const npy_int num_lanes)
{
    float maskint[16] = {-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,
                            1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0};
    float* addr = maskint + num_lanes - num_elem;
    return _mm256_loadu_ps(addr);
}

NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_FMA __m256i
fma_get_partial_load_mask_pd(const npy_int num_elem, const npy_int num_lanes)
{
    npy_int maskint[16] = {-1,-1,-1,-1,-1,-1,-1,-1,1,1,1,1,1,1,1,1};
    npy_int* addr = maskint + 2*num_lanes - 2*num_elem;
    return _mm256_loadu_si256((__m256i*) addr);
}

NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_FMA __m256
fma_masked_gather_ps(__m256 src,
                     npy_float* addr,
                     __m256i vindex,
                     __m256 mask)
{
    return _mm256_mask_i32gather_ps(src, addr, vindex, mask, 4);
}

NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_FMA __m256d
fma_masked_gather_pd(__m256d src,
                     npy_double* addr,
                     __m128i vindex,
                     __m256d mask)
{
    return _mm256_mask_i32gather_pd(src, addr, vindex, mask, 8);
}

NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_FMA __m256
fma_masked_load_ps(__m256 mask, npy_float* addr)
{
    return _mm256_maskload_ps(addr, _mm256_cvtps_epi32(mask));
}

NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_FMA __m256d
fma_masked_load_pd(__m256i mask, npy_double* addr)
{
    return _mm256_maskload_pd(addr, mask);
}

NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_FMA __m256
fma_set_masked_lanes_ps(__m256 x, __m256 val, __m256 mask)
{
    return _mm256_blendv_ps(x, val, mask);
}

NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_FMA __m256d
fma_set_masked_lanes_pd(__m256d x, __m256d val, __m256d mask)
{
    return _mm256_blendv_pd(x, val, mask);
}

NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_FMA __m256
fma_blend(__m256 x, __m256 y, __m256 ymask)
{
    return _mm256_blendv_ps(x, y, ymask);
}

NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_FMA __m256
fma_invert_mask_ps(__m256 ymask)
{
    return _mm256_andnot_ps(ymask, _mm256_set1_ps(-1.0));
}

NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_FMA __m256i
fma_invert_mask_pd(__m256i ymask)
{
    return _mm256_andnot_si256(ymask, _mm256_set1_epi32(0xFFFFFFFF));
}

#line 562
NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_FMA __m256
fma_abs_ps(__m256 x)
{
    return _mm256_andnot_ps(_mm256_set1_ps(-0.0), x);
}

NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_FMA __m256
fma_reciprocal_ps(__m256 x)
{
    return _mm256_div_ps(_mm256_set1_ps(1.0f), x);
}

NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_FMA __m256
fma_rint_ps(__m256 x)
{
    return _mm256_round_ps(x, _MM_FROUND_TO_NEAREST_INT);
}

NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_FMA __m256
fma_floor_ps(__m256 x)
{
    return _mm256_round_ps(x, _MM_FROUND_TO_NEG_INF);
}

NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_FMA __m256
fma_trunc_ps(__m256 x)
{
    return _mm256_round_ps(x, _MM_FROUND_TO_ZERO);
}

#line 562
NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_FMA __m256d
fma_abs_pd(__m256d x)
{
    return _mm256_andnot_pd(_mm256_set1_pd(-0.0), x);
}

NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_FMA __m256d
fma_reciprocal_pd(__m256d x)
{
    return _mm256_div_pd(_mm256_set1_pd(1.0f), x);
}

NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_FMA __m256d
fma_rint_pd(__m256d x)
{
    return _mm256_round_pd(x, _MM_FROUND_TO_NEAREST_INT);
}

NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_FMA __m256d
fma_floor_pd(__m256d x)
{
    return _mm256_round_pd(x, _MM_FROUND_TO_NEG_INF);
}

NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_FMA __m256d
fma_trunc_pd(__m256d x)
{
    return _mm256_round_pd(x, _MM_FROUND_TO_ZERO);
}

#endif

#if defined HAVE_ATTRIBUTE_TARGET_AVX512F_WITH_INTRINSICS
NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F __mmask16
avx512_get_full_load_mask_ps(void)
{
    return 0xFFFF;
}

NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F __mmask8
avx512_get_full_load_mask_pd(void)
{
    return 0xFF;
}

NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F __mmask16
avx512_get_partial_load_mask_ps(const npy_int num_elem, const npy_int total_elem)
{
    return (0x0001 << num_elem) - 0x0001;
}

NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F __mmask8
avx512_get_partial_load_mask_pd(const npy_int num_elem, const npy_int total_elem)
{
    return (0x01 << num_elem) - 0x01;
}

NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F __m512
avx512_masked_gather_ps(__m512 src,
                        npy_float* addr,
                        __m512i vindex,
                        __mmask16 kmask)
{
    return _mm512_mask_i32gather_ps(src, kmask, vindex, addr, 4);
}

NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F __m512d
avx512_masked_gather_pd(__m512d src,
                        npy_double* addr,
                        __m256i vindex,
                        __mmask8 kmask)
{
    return _mm512_mask_i32gather_pd(src, kmask, vindex, addr, 8);
}

NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F __m512
avx512_masked_load_ps(__mmask16 mask, npy_float* addr)
{
    return _mm512_maskz_loadu_ps(mask, (__m512 *)addr);
}

NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F __m512d
avx512_masked_load_pd(__mmask8 mask, npy_double* addr)
{
    return _mm512_maskz_loadu_pd(mask, (__m512d *)addr);
}

NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F __m512
avx512_set_masked_lanes_ps(__m512 x, __m512 val, __mmask16 mask)
{
    return _mm512_mask_blend_ps(mask, x, val);
}

NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F __m512d
avx512_set_masked_lanes_pd(__m512d x, __m512d val, __mmask8 mask)
{
    return _mm512_mask_blend_pd(mask, x, val);
}

NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F __m512
avx512_blend(__m512 x, __m512 y, __mmask16 ymask)
{
    return _mm512_mask_mov_ps(x, ymask, y);
}

NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F __mmask16
avx512_invert_mask_ps(__mmask16 ymask)
{
    return _mm512_knot(ymask);
}

NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F __mmask8
avx512_invert_mask_pd(__mmask8 ymask)
{
    return _mm512_knot(ymask);
}

#line 693
NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F __m512
avx512_abs_ps(__m512 x)
{
    return (__m512) _mm512_and_epi32((__m512i) x,
				    _mm512_set1_epi32 (0x7fffffff));
}

NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F __m512
avx512_reciprocal_ps(__m512 x)
{
    return _mm512_div_ps(_mm512_set1_ps(1.0f), x);
}

NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F __m512
avx512_rint_ps(__m512 x)
{
    return _mm512_roundscale_ps(x, 0x08);
}

NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F __m512
avx512_floor_ps(__m512 x)
{
    return _mm512_roundscale_ps(x, 0x09);
}

NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F __m512
avx512_trunc_ps(__m512 x)
{
    return _mm512_roundscale_ps(x, 0x0B);
}

NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F __m512
avx512_hadd_ps(const __m512 x)
{
    return _mm512_add_ps(x, _mm512_permute_ps(x, 0xb1));
}

NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F __m512
avx512_hsub_ps(const __m512 x)
{
    return _mm512_sub_ps(x, _mm512_permute_ps(x, 0xb1));
}

NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F __m512
avx512_cabsolute_ps(const __m512 x1,
                        const __m512 x2,
                        const __m512i re_indices,
                        const __m512i im_indices)
{
    __m512 inf = _mm512_set1_ps(NPY_INFINITYF);
    __m512 nan = _mm512_set1_ps(NPY_NANF);
    __m512 x1_abs = avx512_abs_ps(x1);
    __m512 x2_abs = avx512_abs_ps(x2);
    __m512 re = _mm512_permutex2var_ps(x1_abs, re_indices, x2_abs);
    __m512 im = _mm512_permutex2var_ps(x1_abs, im_indices , x2_abs);
    /*
     * If real or imag = INF, then convert it to inf + j*inf
     * Handles: inf + j*nan, nan + j*inf
     */
    __mmask16 re_infmask = _mm512_cmp_ps_mask(re, inf, _CMP_EQ_OQ);
    __mmask16 im_infmask = _mm512_cmp_ps_mask(im, inf, _CMP_EQ_OQ);
    im = _mm512_mask_mov_ps(im, re_infmask, inf);
    re = _mm512_mask_mov_ps(re, im_infmask, inf);

    /*
     * If real or imag = NAN, then convert it to nan + j*nan
     * Handles: x + j*nan, nan + j*x
     */
    __mmask16 re_nanmask = _mm512_cmp_ps_mask(re, re, _CMP_NEQ_UQ);
    __mmask16 im_nanmask = _mm512_cmp_ps_mask(im, im, _CMP_NEQ_UQ);
    im = _mm512_mask_mov_ps(im, re_nanmask, nan);
    re = _mm512_mask_mov_ps(re, im_nanmask, nan);

    __m512 larger  = _mm512_max_ps(re, im);
    __m512 smaller = _mm512_min_ps(im, re);

    /*
     * Calculate div_mask to prevent 0./0. and inf/inf operations in div
     */
    __mmask16 zeromask = _mm512_cmp_ps_mask(larger, _mm512_setzero_ps(), _CMP_EQ_OQ);
    __mmask16 infmask = _mm512_cmp_ps_mask(smaller, inf, _CMP_EQ_OQ);
    __mmask16 div_mask = _mm512_knot(_mm512_kor(zeromask, infmask));
    __m512 ratio = _mm512_maskz_div_ps(div_mask, smaller, larger);
    __m512 hypot = _mm512_sqrt_ps(_mm512_fmadd_ps(
                                        ratio, ratio, _mm512_set1_ps(1.0f)));
    return _mm512_mul_ps(hypot, larger);
}

NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F __m512
avx512_conjugate_ps(const __m512 x)
{
    /*
     * __mm512_mask_xor_ps/pd requires AVX512DQ. We cast it to __m512i and
     * use the xor_epi32/64 uinstruction instead. Cast is a zero latency instruction
     */
    __m512i cast_x = _mm512_castps_si512(x);
    __m512i res = _mm512_mask_xor_epi32(cast_x, 0xAAAA,
                                        cast_x, _mm512_set1_epi32(0x80000000));
    return _mm512_castsi512_ps(res);
}

NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F __m512
avx512_cmul_ps(__m512 x1, __m512 x2)
{
    // x1 = r1, i1
    // x2 = r2, i2
    __m512 x3  = _mm512_permute_ps(x2, 0xb1);   // i2, r2
    __m512 x12 = _mm512_mul_ps(x1, x2);            // r1*r2, i1*i2
    __m512 x13 = _mm512_mul_ps(x1, x3);            // r1*i2, r2*i1
    __m512 outreal = avx512_hsub_ps(x12);          // r1*r2 - i1*i2, r1*r2 - i1*i2
    __m512 outimg  = avx512_hadd_ps(x13);          // r1*i2 + i1*r2, r1*i2 + i1*r2
    return _mm512_mask_blend_ps(0xAAAA, outreal, outimg);
}

NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F __m512
avx512_csquare_ps(__m512 x)
{
    return avx512_cmul_ps(x, x);
}


#line 693
NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F __m512d
avx512_abs_pd(__m512d x)
{
    return (__m512d) _mm512_and_epi64((__m512i) x,
				    _mm512_set1_epi64 (0x7fffffffffffffffLL));
}

NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F __m512d
avx512_reciprocal_pd(__m512d x)
{
    return _mm512_div_pd(_mm512_set1_pd(1.0f), x);
}

NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F __m512d
avx512_rint_pd(__m512d x)
{
    return _mm512_roundscale_pd(x, 0x08);
}

NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F __m512d
avx512_floor_pd(__m512d x)
{
    return _mm512_roundscale_pd(x, 0x09);
}

NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F __m512d
avx512_trunc_pd(__m512d x)
{
    return _mm512_roundscale_pd(x, 0x0B);
}

NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F __m512d
avx512_hadd_pd(const __m512d x)
{
    return _mm512_add_pd(x, _mm512_permute_pd(x, 0x55));
}

NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F __m512d
avx512_hsub_pd(const __m512d x)
{
    return _mm512_sub_pd(x, _mm512_permute_pd(x, 0x55));
}

NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F __m512d
avx512_cabsolute_pd(const __m512d x1,
                        const __m512d x2,
                        const __m512i re_indices,
                        const __m512i im_indices)
{
    __m512d inf = _mm512_set1_pd(NPY_INFINITY);
    __m512d nan = _mm512_set1_pd(NPY_NAN);
    __m512d x1_abs = avx512_abs_pd(x1);
    __m512d x2_abs = avx512_abs_pd(x2);
    __m512d re = _mm512_permutex2var_pd(x1_abs, re_indices, x2_abs);
    __m512d im = _mm512_permutex2var_pd(x1_abs, im_indices , x2_abs);
    /*
     * If real or imag = INF, then convert it to inf + j*inf
     * Handles: inf + j*nan, nan + j*inf
     */
    __mmask8 re_infmask = _mm512_cmp_pd_mask(re, inf, _CMP_EQ_OQ);
    __mmask8 im_infmask = _mm512_cmp_pd_mask(im, inf, _CMP_EQ_OQ);
    im = _mm512_mask_mov_pd(im, re_infmask, inf);
    re = _mm512_mask_mov_pd(re, im_infmask, inf);

    /*
     * If real or imag = NAN, then convert it to nan + j*nan
     * Handles: x + j*nan, nan + j*x
     */
    __mmask8 re_nanmask = _mm512_cmp_pd_mask(re, re, _CMP_NEQ_UQ);
    __mmask8 im_nanmask = _mm512_cmp_pd_mask(im, im, _CMP_NEQ_UQ);
    im = _mm512_mask_mov_pd(im, re_nanmask, nan);
    re = _mm512_mask_mov_pd(re, im_nanmask, nan);

    __m512d larger  = _mm512_max_pd(re, im);
    __m512d smaller = _mm512_min_pd(im, re);

    /*
     * Calculate div_mask to prevent 0./0. and inf/inf operations in div
     */
    __mmask8 zeromask = _mm512_cmp_pd_mask(larger, _mm512_setzero_pd(), _CMP_EQ_OQ);
    __mmask8 infmask = _mm512_cmp_pd_mask(smaller, inf, _CMP_EQ_OQ);
    __mmask8 div_mask = _mm512_knot(_mm512_kor(zeromask, infmask));
    __m512d ratio = _mm512_maskz_div_pd(div_mask, smaller, larger);
    __m512d hypot = _mm512_sqrt_pd(_mm512_fmadd_pd(
                                        ratio, ratio, _mm512_set1_pd(1.0f)));
    return _mm512_mul_pd(hypot, larger);
}

NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F __m512d
avx512_conjugate_pd(const __m512d x)
{
    /*
     * __mm512_mask_xor_ps/pd requires AVX512DQ. We cast it to __m512i and
     * use the xor_epi32/64 uinstruction instead. Cast is a zero latency instruction
     */
    __m512i cast_x = _mm512_castpd_si512(x);
    __m512i res = _mm512_mask_xor_epi64(cast_x, 0xAA,
                                        cast_x, _mm512_set1_epi64(0x8000000000000000));
    return _mm512_castsi512_pd(res);
}

NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F __m512d
avx512_cmul_pd(__m512d x1, __m512d x2)
{
    // x1 = r1, i1
    // x2 = r2, i2
    __m512d x3  = _mm512_permute_pd(x2, 0x55);   // i2, r2
    __m512d x12 = _mm512_mul_pd(x1, x2);            // r1*r2, i1*i2
    __m512d x13 = _mm512_mul_pd(x1, x3);            // r1*i2, r2*i1
    __m512d outreal = avx512_hsub_pd(x12);          // r1*r2 - i1*i2, r1*r2 - i1*i2
    __m512d outimg  = avx512_hadd_pd(x13);          // r1*i2 + i1*r2, r1*i2 + i1*r2
    return _mm512_mask_blend_pd(0xAA, outreal, outimg);
}

NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F __m512d
avx512_csquare_pd(__m512d x)
{
    return avx512_cmul_pd(x, x);
}


#endif

#line 827

#if defined HAVE_ATTRIBUTE_TARGET_AVX2_WITH_INTRINSICS

NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_FMA __m256
fma_sqrt_ps(__m256 x)
{
    return _mm256_sqrt_ps(x);
}

NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_FMA __m256d
fma_sqrt_pd(__m256d x)
{
    return _mm256_sqrt_pd(x);
}

NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_FMA __m256
fma_square_ps(__m256 x)
{
    return _mm256_mul_ps(x,x);
}

NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_FMA __m256d
fma_square_pd(__m256d x)
{
    return _mm256_mul_pd(x,x);
}

#endif

#line 827

#if defined HAVE_ATTRIBUTE_TARGET_AVX512F_WITH_INTRINSICS

NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F __m512
avx512_sqrt_ps(__m512 x)
{
    return _mm512_sqrt_ps(x);
}

NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F __m512d
avx512_sqrt_pd(__m512d x)
{
    return _mm512_sqrt_pd(x);
}

NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F __m512
avx512_square_ps(__m512 x)
{
    return _mm512_mul_ps(x,x);
}

NPY_FINLINE NPY_GCC_OPT_3 NPY_GCC_TARGET_AVX512F __m512d
avx512_square_pd(__m512d x)
{
    return _mm512_mul_pd(x,x);
}

#endif


#line 869

#line 876

#if defined HAVE_ATTRIBUTE_TARGET_AVX512_SKX_WITH_INTRINSICS && defined NPY_HAVE_SSE2_INTRINSICS
static NPY_INLINE NPY_GCC_TARGET_AVX512_SKX void
AVX512_SKX_isnan_FLOAT(npy_bool* op, npy_float* ip, const npy_intp array_size, const npy_intp steps)
{
    const npy_intp stride_ip = steps/(npy_intp)sizeof(npy_float);
    npy_intp num_remaining_elements = array_size;

    __mmask16 load_mask = avx512_get_full_load_mask_ps();
#if 0
    __m512 signbit = _mm512_set1_ps(-0.0);
#endif

    /*
     * Note: while generally indices are npy_intp, we ensure that our maximum
     * index will fit in an int32 as a precondition for this function via
     * IS_OUTPUT_BLOCKABLE_UNARY
     */

    npy_int32 index_ip[16];
    for (npy_int32 ii = 0; ii < 16; ii++) {
        index_ip[ii] = ii*stride_ip;
    }
    __m512i vindex_ip = _mm512_loadu_si512((__m512i*)&index_ip[0]);
    __m512 zeros_f = _mm512_setzero_ps();
    __m512i ones = _mm512_set1_epi32(1);

    while (num_remaining_elements > 0) {
        if (num_remaining_elements < 16) {
            load_mask = avx512_get_partial_load_mask_ps(
                                    num_remaining_elements, 16);
        }
        __m512 x1;
        if (stride_ip == 1) {
            x1 = avx512_masked_load_ps(load_mask, ip);
        }
        else {
            x1 = avx512_masked_gather_ps(zeros_f, ip, vindex_ip, load_mask);
        }
#if 0
        x1 = _mm512_and_ps(x1,signbit);
#endif

        __mmask16 fpclassmask = _mm512_fpclass_ps_mask(x1, 0x81);
#if 0
        fpclassmask = _mm512_knot(fpclassmask);
#endif

        __m128i out =_mm512_maskz_cvtsepi32_epi8(fpclassmask, ones);
        _mm_mask_storeu_epi8(op, load_mask, out);

        ip += 16*stride_ip;
        op += 16;
        num_remaining_elements -= 16;
    }
}
#endif

#line 876

#if defined HAVE_ATTRIBUTE_TARGET_AVX512_SKX_WITH_INTRINSICS && defined NPY_HAVE_SSE2_INTRINSICS
static NPY_INLINE NPY_GCC_TARGET_AVX512_SKX void
AVX512_SKX_isfinite_FLOAT(npy_bool* op, npy_float* ip, const npy_intp array_size, const npy_intp steps)
{
    const npy_intp stride_ip = steps/(npy_intp)sizeof(npy_float);
    npy_intp num_remaining_elements = array_size;

    __mmask16 load_mask = avx512_get_full_load_mask_ps();
#if 0
    __m512 signbit = _mm512_set1_ps(-0.0);
#endif

    /*
     * Note: while generally indices are npy_intp, we ensure that our maximum
     * index will fit in an int32 as a precondition for this function via
     * IS_OUTPUT_BLOCKABLE_UNARY
     */

    npy_int32 index_ip[16];
    for (npy_int32 ii = 0; ii < 16; ii++) {
        index_ip[ii] = ii*stride_ip;
    }
    __m512i vindex_ip = _mm512_loadu_si512((__m512i*)&index_ip[0]);
    __m512 zeros_f = _mm512_setzero_ps();
    __m512i ones = _mm512_set1_epi32(1);

    while (num_remaining_elements > 0) {
        if (num_remaining_elements < 16) {
            load_mask = avx512_get_partial_load_mask_ps(
                                    num_remaining_elements, 16);
        }
        __m512 x1;
        if (stride_ip == 1) {
            x1 = avx512_masked_load_ps(load_mask, ip);
        }
        else {
            x1 = avx512_masked_gather_ps(zeros_f, ip, vindex_ip, load_mask);
        }
#if 0
        x1 = _mm512_and_ps(x1,signbit);
#endif

        __mmask16 fpclassmask = _mm512_fpclass_ps_mask(x1, 0x99);
#if 1
        fpclassmask = _mm512_knot(fpclassmask);
#endif

        __m128i out =_mm512_maskz_cvtsepi32_epi8(fpclassmask, ones);
        _mm_mask_storeu_epi8(op, load_mask, out);

        ip += 16*stride_ip;
        op += 16;
        num_remaining_elements -= 16;
    }
}
#endif

#line 876

#if defined HAVE_ATTRIBUTE_TARGET_AVX512_SKX_WITH_INTRINSICS && defined NPY_HAVE_SSE2_INTRINSICS
static NPY_INLINE NPY_GCC_TARGET_AVX512_SKX void
AVX512_SKX_isinf_FLOAT(npy_bool* op, npy_float* ip, const npy_intp array_size, const npy_intp steps)
{
    const npy_intp stride_ip = steps/(npy_intp)sizeof(npy_float);
    npy_intp num_remaining_elements = array_size;

    __mmask16 load_mask = avx512_get_full_load_mask_ps();
#if 0
    __m512 signbit = _mm512_set1_ps(-0.0);
#endif

    /*
     * Note: while generally indices are npy_intp, we ensure that our maximum
     * index will fit in an int32 as a precondition for this function via
     * IS_OUTPUT_BLOCKABLE_UNARY
     */

    npy_int32 index_ip[16];
    for (npy_int32 ii = 0; ii < 16; ii++) {
        index_ip[ii] = ii*stride_ip;
    }
    __m512i vindex_ip = _mm512_loadu_si512((__m512i*)&index_ip[0]);
    __m512 zeros_f = _mm512_setzero_ps();
    __m512i ones = _mm512_set1_epi32(1);

    while (num_remaining_elements > 0) {
        if (num_remaining_elements < 16) {
            load_mask = avx512_get_partial_load_mask_ps(
                                    num_remaining_elements, 16);
        }
        __m512 x1;
        if (stride_ip == 1) {
            x1 = avx512_masked_load_ps(load_mask, ip);
        }
        else {
            x1 = avx512_masked_gather_ps(zeros_f, ip, vindex_ip, load_mask);
        }
#if 0
        x1 = _mm512_and_ps(x1,signbit);
#endif

        __mmask16 fpclassmask = _mm512_fpclass_ps_mask(x1, 0x18);
#if 0
        fpclassmask = _mm512_knot(fpclassmask);
#endif

        __m128i out =_mm512_maskz_cvtsepi32_epi8(fpclassmask, ones);
        _mm_mask_storeu_epi8(op, load_mask, out);

        ip += 16*stride_ip;
        op += 16;
        num_remaining_elements -= 16;
    }
}
#endif

#line 876

#if defined HAVE_ATTRIBUTE_TARGET_AVX512_SKX_WITH_INTRINSICS && defined NPY_HAVE_SSE2_INTRINSICS
static NPY_INLINE NPY_GCC_TARGET_AVX512_SKX void
AVX512_SKX_signbit_FLOAT(npy_bool* op, npy_float* ip, const npy_intp array_size, const npy_intp steps)
{
    const npy_intp stride_ip = steps/(npy_intp)sizeof(npy_float);
    npy_intp num_remaining_elements = array_size;

    __mmask16 load_mask = avx512_get_full_load_mask_ps();
#if 1
    __m512 signbit = _mm512_set1_ps(-0.0);
#endif

    /*
     * Note: while generally indices are npy_intp, we ensure that our maximum
     * index will fit in an int32 as a precondition for this function via
     * IS_OUTPUT_BLOCKABLE_UNARY
     */

    npy_int32 index_ip[16];
    for (npy_int32 ii = 0; ii < 16; ii++) {
        index_ip[ii] = ii*stride_ip;
    }
    __m512i vindex_ip = _mm512_loadu_si512((__m512i*)&index_ip[0]);
    __m512 zeros_f = _mm512_setzero_ps();
    __m512i ones = _mm512_set1_epi32(1);

    while (num_remaining_elements > 0) {
        if (num_remaining_elements < 16) {
            load_mask = avx512_get_partial_load_mask_ps(
                                    num_remaining_elements, 16);
        }
        __m512 x1;
        if (stride_ip == 1) {
            x1 = avx512_masked_load_ps(load_mask, ip);
        }
        else {
            x1 = avx512_masked_gather_ps(zeros_f, ip, vindex_ip, load_mask);
        }
#if 1
        x1 = _mm512_and_ps(x1,signbit);
#endif

        __mmask16 fpclassmask = _mm512_fpclass_ps_mask(x1, 0x04);
#if 0
        fpclassmask = _mm512_knot(fpclassmask);
#endif

        __m128i out =_mm512_maskz_cvtsepi32_epi8(fpclassmask, ones);
        _mm_mask_storeu_epi8(op, load_mask, out);

        ip += 16*stride_ip;
        op += 16;
        num_remaining_elements -= 16;
    }
}
#endif


#line 869

#line 876

#if defined HAVE_ATTRIBUTE_TARGET_AVX512_SKX_WITH_INTRINSICS && defined NPY_HAVE_SSE2_INTRINSICS
static NPY_INLINE NPY_GCC_TARGET_AVX512_SKX void
AVX512_SKX_isnan_DOUBLE(npy_bool* op, npy_double* ip, const npy_intp array_size, const npy_intp steps)
{
    const npy_intp stride_ip = steps/(npy_intp)sizeof(npy_double);
    npy_intp num_remaining_elements = array_size;

    __mmask8 load_mask = avx512_get_full_load_mask_pd();
#if 0
    __m512d signbit = _mm512_set1_pd(-0.0);
#endif

    /*
     * Note: while generally indices are npy_intp, we ensure that our maximum
     * index will fit in an int32 as a precondition for this function via
     * IS_OUTPUT_BLOCKABLE_UNARY
     */

    npy_int32 index_ip[8];
    for (npy_int32 ii = 0; ii < 8; ii++) {
        index_ip[ii] = ii*stride_ip;
    }
    __m256i vindex_ip = _mm256_loadu_si256((__m256i*)&index_ip[0]);
    __m512d zeros_f = _mm512_setzero_pd();
    __m512i ones = _mm512_set1_epi64(1);

    while (num_remaining_elements > 0) {
        if (num_remaining_elements < 8) {
            load_mask = avx512_get_partial_load_mask_pd(
                                    num_remaining_elements, 8);
        }
        __m512d x1;
        if (stride_ip == 1) {
            x1 = avx512_masked_load_pd(load_mask, ip);
        }
        else {
            x1 = avx512_masked_gather_pd(zeros_f, ip, vindex_ip, load_mask);
        }
#if 0
        x1 = _mm512_and_pd(x1,signbit);
#endif

        __mmask8 fpclassmask = _mm512_fpclass_pd_mask(x1, 0x81);
#if 0
        fpclassmask = _mm512_knot(fpclassmask);
#endif

        __m128i out =_mm512_maskz_cvtsepi64_epi8(fpclassmask, ones);
        _mm_mask_storeu_epi8(op, load_mask, out);

        ip += 8*stride_ip;
        op += 8;
        num_remaining_elements -= 8;
    }
}
#endif

#line 876

#if defined HAVE_ATTRIBUTE_TARGET_AVX512_SKX_WITH_INTRINSICS && defined NPY_HAVE_SSE2_INTRINSICS
static NPY_INLINE NPY_GCC_TARGET_AVX512_SKX void
AVX512_SKX_isfinite_DOUBLE(npy_bool* op, npy_double* ip, const npy_intp array_size, const npy_intp steps)
{
    const npy_intp stride_ip = steps/(npy_intp)sizeof(npy_double);
    npy_intp num_remaining_elements = array_size;

    __mmask8 load_mask = avx512_get_full_load_mask_pd();
#if 0
    __m512d signbit = _mm512_set1_pd(-0.0);
#endif

    /*
     * Note: while generally indices are npy_intp, we ensure that our maximum
     * index will fit in an int32 as a precondition for this function via
     * IS_OUTPUT_BLOCKABLE_UNARY
     */

    npy_int32 index_ip[8];
    for (npy_int32 ii = 0; ii < 8; ii++) {
        index_ip[ii] = ii*stride_ip;
    }
    __m256i vindex_ip = _mm256_loadu_si256((__m256i*)&index_ip[0]);
    __m512d zeros_f = _mm512_setzero_pd();
    __m512i ones = _mm512_set1_epi64(1);

    while (num_remaining_elements > 0) {
        if (num_remaining_elements < 8) {
            load_mask = avx512_get_partial_load_mask_pd(
                                    num_remaining_elements, 8);
        }
        __m512d x1;
        if (stride_ip == 1) {
            x1 = avx512_masked_load_pd(load_mask, ip);
        }
        else {
            x1 = avx512_masked_gather_pd(zeros_f, ip, vindex_ip, load_mask);
        }
#if 0
        x1 = _mm512_and_pd(x1,signbit);
#endif

        __mmask8 fpclassmask = _mm512_fpclass_pd_mask(x1, 0x99);
#if 1
        fpclassmask = _mm512_knot(fpclassmask);
#endif

        __m128i out =_mm512_maskz_cvtsepi64_epi8(fpclassmask, ones);
        _mm_mask_storeu_epi8(op, load_mask, out);

        ip += 8*stride_ip;
        op += 8;
        num_remaining_elements -= 8;
    }
}
#endif

#line 876

#if defined HAVE_ATTRIBUTE_TARGET_AVX512_SKX_WITH_INTRINSICS && defined NPY_HAVE_SSE2_INTRINSICS
static NPY_INLINE NPY_GCC_TARGET_AVX512_SKX void
AVX512_SKX_isinf_DOUBLE(npy_bool* op, npy_double* ip, const npy_intp array_size, const npy_intp steps)
{
    const npy_intp stride_ip = steps/(npy_intp)sizeof(npy_double);
    npy_intp num_remaining_elements = array_size;

    __mmask8 load_mask = avx512_get_full_load_mask_pd();
#if 0
    __m512d signbit = _mm512_set1_pd(-0.0);
#endif

    /*
     * Note: while generally indices are npy_intp, we ensure that our maximum
     * index will fit in an int32 as a precondition for this function via
     * IS_OUTPUT_BLOCKABLE_UNARY
     */

    npy_int32 index_ip[8];
    for (npy_int32 ii = 0; ii < 8; ii++) {
        index_ip[ii] = ii*stride_ip;
    }
    __m256i vindex_ip = _mm256_loadu_si256((__m256i*)&index_ip[0]);
    __m512d zeros_f = _mm512_setzero_pd();
    __m512i ones = _mm512_set1_epi64(1);

    while (num_remaining_elements > 0) {
        if (num_remaining_elements < 8) {
            load_mask = avx512_get_partial_load_mask_pd(
                                    num_remaining_elements, 8);
        }
        __m512d x1;
        if (stride_ip == 1) {
            x1 = avx512_masked_load_pd(load_mask, ip);
        }
        else {
            x1 = avx512_masked_gather_pd(zeros_f, ip, vindex_ip, load_mask);
        }
#if 0
        x1 = _mm512_and_pd(x1,signbit);
#endif

        __mmask8 fpclassmask = _mm512_fpclass_pd_mask(x1, 0x18);
#if 0
        fpclassmask = _mm512_knot(fpclassmask);
#endif

        __m128i out =_mm512_maskz_cvtsepi64_epi8(fpclassmask, ones);
        _mm_mask_storeu_epi8(op, load_mask, out);

        ip += 8*stride_ip;
        op += 8;
        num_remaining_elements -= 8;
    }
}
#endif

#line 876

#if defined HAVE_ATTRIBUTE_TARGET_AVX512_SKX_WITH_INTRINSICS && defined NPY_HAVE_SSE2_INTRINSICS
static NPY_INLINE NPY_GCC_TARGET_AVX512_SKX void
AVX512_SKX_signbit_DOUBLE(npy_bool* op, npy_double* ip, const npy_intp array_size, const npy_intp steps)
{
    const npy_intp stride_ip = steps/(npy_intp)sizeof(npy_double);
    npy_intp num_remaining_elements = array_size;

    __mmask8 load_mask = avx512_get_full_load_mask_pd();
#if 1
    __m512d signbit = _mm512_set1_pd(-0.0);
#endif

    /*
     * Note: while generally indices are npy_intp, we ensure that our maximum
     * index will fit in an int32 as a precondition for this function via
     * IS_OUTPUT_BLOCKABLE_UNARY
     */

    npy_int32 index_ip[8];
    for (npy_int32 ii = 0; ii < 8; ii++) {
        index_ip[ii] = ii*stride_ip;
    }
    __m256i vindex_ip = _mm256_loadu_si256((__m256i*)&index_ip[0]);
    __m512d zeros_f = _mm512_setzero_pd();
    __m512i ones = _mm512_set1_epi64(1);

    while (num_remaining_elements > 0) {
        if (num_remaining_elements < 8) {
            load_mask = avx512_get_partial_load_mask_pd(
                                    num_remaining_elements, 8);
        }
        __m512d x1;
        if (stride_ip == 1) {
            x1 = avx512_masked_load_pd(load_mask, ip);
        }
        else {
            x1 = avx512_masked_gather_pd(zeros_f, ip, vindex_ip, load_mask);
        }
#if 1
        x1 = _mm512_and_pd(x1,signbit);
#endif

        __mmask8 fpclassmask = _mm512_fpclass_pd_mask(x1, 0x04);
#if 0
        fpclassmask = _mm512_knot(fpclassmask);
#endif

        __m128i out =_mm512_maskz_cvtsepi64_epi8(fpclassmask, ones);
        _mm_mask_storeu_epi8(op, load_mask, out);

        ip += 8*stride_ip;
        op += 8;
        num_remaining_elements -= 8;
    }
}
#endif



#line 950

#line 955

#if defined HAVE_ATTRIBUTE_TARGET_AVX512F_WITH_INTRINSICS && defined NPY_HAVE_SSE2_INTRINSICS
static NPY_GCC_OPT_3 NPY_INLINE NPY_GCC_TARGET_AVX512F void
AVX512F_square_CFLOAT(npy_float * op,
                      npy_float * ip,
                      const npy_intp array_size,
                      const npy_intp steps)
{
    npy_intp num_remaining_elements = 2*array_size;
    const npy_intp stride_ip1 = steps/(npy_intp)sizeof(npy_float)/2;

     /*
      * Note: while generally indices are npy_intp, we ensure that our maximum index
      * will fit in an int32 as a precondition for this function via max_stride
      */
    npy_int32 index_ip1[16];
    for (npy_int32 ii = 0; ii < 16; ii=ii+2) {
        index_ip1[ii] = ii*stride_ip1;
        index_ip1[ii+1] = ii*stride_ip1 + 1;
    }
    __m512i vindex = _mm512_loadu_si512((__m512i*)index_ip1);
    __mmask16 load_mask = avx512_get_full_load_mask_ps();
    __m512 zeros = _mm512_setzero_ps();

    while (num_remaining_elements > 0) {
        if (num_remaining_elements < 16) {
            load_mask = avx512_get_partial_load_mask_ps(
                                    num_remaining_elements, 16);
        }
        __m512 x1;
        if (stride_ip1 == 1) {
            x1 = avx512_masked_load_ps(load_mask, ip);
        }
        else {
            x1  = avx512_masked_gather_ps(zeros, ip, vindex, load_mask);
        }

        __m512 out = avx512_csquare_ps(x1);

        _mm512_mask_storeu_ps(op, load_mask, out);
        op += 16;
        ip += 16*stride_ip1;
        num_remaining_elements -= 16;
    }
}
#endif

#line 955

#if defined HAVE_ATTRIBUTE_TARGET_AVX512F_WITH_INTRINSICS && defined NPY_HAVE_SSE2_INTRINSICS
static NPY_GCC_OPT_3 NPY_INLINE NPY_GCC_TARGET_AVX512F void
AVX512F_conjugate_CFLOAT(npy_float * op,
                      npy_float * ip,
                      const npy_intp array_size,
                      const npy_intp steps)
{
    npy_intp num_remaining_elements = 2*array_size;
    const npy_intp stride_ip1 = steps/(npy_intp)sizeof(npy_float)/2;

     /*
      * Note: while generally indices are npy_intp, we ensure that our maximum index
      * will fit in an int32 as a precondition for this function via max_stride
      */
    npy_int32 index_ip1[16];
    for (npy_int32 ii = 0; ii < 16; ii=ii+2) {
        index_ip1[ii] = ii*stride_ip1;
        index_ip1[ii+1] = ii*stride_ip1 + 1;
    }
    __m512i vindex = _mm512_loadu_si512((__m512i*)index_ip1);
    __mmask16 load_mask = avx512_get_full_load_mask_ps();
    __m512 zeros = _mm512_setzero_ps();

    while (num_remaining_elements > 0) {
        if (num_remaining_elements < 16) {
            load_mask = avx512_get_partial_load_mask_ps(
                                    num_remaining_elements, 16);
        }
        __m512 x1;
        if (stride_ip1 == 1) {
            x1 = avx512_masked_load_ps(load_mask, ip);
        }
        else {
            x1  = avx512_masked_gather_ps(zeros, ip, vindex, load_mask);
        }

        __m512 out = avx512_conjugate_ps(x1);

        _mm512_mask_storeu_ps(op, load_mask, out);
        op += 16;
        ip += 16*stride_ip1;
        num_remaining_elements -= 16;
    }
}
#endif


#if defined HAVE_ATTRIBUTE_TARGET_AVX512F_WITH_INTRINSICS && defined NPY_HAVE_SSE2_INTRINSICS
static NPY_GCC_OPT_3 NPY_INLINE NPY_GCC_TARGET_AVX512F void
AVX512F_absolute_CFLOAT(npy_float * op,
                        npy_float * ip,
                        const npy_intp array_size,
                        const npy_intp steps)
{
    npy_intp num_remaining_elements = 2*array_size;
    const npy_intp stride_ip1 = steps/(npy_intp)sizeof(npy_float)/2;

    /*
     * Note: while generally indices are npy_intp, we ensure that our maximum index
     * will fit in an int32 as a precondition for this function via max_stride
     */
    npy_int32 index_ip[32];
    for (npy_int32 ii = 0; ii < 2*16; ii=ii+2) {
        index_ip[ii] = ii*stride_ip1;
        index_ip[ii+1] = ii*stride_ip1 + 1;
    }
    __m512i vindex1 = _mm512_loadu_si512((__m512i*)index_ip);
    __m512i vindex2 = _mm512_loadu_si512((__m512i*)(index_ip+16));

    __mmask16 load_mask1 = avx512_get_full_load_mask_ps();
    __mmask16 load_mask2 = avx512_get_full_load_mask_ps();
    __mmask16 store_mask = avx512_get_full_load_mask_ps();
    __m512 zeros = _mm512_setzero_ps();

#if 1
    __m512i re_index = _mm512_set_epi32(30,28,26,24,22,20,18,16,14,12,10,8,6,4,2,0);
    __m512i im_index  = _mm512_set_epi32(31,29,27,25,23,21,19,17,15,13,11,9,7,5,3,1);
#else
    __m512i re_index = _mm512_set_epi64(14,12,10,8,6,4,2,0);
    __m512i im_index  = _mm512_set_epi64(15,13,11,9,7,5,3,1);
#endif

    while (num_remaining_elements > 0) {
        if (num_remaining_elements < 16) {
            load_mask1 = avx512_get_partial_load_mask_ps(
                                    num_remaining_elements, 16);
            load_mask2 = 0x0000;
            store_mask = avx512_get_partial_load_mask_ps(
                                    num_remaining_elements/2, 16);
        } else if (num_remaining_elements < 2*16) {
            load_mask1 = avx512_get_full_load_mask_ps();
            load_mask2 = avx512_get_partial_load_mask_ps(
                                    num_remaining_elements - 16, 16);
            store_mask = avx512_get_partial_load_mask_ps(
                                    num_remaining_elements/2, 16);
        }
        __m512 x1, x2;
        if (stride_ip1 == 1) {
            x1 = avx512_masked_load_ps(load_mask1, ip);
            x2 = avx512_masked_load_ps(load_mask2, ip+16);
        }
        else {
            x1  = avx512_masked_gather_ps(zeros, ip, vindex1, load_mask1);
            x2  = avx512_masked_gather_ps(zeros, ip, vindex2, load_mask2);
        }

        __m512 out = avx512_cabsolute_ps(x1, x2, re_index, im_index);

        _mm512_mask_storeu_ps(op, store_mask, out);
        op += 16;
        ip += 2*16*stride_ip1;
        num_remaining_elements -= 2*16;
    }
    npy_clear_floatstatus_barrier((char*)&num_remaining_elements);
}

#endif

#line 950

#line 955

#if defined HAVE_ATTRIBUTE_TARGET_AVX512F_WITH_INTRINSICS && defined NPY_HAVE_SSE2_INTRINSICS
static NPY_GCC_OPT_3 NPY_INLINE NPY_GCC_TARGET_AVX512F void
AVX512F_square_CDOUBLE(npy_double * op,
                      npy_double * ip,
                      const npy_intp array_size,
                      const npy_intp steps)
{
    npy_intp num_remaining_elements = 2*array_size;
    const npy_intp stride_ip1 = steps/(npy_intp)sizeof(npy_double)/2;

     /*
      * Note: while generally indices are npy_intp, we ensure that our maximum index
      * will fit in an int32 as a precondition for this function via max_stride
      */
    npy_int32 index_ip1[16];
    for (npy_int32 ii = 0; ii < 8; ii=ii+2) {
        index_ip1[ii] = ii*stride_ip1;
        index_ip1[ii+1] = ii*stride_ip1 + 1;
    }
    __m256i vindex = _mm256_loadu_si256((__m256i*)index_ip1);
    __mmask8 load_mask = avx512_get_full_load_mask_pd();
    __m512d zeros = _mm512_setzero_pd();

    while (num_remaining_elements > 0) {
        if (num_remaining_elements < 8) {
            load_mask = avx512_get_partial_load_mask_pd(
                                    num_remaining_elements, 8);
        }
        __m512d x1;
        if (stride_ip1 == 1) {
            x1 = avx512_masked_load_pd(load_mask, ip);
        }
        else {
            x1  = avx512_masked_gather_pd(zeros, ip, vindex, load_mask);
        }

        __m512d out = avx512_csquare_pd(x1);

        _mm512_mask_storeu_pd(op, load_mask, out);
        op += 8;
        ip += 8*stride_ip1;
        num_remaining_elements -= 8;
    }
}
#endif

#line 955

#if defined HAVE_ATTRIBUTE_TARGET_AVX512F_WITH_INTRINSICS && defined NPY_HAVE_SSE2_INTRINSICS
static NPY_GCC_OPT_3 NPY_INLINE NPY_GCC_TARGET_AVX512F void
AVX512F_conjugate_CDOUBLE(npy_double * op,
                      npy_double * ip,
                      const npy_intp array_size,
                      const npy_intp steps)
{
    npy_intp num_remaining_elements = 2*array_size;
    const npy_intp stride_ip1 = steps/(npy_intp)sizeof(npy_double)/2;

     /*
      * Note: while generally indices are npy_intp, we ensure that our maximum index
      * will fit in an int32 as a precondition for this function via max_stride
      */
    npy_int32 index_ip1[16];
    for (npy_int32 ii = 0; ii < 8; ii=ii+2) {
        index_ip1[ii] = ii*stride_ip1;
        index_ip1[ii+1] = ii*stride_ip1 + 1;
    }
    __m256i vindex = _mm256_loadu_si256((__m256i*)index_ip1);
    __mmask8 load_mask = avx512_get_full_load_mask_pd();
    __m512d zeros = _mm512_setzero_pd();

    while (num_remaining_elements > 0) {
        if (num_remaining_elements < 8) {
            load_mask = avx512_get_partial_load_mask_pd(
                                    num_remaining_elements, 8);
        }
        __m512d x1;
        if (stride_ip1 == 1) {
            x1 = avx512_masked_load_pd(load_mask, ip);
        }
        else {
            x1  = avx512_masked_gather_pd(zeros, ip, vindex, load_mask);
        }

        __m512d out = avx512_conjugate_pd(x1);

        _mm512_mask_storeu_pd(op, load_mask, out);
        op += 8;
        ip += 8*stride_ip1;
        num_remaining_elements -= 8;
    }
}
#endif


#if defined HAVE_ATTRIBUTE_TARGET_AVX512F_WITH_INTRINSICS && defined NPY_HAVE_SSE2_INTRINSICS
static NPY_GCC_OPT_3 NPY_INLINE NPY_GCC_TARGET_AVX512F void
AVX512F_absolute_CDOUBLE(npy_double * op,
                        npy_double * ip,
                        const npy_intp array_size,
                        const npy_intp steps)
{
    npy_intp num_remaining_elements = 2*array_size;
    const npy_intp stride_ip1 = steps/(npy_intp)sizeof(npy_double)/2;

    /*
     * Note: while generally indices are npy_intp, we ensure that our maximum index
     * will fit in an int32 as a precondition for this function via max_stride
     */
    npy_int32 index_ip[32];
    for (npy_int32 ii = 0; ii < 2*8; ii=ii+2) {
        index_ip[ii] = ii*stride_ip1;
        index_ip[ii+1] = ii*stride_ip1 + 1;
    }
    __m256i vindex1 = _mm256_loadu_si256((__m256i*)index_ip);
    __m256i vindex2 = _mm256_loadu_si256((__m256i*)(index_ip+8));

    __mmask8 load_mask1 = avx512_get_full_load_mask_pd();
    __mmask8 load_mask2 = avx512_get_full_load_mask_pd();
    __mmask8 store_mask = avx512_get_full_load_mask_pd();
    __m512d zeros = _mm512_setzero_pd();

#if 0
    __m512i re_index = _mm512_set_epi32(30,28,26,24,22,20,18,16,14,12,10,8,6,4,2,0);
    __m512i im_index  = _mm512_set_epi32(31,29,27,25,23,21,19,17,15,13,11,9,7,5,3,1);
#else
    __m512i re_index = _mm512_set_epi64(14,12,10,8,6,4,2,0);
    __m512i im_index  = _mm512_set_epi64(15,13,11,9,7,5,3,1);
#endif

    while (num_remaining_elements > 0) {
        if (num_remaining_elements < 8) {
            load_mask1 = avx512_get_partial_load_mask_pd(
                                    num_remaining_elements, 8);
            load_mask2 = 0x0000;
            store_mask = avx512_get_partial_load_mask_pd(
                                    num_remaining_elements/2, 8);
        } else if (num_remaining_elements < 2*8) {
            load_mask1 = avx512_get_full_load_mask_pd();
            load_mask2 = avx512_get_partial_load_mask_pd(
                                    num_remaining_elements - 8, 8);
            store_mask = avx512_get_partial_load_mask_pd(
                                    num_remaining_elements/2, 8);
        }
        __m512d x1, x2;
        if (stride_ip1 == 1) {
            x1 = avx512_masked_load_pd(load_mask1, ip);
            x2 = avx512_masked_load_pd(load_mask2, ip+8);
        }
        else {
            x1  = avx512_masked_gather_pd(zeros, ip, vindex1, load_mask1);
            x2  = avx512_masked_gather_pd(zeros, ip, vindex2, load_mask2);
        }

        __m512d out = avx512_cabsolute_pd(x1, x2, re_index, im_index);

        _mm512_mask_storeu_pd(op, store_mask, out);
        op += 8;
        ip += 2*8*stride_ip1;
        num_remaining_elements -= 2*8;
    }
    npy_clear_floatstatus_barrier((char*)&num_remaining_elements);
}

#endif


/*
 *****************************************************************************
 **                           BOOL LOOPS
 *****************************************************************************
 */

#line 1094

/*
 * convert any bit set to boolean true so vectorized and normal operations are
 * consistent, should not be required if bool is used correctly everywhere but
 * you never know
 */
#if !0
NPY_FINLINE __m128i byte_to_true(__m128i v)
{
    const __m128i zero = _mm_setzero_si128();
    const __m128i truemask = _mm_set1_epi8(1 == 1);
    /* get 0xFF for zeros */
    __m128i tmp = _mm_cmpeq_epi8(v, zero);
    /* filled with 0xFF/0x00, negate and mask to boolean true */
    return _mm_andnot_si128(tmp, truemask);
}
#endif

static void
sse2_binary_logical_or_BOOL(npy_bool * op, npy_bool * ip1, npy_bool * ip2, npy_intp n)
{
    LOOP_BLOCK_ALIGN_VAR(op, npy_bool, VECTOR_SIZE_BYTES)
        op[i] = ip1[i] || ip2[i];
    LOOP_BLOCKED(npy_bool, VECTOR_SIZE_BYTES) {
        __m128i a = _mm_loadu_si128((__m128i*)&ip1[i]);
        __m128i b = _mm_loadu_si128((__m128i*)&ip2[i]);
#if 0
        const __m128i zero = _mm_setzero_si128();
        /* get 0xFF for non zeros*/
        __m128i tmp = _mm_cmpeq_epi8(a, zero);
        /* andnot -> 0x00 for zeros xFF for non zeros, & with ip2 */
        tmp = _mm_andnot_si128(tmp, b);
#else
        __m128i tmp = _mm_or_si128(a, b);
#endif

        _mm_store_si128((__m128i*)&op[i], byte_to_true(tmp));
    }
    LOOP_BLOCKED_END {
        op[i] = (ip1[i] || ip2[i]);
    }
}


static void
sse2_reduce_logical_or_BOOL(npy_bool * op, npy_bool * ip, const npy_intp n)
{
    const __m128i zero = _mm_setzero_si128();
    LOOP_BLOCK_ALIGN_VAR(ip, npy_bool, VECTOR_SIZE_BYTES) {
        *op = *op || ip[i];
        if (*op != 0) {
            return;
        }
    }
    /* unrolled once to replace a slow movmsk with a fast pmaxb */
    LOOP_BLOCKED(npy_bool, 2 * VECTOR_SIZE_BYTES) {
        __m128i v = _mm_load_si128((__m128i*)&ip[i]);
        __m128i v2 = _mm_load_si128((__m128i*)&ip[i + VECTOR_SIZE_BYTES]);
        v = _mm_cmpeq_epi8(v, zero);
        v2 = _mm_cmpeq_epi8(v2, zero);
#if 0
        if ((_mm_movemask_epi8(_mm_max_epu8(v, v2)) != 0)) {
            *op = 0;
#else
        if ((_mm_movemask_epi8(_mm_min_epu8(v, v2)) != 0xFFFF)) {
            *op = 1;
#endif
            return;
        }
    }
    LOOP_BLOCKED_END {
        *op = *op || ip[i];
        if (*op != 0) {
            return;
        }
    }
}


#line 1094

/*
 * convert any bit set to boolean true so vectorized and normal operations are
 * consistent, should not be required if bool is used correctly everywhere but
 * you never know
 */
#if !1
NPY_FINLINE __m128i byte_to_true(__m128i v)
{
    const __m128i zero = _mm_setzero_si128();
    const __m128i truemask = _mm_set1_epi8(1 == 1);
    /* get 0xFF for zeros */
    __m128i tmp = _mm_cmpeq_epi8(v, zero);
    /* filled with 0xFF/0x00, negate and mask to boolean true */
    return _mm_andnot_si128(tmp, truemask);
}
#endif

static void
sse2_binary_logical_and_BOOL(npy_bool * op, npy_bool * ip1, npy_bool * ip2, npy_intp n)
{
    LOOP_BLOCK_ALIGN_VAR(op, npy_bool, VECTOR_SIZE_BYTES)
        op[i] = ip1[i] && ip2[i];
    LOOP_BLOCKED(npy_bool, VECTOR_SIZE_BYTES) {
        __m128i a = _mm_loadu_si128((__m128i*)&ip1[i]);
        __m128i b = _mm_loadu_si128((__m128i*)&ip2[i]);
#if 1
        const __m128i zero = _mm_setzero_si128();
        /* get 0xFF for non zeros*/
        __m128i tmp = _mm_cmpeq_epi8(a, zero);
        /* andnot -> 0x00 for zeros xFF for non zeros, & with ip2 */
        tmp = _mm_andnot_si128(tmp, b);
#else
        __m128i tmp = _mm_or_si128(a, b);
#endif

        _mm_store_si128((__m128i*)&op[i], byte_to_true(tmp));
    }
    LOOP_BLOCKED_END {
        op[i] = (ip1[i] && ip2[i]);
    }
}


static void
sse2_reduce_logical_and_BOOL(npy_bool * op, npy_bool * ip, const npy_intp n)
{
    const __m128i zero = _mm_setzero_si128();
    LOOP_BLOCK_ALIGN_VAR(ip, npy_bool, VECTOR_SIZE_BYTES) {
        *op = *op && ip[i];
        if (*op == 0) {
            return;
        }
    }
    /* unrolled once to replace a slow movmsk with a fast pmaxb */
    LOOP_BLOCKED(npy_bool, 2 * VECTOR_SIZE_BYTES) {
        __m128i v = _mm_load_si128((__m128i*)&ip[i]);
        __m128i v2 = _mm_load_si128((__m128i*)&ip[i + VECTOR_SIZE_BYTES]);
        v = _mm_cmpeq_epi8(v, zero);
        v2 = _mm_cmpeq_epi8(v2, zero);
#if 1
        if ((_mm_movemask_epi8(_mm_max_epu8(v, v2)) != 0)) {
            *op = 0;
#else
        if ((_mm_movemask_epi8(_mm_min_epu8(v, v2)) != 0xFFFF)) {
            *op = 1;
#endif
            return;
        }
    }
    LOOP_BLOCKED_END {
        *op = *op && ip[i];
        if (*op == 0) {
            return;
        }
    }
}



#line 1185

static void
sse2_absolute_BOOL(npy_bool * op, npy_bool * ip, const npy_intp n)
{
    LOOP_BLOCK_ALIGN_VAR(op, npy_bool, VECTOR_SIZE_BYTES)
        op[i] = (ip[i] != 0);
    LOOP_BLOCKED(npy_bool, VECTOR_SIZE_BYTES) {
        __m128i a = _mm_loadu_si128((__m128i*)&ip[i]);
#if 0
        const __m128i zero = _mm_setzero_si128();
        const __m128i truemask = _mm_set1_epi8(1 == 1);
        /* equivalent to byte_to_true but can skip the negation */
        a = _mm_cmpeq_epi8(a, zero);
        a = _mm_and_si128(a, truemask);
#else
        /* abs is kind of pointless but maybe its used for byte_to_true */
        a = byte_to_true(a);
#endif
        _mm_store_si128((__m128i*)&op[i], a);
    }
    LOOP_BLOCKED_END {
        op[i] = (ip[i] != 0);
    }
}


#line 1185

static void
sse2_logical_not_BOOL(npy_bool * op, npy_bool * ip, const npy_intp n)
{
    LOOP_BLOCK_ALIGN_VAR(op, npy_bool, VECTOR_SIZE_BYTES)
        op[i] = (ip[i] == 0);
    LOOP_BLOCKED(npy_bool, VECTOR_SIZE_BYTES) {
        __m128i a = _mm_loadu_si128((__m128i*)&ip[i]);
#if 1
        const __m128i zero = _mm_setzero_si128();
        const __m128i truemask = _mm_set1_epi8(1 == 1);
        /* equivalent to byte_to_true but can skip the negation */
        a = _mm_cmpeq_epi8(a, zero);
        a = _mm_and_si128(a, truemask);
#else
        /* abs is kind of pointless but maybe its used for byte_to_true */
        a = byte_to_true(a);
#endif
        _mm_store_si128((__m128i*)&op[i], a);
    }
    LOOP_BLOCKED_END {
        op[i] = (ip[i] == 0);
    }
}



#undef VECTOR_SIZE_BYTES
#endif  /* NPY_HAVE_SSE2_INTRINSICS */
#endif


