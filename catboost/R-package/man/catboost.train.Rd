% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/catboost.R
\name{catboost.train}
\alias{catboost.train}
\title{Train the model}
\usage{
catboost.train(learn_pool, test_pool = NULL, params = list())
}
\arguments{
\item{learn_pool}{The dataset used for training the model.

Default value: Required argument}

\item{test_pool}{The dataset used for testing the quality of the model.

Default value: NULL (not used)}

\item{params}{The list of parameters to start training with.

If omitted, default values are used (see The list of parameters).

If set, the passed list of parameters overrides the default values.

Default value: Required argument}
}
\value{
Model object.
}
\description{
Train the model using a CatBoost dataset.

The list of parameters

\itemize{
  \item Common parameters
  \itemize{
    \item fold_permutation_block

      Objects in the dataset are grouped in blocks before the random permutations.
      This parameter defines the size of the blocks.
      The smaller is the value, the slower is the training.
      Large values may result in quality degradation.

      Default value:

      Default value differs depending on the dataset size and ranges from 1 to 256 inclusively
    \item ignored_features

      Identifiers of features to exclude from training.
      The non-negative indices that do not match any features are successfully ignored.
      For example, if five features are defined for the objects in the dataset and this parameter
      is set to "42", the corresponding non-existing feature is successfully ignored.

      The identifier corresponds to the feature's index.
      Feature indices used in train and feature importance are numbered from 0 to featureCount-1.
      If a file is used as input data then any non-feature column types are ignored when calculating these
      indices. For example, each row in the input file contains data in the following order:
      "categorical feature<\verb{\t}>label<\verb{\t}>numerical feature". So for the row "rock<\verb{\t}>0<\verb{\t}>42",
      the identifier for the "rock" feature is 0, and for the "42" feature it is 1.

      The identifiers of features to exclude should be enumerated at vector.

      For example, if training should exclude features with the identifiers
      1, 2, 7, 42, 43, 44, 45, the value of this parameter should be set to c(1,2,7,42,43,44,45).

      Default value:

      None (use all features)
    \item use_best_model

      If this parameter is set, the number of trees that are saved in the resulting model is defined as follows:

      Build the number of trees defined by the training parameters.
      \itemize{
        \item Identify the iteration with the optimal loss function value.
        \item No trees are saved after this iteration.
      }

      This option requires a test dataset to be provided.

      Default value:

      FALSE (not used)
    \item loss_function

      The loss function (see \url{https://catboost.ai/docs/concepts/loss-functions.html#loss-functions})
      to use in training. The specified value also determines the machine learning problem to solve.

      Format:

      <Loss function 1>[:<parameter 1>=<value>:..<parameter N>=<value>:]

      Supported loss functions:
      \itemize{
        \item 'Logloss'
        \item 'CrossEntropy'
        \item 'MultiClass'
        \item 'MultiClassOneVsAll'
        \item 'RMSE'
        \item 'MAE'
        \item 'Quantile'
        \item 'LogLinQuantile'
        \item 'MAPE'
        \item 'Poisson'
        \item 'Lq'
        \item 'PairLogit'
        \item 'PairLogitPairwise'
        \item 'YetiRank'
        \item 'YetiRankPairwise'
        \item 'QueryCrossEntropy'
        \item 'QueryRMSE'
        \item 'QuerySoftMax'
      }

      Supported parameters:
      \itemize{
        \item alpha - The coefficient used in quantile-based losses ('Quantile' and 'LogLinQuantile'). The default value is 0.5.

        For example, if you need to calculate the value of Quantile with the coefficient \eqn{\alpha = 0.1}, use the following construction:

        'Quantile:alpha=0.1'
      }

      Default value:

      'RMSE'
    \item custom_loss

      Loss function (see \url{https://catboost.ai/docs/concepts/loss-functions.html#loss-functions})
      values to output during training.
      These functions are not used for optimization and are displayed for informational purposes only.

      Format:

      c(<Loss function 1>[:<parameter>=<value>],<Loss function 2>[:<parameter>=<value>],...,<Loss function N>[:<parameter>=<value>])

      Supported loss functions:
      \itemize{
        \item 'Logloss'
        \item 'CrossEntropy'
        \item 'Precision'
        \item 'Recall'
        \item 'F1'
        \item 'F'
        \item 'BalancedAccuracy'
        \item 'BalancedErrorRate'
        \item 'MCC'
        \item 'Accuracy'
        \item 'CtrFactor'
        \item 'AUC'
        \item 'BrierScore'
        \item 'HingeLoss'
        \item 'HammingLoss'
        \item 'ZeroOneLoss'
        \item 'Kappa'
        \item 'WKappa'
        \item 'LogLikelihoodOfPrediction'
        \item 'MultiClass'
        \item 'MultiClassOneVsAll'
        \item 'TotalF1'
        \item 'MAE'
        \item 'MAPE'
        \item 'Poisson'
        \item 'Quantile'
        \item 'RMSE'
        \item 'LogLinQuantile'
        \item 'Lq'
        \item 'NumErrors'
        \item 'SMAPE'
        \item 'R2'
        \item 'MSLE'
        \item 'MedianAbsoluteError'
        \item 'PairLogit'
        \item 'PairLogitPairwise'
        \item 'PairAccuracy'
        \item 'QueryCrossEntropy'
        \item 'QueryRMSE'
        \item 'QuerySoftMax'
        \item 'PFound'
        \item 'NDCG'
        \item 'AverageGain'
        \item 'PrecisionAt'
        \item 'RecallAt'
        \item 'MAP'
        \item 'MRR'
        \item 'ERR'
      }

      Supported parameters:
      \itemize{
        \item alpha - The coefficient used in quantile-based losses ('Quantile' and 'LogLinQuantile'). The default value is 0.5.
      }

      For example, if you need to calculate the value of CrossEntropy and Quantile with the coefficient \eqn{\alpha = 0.1}, use the following construction:

      c('CrossEntropy') or simply 'CrossEntropy'.

      Values of all custom loss functions for learning and test datasets are saved to the Loss function
      (see \url{https://catboost.ai/docs/concepts/output-data_loss-function.html#output-data_loss-function})
      output files (learn_error.tsv and test_error.tsv respectively). The catalog for these files is specified in the train-dir (train_dir) parameter.

      Default value:

      None (use one of the loss functions supported by the library)
    \item eval_metric

      The loss function used for overfitting detection (if enabled) and best model selection (if enabled).

      Supported loss functions:
      \itemize{
        \item 'Logloss'
        \item 'CrossEntropy'
        \item 'Precision'
        \item 'Recall'
        \item 'F1'
        \item 'F'
        \item 'BalancedAccuracy'
        \item 'BalancedErrorRate'
        \item 'MCC'
        \item 'Accuracy'
        \item 'CtrFactor'
        \item 'AUC'
        \item 'BrierScore'
        \item 'HingeLoss'
        \item 'HammingLoss'
        \item 'ZeroOneLoss'
        \item 'Kappa'
        \item 'WKappa'
        \item 'LogLikelihoodOfPrediction'
        \item 'MultiClass'
        \item 'MultiClassOneVsAll'
        \item 'TotalF1'
        \item 'MAE'
        \item 'MAPE'
        \item 'Poisson'
        \item 'Quantile'
        \item 'RMSE'
        \item 'LogLinQuantile'
        \item 'Lq'
        \item 'NumErrors'
        \item 'SMAPE'
        \item 'R2'
        \item 'MSLE'
        \item 'MedianAbsoluteError'
        \item 'PairLogit'
        \item 'PairLogitPairwise'
        \item 'PairAccuracy'
        \item 'QueryCrossEntropy'
        \item 'QueryRMSE'
        \item 'QuerySoftMax'
        \item 'PFound'
        \item 'NDCG'
        \item 'AverageGain'
        \item 'PrecisionAt'
        \item 'RecallAt'
        \item 'MAP'
        \item 'MRR'
        \item 'ERR'
      }

      Format:

      metric_name:param=Value

      Examples:

      \code{'R2'}

      \code{'Quantile:alpha=0.3'}

      Default value:

      Optimized objective is used

    \item iterations

      The maximum number of trees that can be built when solving machine learning problems.

      When using other parameters that limit the number of iterations, the final number of trees may be less
      than the number specified in this parameter.

      Default value:

      1000

    \item border

      The target border. If the value is strictly greater than this threshold,
      it is considered a positive class. Otherwise it is considered a negative class.

      The parameter is obligatory if the Logloss function is used, since it uses borders to transform
      any given target to a binary target.

      Used in binary classification.

      Default value:

      0.5

    \item leaf_estimation_iterations

      The number of gradient steps when calculating the values in leaves.

      Default value:

      1

    \item depth

      Depth of the trees.

      The value can be any integer up to 16. It is recommended to use values in the range [1; 10].

      Default value:

      6
    \item learning_rate

      The learning rate.

      Used for reducing the gradient step.

      Default value:

      0.03

    \item rsm

      Random subspace method. The percentage of features to use at each iteration of building trees. At each iteration, features are selected over again at random.

      The value must be in the range [0;1].

      Default value:

      1

    \item random_seed

      The random seed used for training.

      Default value:

      0

   \item nan_mode

      Way to process missing values.

      Possible values:
      \itemize{
        \item \code{'Min'}
        \item \code{'Max'}
        \item \code{'Forbidden'}
      }

      Default value:

      \code{'Min'}

    \item od_pval

      Use the Overfitting detector (see \url{https://catboost.ai/docs/concepts/overfitting-detector.html#overfitting-detector})
      to stop training when the threshold is reached.
      Requires that a test dataset was input.

      For best results, it is recommended to set a value in the range [10^-10; 10^-2].

      The larger the value, the earlier overfitting is detected.

      Default value:

      The overfitting detection is turned off

    \item od_type

      The method used to calculate the values in leaves.

      Possible values:
      \itemize{
        \item IncToDec
        \item Iter
      }

      Restriction.
      Do not specify the overfitting detector threshold when using the Iter type.

      Default value:

      'IncToDec'

    \item od_wait

      The number of iterations to continue the training after the iteration with the optimal loss function value.
      The purpose of this parameter differs depending on the selected overfitting detector type:
      \itemize{
        \item IncToDec - Ignore the overfitting detector when the threshold is reached and continue learning for the specified number of iterations after the iteration with the optimal loss function value.
        \item Iter - Consider the model overfitted and stop training after the specified number of iterations since the iteration with the optimal loss function value.
      }

      Default value:

      20

    \item leaf_estimation_method

      The method used to calculate the values in leaves.

      Possible values:
      \itemize{
        \item Newton
        \item Gradient
      }

      Default value:

      Default value depends on the selected loss function

    \item grow_policy

      GPU only. The tree growing policy. It describes how to perform greedy tree construction.

      Possible values:
      \itemize{
        \item SymmetricTree
        \item Lossguide
        \item Depthwise
      }

      Default value:

      SymmetricTree

    \item min_data_in_leaf

      GPU only.
      The minimum training samples count in leaf.
      CatBoost will not search for new splits in leaves with samples count less than min_data_in_leaf.
      This parameter is used only for Depthwise and Lossguide growing policies.

      Default value:

      1

    \item max_leaves

      GPU only. The maximum leaf count in resulting tree. Used only for Lossguide growing policy.
      This parameter is used only for Lossguide growing policy.

      Default value:

      31

    \item score_function
      GPU only. Score that is used during tree construction to select the next tree split.

      Possible values:
      \itemize{
        \item L2
        \item Cosine
        \item NewtonL2
        \item NewtonCosine
      }

      Default value:

      Cosine

      For growing policy Lossguide default is NewtonL2.

    \item l2_leaf_reg

      L2 regularization coefficient. Used for leaf value calculation.

      Any positive values are allowed.

      Default value:

      3

    \item model_size_reg

      Model size regularization coefficient. The influence coefficient of the model size for choosing tree structure.
      To get a smaller model size - increase this coefficient.

      Any positive values are allowed.

      Default value:

      0.5

    \item has_time

      Use the order of objects in the input data
      (do not perform a random permutation of the dataset at the preprocessing stage)

      Default value:

      FALSE (not used; permute input dataset)

    \item allow_const_label

      To allow the constant label value in the dataset.

      Default value:

      FALSE

    \item name

      The experiment name to display in visualization tools (see \url{https://catboost.ai/docs/features/visualization.html#visualization}).

      Default value:

      experiment

    \item prediction_type

      The format for displaying approximated values in output data.

      Possible values:
      \itemize{
        \item 'Probability'
        \item 'Class'
        \item 'RawFormulaVal'
      }

      Default value:

      \code{'RawFormulaVal'}

    \item fold_len_multiplier

      Coefficient for changing the length of folds.

      The value must be greater than 1. The best validation result is achieved with minimum values.

      With values close to 1 (for example, \eqn{1 + \epsilon}), each iteration takes a quadratic amount of memory and time
      for the number of objects in the iteration. Thus, low values are possible only when there is a small number of objects.

      Default value:

      2

    \item class_weights

      Classes weights. The values are used as multipliers for the object weights.

      For example, for 3 class classification you could use:

      \code{c(0.85, 1.2, 1)}

      Default value:

      None (the weight for all classes is set to 1)

    \item classes_count

      The upper limit for the numeric class label. Defines the number of classes for multiclassification.

      Only non-negative integers can be specified. The given integer should be greater than any of the target
      values.

      If this parameter is specified the labels for all classes in the input dataset should be smaller
      than the given value.

      Default value:

      maximum class label + 1

    \item one_hot_max_size

      Convert the feature to float if the number of different values that it takes exceeds the specified value. Ctrs are not calculated for such features.

      The one-vs.-all delimiter is used for the resulting float features.

      Default value:

      FALSE

      Do not convert features to float based on the number of different values

    \item random_strength

      Score standard deviation multiplier.

       Default value:

       1

    \item bootstrap_type

      Bootstrap type. Defines the method for sampling the weights of documents.

      Possible values:
      \itemize{
        \item 'Bayesian'
        \item 'Bernoulli'
        \item 'Poisson'
        \item 'MVS'
        \item 'No'
      }

      Poisson bootstrap is supported only on GPU.

      Default value:

      \code{'Bayesian'}

    \item bagging_temperature

       Controls intensity of Bayesian bagging. The higher the temperature the more aggressive bagging is.

       Typical values are in the range \eqn{[0, 1]} (0 is for no bagging).

       Possible values are in the range \eqn{[0, +\infty)}.

       Default value:

       1

    \item subsample

      Sample rate for bagging. This parameter can be used if one of the following bootstrap types is defined:
      \itemize{
        \item 'Bernoulli'
      }

      Default value:

      0.66

    \item sampling_unit

      The parameter allows to specify the sampling scheme: sample weights for each object individually or for an entire group of objects together.

      Possible values:
      \itemize{
        \item 'Object'
        \item 'Group'
      }

      Default value:

      \code{'Object'}

    \item sampling_frequency

      Frequency to sample weights and objects when building trees.

      Possible values:
      \itemize{
        \item 'PerTree'
        \item 'PerTreeLevel'
      }

      Default value:

      \code{'PerTreeLevel'}

    \item model_shrink_rate

      For i > 0 at the start of i-th iteration multiplies model by (1 - model_shrink_rate / i).

      Possible values: [0, 1).

      Default value: 0
  }
  \item CTR settings
  \itemize{
    \item simple_ctr

      Binarization settings for categorical features (see \url{https://catboost.ai/docs/concepts/algorithm-main-stages_cat-to-numberic.html}).

      Format:

      \code{c(CtrType[:TargetBorderCount=BorderCount][:TargetBorderType=BorderType][:CtrBorderCount=Count][:CtrBorderType=Type][:Prior=num_1/denum_1]..[:Prior=num_N/denum_N])}

      Components:
      \itemize{
        \item CTR types for training on CPU:
        \itemize{
          \item \code{'Borders'}
          \item \code{'Buckets'}
          \item \code{'BinarizedTargetMeanValue'}
          \item \code{'Counter'}
        }
        \item CTR types for training on GPU:
        \itemize{
          \item \code{'Borders'}
          \item \code{'Buckets'}
          \item \code{'FeatureFreq'}
          \item \code{'FloatTargetMeanValue'}
        }
        \item The number of borders for label value binarization. (see \url{https://catboost.ai/docs/concepts/quantization.html})
        Only used for regression problems. Allowed values are integers from 1 to 255 inclusively. The default value is 1.
        This option is available for training on CPU only.
        \item The binarization (see \url{https://catboost.ai/docs/concepts/quantization.html})
        type for the label value. Only used for regression problems.

        Possible values:
        \itemize{
          \item \code{'Median'}
          \item \code{'Uniform'}
          \item \code{'UniformAndQuantiles'}
          \item \code{'MaxLogSum'}
          \item \code{'MinEntropy'}
          \item \code{'GreedyLogSum'}
        }
        By default, \code{'MinEntropy'}
        This option is available for training on CPU only.
        \item The number of splits for categorical features. Allowed values are integers from 1 to 255 inclusively.
        \item The binarization type for categorical features.
        Supported values for training on CPU:
        \itemize{
          \item \code{'Uniform'}
        }
        Supported values for training on GPU:
        \itemize{
          \item \code{'Median'}
          \item \code{'Uniform'}
          \item \code{'UniformAndQuantiles'}
          \item \code{'MaxLogSum'}
          \item \code{'MinEntropy'}
          \item \code{'GreedyLogSum'}
        }
        \item Priors to use during training (several values can be specified)
        Possible formats:
        \itemize{
          \item \code{'One number - Adds the value to the numerator.'}
          \item \code{'Two slash-delimited numbers (for GPU only) - Use this format to set a fraction. The number is added to the numerator and the second is added to the denominator.'}
        }
      }

    \item combinations_ctr

      Binarization settings for combinations of categorical features (see \url{https://catboost.ai/docs/concepts/algorithm-main-stages_cat-to-numberic.html}).

      Format:

      \code{c(CtrType[:TargetBorderCount=BorderCount][:TargetBorderType=BorderType][:CtrBorderCount=Count][:CtrBorderType=Type][:Prior=num_1/denum_1]..[:Prior=num_N/denum_N])}

      Components:
      \itemize{
        \item CTR types for training on CPU:
        \itemize{
          \item \code{'Borders'}
          \item \code{'Buckets'}
          \item \code{'BinarizedTargetMeanValue'}
          \item \code{'Counter'}
        }
        \item CTR types for training on GPU:
        \itemize{
          \item \code{'Borders'}
          \item \code{'Buckets'}
          \item \code{'FeatureFreq'}
          \item \code{'FloatTargetMeanValue'}
        }
        \item The number of borders for target binarization. (see \url{https://catboost.ai/docs/concepts/quantization.html})
        Only used for regression problems. Allowed values are integers from 1 to 255 inclusively. The default value is 1.
        This option is available for training on CPU only.
        \item The binarization (see \url{https://catboost.ai/docs/concepts/quantization.html})
        type for the target. Only used for regression problems.

        Possible values:
        \itemize{
          \item \code{'Median'}
          \item \code{'Uniform'}
          \item \code{'UniformAndQuantiles'}
          \item \code{'MaxLogSum'}
          \item \code{'MinEntropy'}
          \item \code{'GreedyLogSum'}
        }
        By default, \code{'MinEntropy'}
        This option is available for training on CPU only.
        \item The number of splits for categorical features. Allowed values are integers from 1 to 255 inclusively.
        \item The binarization type for categorical features.
        Supported values for training on CPU:
        \itemize{
          \item \code{'Uniform'}
        }
        Supported values for training on GPU:
        \itemize{
          \item \code{'Median'}
          \item \code{'Uniform'}
          \item \code{'UniformAndQuantiles'}
          \item \code{'MaxLogSum'}
          \item \code{'MinEntropy'}
          \item \code{'GreedyLogSum'}
        }
        \item Priors to use during training (several values can be specified)
        Possible formats:
        \itemize{
          \item \code{'One number - Adds the value to the numerator.'}
          \item \code{'Two slash-delimited numbers (for GPU only) - Use this format to set a fraction. The number is added to the numerator and the second is added to the denominator.'}
        }
      }

    \item ctr_target_border_count

      Maximum number of borders used in target binarization for categorical features that need it.
      If TargetBorderCount is specified in 'simple_ctr', 'combinations_ctr' or 'per_feature_ctr' option it overrides this value.

      Default value:

      1

    \item counter_calc_method

      The method for calculating the Counter CTR type for the test dataset.

      Possible values:
        \itemize{
          \item \code{'Full'}
          \item \code{'FullTest'}
          \item \code{'PrefixTest'}
          \item \code{'SkipTest'}
        }

        Default value: \code{'PrefixTest'}

    \item max_ctr_complexity

      The maximum number of categorical features that can be combined.

      Default value:

      4

    \item ctr_leaf_count_limit

      The maximum number of leaves with categorical features.
      If the number of leaves exceeds the specified limit, some leaves are discarded.
      The value must be positive (for zero limit use \code{ignored_features} parameter).

      The leaves to be discarded are selected as follows:
      \enumerate{
        \item The leaves are sorted by the frequency of the values.
        \item The top N leaves are selected, where N is the value specified in the parameter.
        \item All leaves starting from N+1 are discarded.
      }

      This option reduces the resulting model size and the amount of memory required for training.
      Note that the resulting quality of the model can be affected.

      Default value:

      None (The number of leaves with categorical features is not limited)

    \item store_all_simple_ctr

      Ignore categorical features, which are not used in feature combinations,
      when choosing candidates for exclusion.

      Use this parameter with ctr-leaf-count-limit only.

      Default value:

      FALSE (Both simple features and feature combinations are taken in account when limiting the number of leaves with categorical features)


  }
  \item Binarization settings
  \itemize{
    \item  border_count

      The number of splits for numerical features. Allowed values are integers from 1 to 255 inclusively.

      Default value:

      254 for training on CPU or 128 for training on GPU

    \item feature_border_type

      The binarization mode (see \url{https://catboost.ai/docs/concepts/quantization.html})
      for numerical features.

      Possible values:
      \itemize{
        \item \code{'Median'}
        \item \code{'Uniform'}
        \item \code{'UniformAndQuantiles'}
        \item \code{'MaxLogSum'}
        \item \code{'MinEntropy'}
        \item \code{'GreedyLogSum'}
      }

      Default value:

      \code{'MinEntropy'}
  }
  \item Performance settings
  \itemize{
    \item thread_count

      The number of threads to use when applying the model.

      Allows you to optimize the speed of execution. This parameter doesn't affect results.

      Default value:

      The number of CPU cores.
  }
  \item Output settings
  \itemize{
    \item logging_level

      Possible values:
      \itemize{
        \item \code{'Silent'}
        \item \code{'Verbose'}
        \item \code{'Info'}
        \item \code{'Debug'}
      }

      Default value:

      'Silent'

    \item metric_period

      The frequency of iterations to print the information to stdout. The value should be a positive integer.

      Default value:

      1

    \item train_dir

      The directory for storing the files generated during training.

      Default value:

      None (current catalog)

    \item save_snapshot

      Enable snapshotting for restoring the training progress after an interruption.

      Default value:

      None

    \item snapshot_file

      Settings for recovering training after an interruption (see
      \url{https://catboost.ai/docs/features/snapshots.html}).

      Depending on whether the file specified exists in the file system:
      \itemize{
        \item Missing - write information about training progress to the specified file.
        \item Exists - load data from the specified file and continue training from where it left off.
      }

      Default value:

      File can't be generated or read. If the value is omitted, the file name is experiment.cbsnapshot.

  \item snapshot_interval

      Interval between saving snapshots (seconds)

      Default value:

      600

  \item allow_writing_files

      If this flag is set to FALSE, no files with different diagnostic info will be created during training.
      With this flag set to FALSE no snapshotting can be done. Plus visualisation will not
      work, because visualisation uses files that are created and updated during training.

      Default value:

      TRUE

  \item approx_on_full_history

      If this flag is set to TRUE, each approximated value is calculated using all the preceding rows in the fold (slower, more accurate).
      If this flag is set to FALSE, each approximated value is calculated using only the beginning 1/fold_len_multiplier fraction of the fold (faster, slightly less accurate).

      Default value:

      FALSE

  \item boosting_type

      Boosting scheme.
     Possible values:
         - 'Ordered' - Gives better quality, but may slow down the training.
         - 'Plain' - The classic gradient boosting scheme. May result in quality degradation, but does not slow down the training.

      Default value:

      Depends on object count and feature count in train dataset and on learning mode.

  \item dev_score_calc_obj_block_size

      CPU only. Size of block of samples in score calculation. Should be > 0
      Used only for learning speed tuning.
      Changing this parameter can affect results in pairwise scoring mode due to numerical accuracy differences

      Default value:

      5000000

  \item dev_efb_max_buckets

      CPU only. Maximum bucket count in exclusive features bundle. Should be in an integer between 0 and 65536.
      Used only for learning speed tuning.

      Default value:

      1024

  \item sparse_features_conflict_fraction

     CPU only. Maximum allowed fraction of conflicting non-default values for features in exclusive features bundle.
     Should be a real value in [0, 1) interval.

     Default value:

     0.0

   \item leaf_estimation_backtracking

       Type of backtracking during gradient descent.
       Possible values:
           - 'No' - never backtrack; supported on CPU and GPU
           - 'AnyImprovement' - reduce the descent step until the value of loss function is less than before the step; supported on CPU and GPU
           - 'Armijo' - reduce the descent step until Armijo condition is satisfied; supported on GPU only

       Default value:

       'AnyImprovement'

  }
}
}
\examples{
\dontrun{
train_pool_path <- system.file("extdata", "adult_train.1000", package = "catboost")
test_pool_path <- system.file("extdata", "adult_test.1000", package = "catboost")
cd_path <- system.file("extdata", "adult.cd", package = "catboost")
train_pool <- catboost.load_pool(train_pool_path, column_description = cd_path)
test_pool <- catboost.load_pool(test_pool_path, column_description = cd_path)
fit_params <- list(
    iterations = 100,
    loss_function = 'Logloss',
    ignored_features = c(4, 9),
    border_count = 32,
    depth = 5,
    learning_rate = 0.03,
    l2_leaf_reg = 3.5,
    train_dir = 'train_dir')
model <- catboost.train(train_pool, test_pool, fit_params)
}
}
\seealso{
\url{https://catboost.ai/docs/concepts/r-reference_catboost-train.html}
}
