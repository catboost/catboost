# Tokenizer

```python
class Tokenizer(lowercasing=None,
                lemmatizing=None,
                number_process_policy=None,
                number_token=None,
                separator_type=None,
                delimiter=None,
                split_by_set=None,
                skip_empty=None,
                token_types=None,
                sub_tokens_policy=None,
                languages=None)
```

## {{ dl--purpose }} {#purpose}

{% include [text-features-python__tokenize_class__description](../_includes/work_src/reusage-python/python__tokenize_class__description.md) %}


## {{ dl--parameters }} {#parameters}

{% include [tokenizer-options-tokenizer_options](../_includes/work_src/reusage-tokenizer/tokenizer_options.md) %}


## {{ dl--methods }} {#methods}

### [tokenize](python-reference_tokenizer_tokenize.md)

{% include [tokenize-tokenize__purpose](../_includes/work_src/reusage-tokenizer/tokenize__purpose.md) %}
